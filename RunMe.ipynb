{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Directional Attention Flow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Checkpoint and Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: PyYAML in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (5.3.1)\n",
      "Requirement already up-to-date: h5py in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from h5py) (1.19.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from h5py) (1.15.0)\n",
      "Requirement already satisfied: pytorch-lightning in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (0.8.4)\n",
      "Requirement already satisfied: torch>=1.3 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pytorch-lightning) (1.5.1+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.14 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pytorch-lightning) (2.2.2)\n",
      "Requirement already satisfied: future>=0.17.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pytorch-lightning) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pytorch-lightning) (1.19.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pytorch-lightning) (5.3.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pytorch-lightning) (4.47.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (2.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (44.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (3.2.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.9.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.4.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.30.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.34.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.18.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (3.12.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U PyYAML\n",
    "!pip install -U h5py\n",
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "pwd = os.getcwd()\n",
    "\n",
    "class Arguments():\n",
    "    data = os.path.join(pwd, 'DATA', 'train_v2.1.json')\n",
    "    exp_folder = os.path.join(pwd, 'Experimente/LightningTest')\n",
    "    word_rep = os.path.join(pwd, 'DATA', 'glove.840B.300d.txt')\n",
    "    cuda = torch.cuda.is_available()\n",
    "    use_covariance = False\n",
    "    force_restart = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "if not os.path.exists(args.exp_folder):\n",
    "    os.makedirs(args.exp_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Configurations (instead of config.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_yaml = \"\"\"\n",
    "    bidaf:\n",
    "        dropout: 0.2\n",
    "        num_highways: 2\n",
    "        num_lstm: 2\n",
    "        hidden_size: 100\n",
    "        embedding_dim: 300\n",
    "        embedding_reduce: 100\n",
    "        characters:\n",
    "            dim: 16\n",
    "            num_filters: 100\n",
    "            filter_sizes:\n",
    "                - 5\n",
    "    training:\n",
    "        lr: 0.001\n",
    "        betas:\n",
    "            - 0.9\n",
    "            - 0.999\n",
    "        eps: 0.00000001\n",
    "        weigth_decay: 0\n",
    "        epochs: 1\n",
    "        batch_size: 60\n",
    "        limit: 400\n",
    "\"\"\"\n",
    "config = yaml.load(config_yaml, Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the MSMARCO Bidaf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(pwd,'MsmarcoQuestionAnswering','Baseline'))\n",
    "sys.path.append(os.path.join(pwd,'MsmarcoQuestionAnswering','Baseline','scripts'))\n",
    "\n",
    "import MsmarcoQuestionAnswering.Baseline.mrcqa as mrcqa\n",
    "import MsmarcoQuestionAnswering.Baseline.scripts.dataset as dataset\n",
    "import MsmarcoQuestionAnswering.Baseline.scripts.checkpointing as checkpointing\n",
    "import MsmarcoQuestionAnswering.Baseline.scripts.train as manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Model, Load and Tokenize Data, Getting ID-->Token and ID-->Char Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to train...\n",
      "Load Data [1/6]\n",
      "Start Organizing Data...\n",
      "Organizing progress: 0.0 x 10⁴\n",
      "Tokenize Data [2/6]\n",
      "0.0 x 10⁴/0.3696 x 10⁴\n",
      "Create Inverse Dictionaries [3/6]\n",
      "Initiate Model [4/6]\n",
      "Load pre-trained embeddings [5/6]\n",
      "Embeddings Loaded: 0.0 Mio / 2.1 Mio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages/torch/nn/modules/rnn.py:47: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Loaded: 0.1 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.2 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.3 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.4 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.5 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.6 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.7 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.8 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.9 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.0 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.1 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.2 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.3 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.4 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.5 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.6 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.7 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.8 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.9 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 2.0 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 2.1 Mio / 2.1 Mio\n",
      "Done init_state [6/6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-db91ab4d4121>:10: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  checkpoint = h5py.File(os.path.join(args.exp_folder, 'checkpoint'))\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "\"\"\"\n",
    "load_data does the following:\n",
    "- \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#data is a tuple: (qid, passage, query, (answer_start, answer_stop))\n",
    "\n",
    "checkpoint, training_state, epoch = manager.try_to_resume(args.force_restart, args.exp_folder)\n",
    "if checkpoint:\n",
    "        print('Resuming training...')\n",
    "        model, id_to_token, id_to_char, optimizer, data = manager.reload_state(checkpoint, training_state, config, args, 10000)\n",
    "else:\n",
    "        print('Preparing to train...')\n",
    "        model, id_to_token, id_to_char, optimizer, data = manager.init_state(config, args,10000)\n",
    "        checkpoint = h5py.File(os.path.join(args.exp_folder, 'checkpoint'))\n",
    "        checkpointing.save_vocab(checkpoint, 'vocab', id_to_token)\n",
    "        checkpointing.save_vocab(checkpoint, 'c_vocab', id_to_char)\n",
    "        checkpointing.checkpoint(model,0,optimizer,checkpoint, args.exp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    data.tensor_type = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import nll_loss\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import mrcqa\n",
    "from mrcqa.modules.highway import Highways\n",
    "\n",
    "from pytorch_lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMatrix(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Matrix (unnormalized)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        \"\"\"\n",
    "        Create a module for attention matrices. The input is a pair of\n",
    "        matrices, the output is a matrix containing similarity scores between\n",
    "        pairs of element in the matrices.\n",
    "\n",
    "        Similarity between two vectors `a` and `b` is measured by\n",
    "        $f(a, b) = W[a;b;ab] + C$, where:\n",
    "            1. $W$ is a 1-by-3H matrix,\n",
    "            2. $C$ is a bias,\n",
    "            3. $ab$ is the element-wise product of $a$ and $b$.\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "            :param: hidden_size (int): The size of the vectors\n",
    "\n",
    "        Variables/sub-modules:\n",
    "            projection: The linear projection $W$, $C$.\n",
    "\n",
    "        Inputs:\n",
    "            :param: mat_0 ([batch, n, hidden_size] Tensor): the first matrices\n",
    "            :param: mat_1 ([batch, m, hidden_size] Tensor): the second matrices\n",
    "\n",
    "        Returns:\n",
    "            :return: similarity (batch, n, m) Tensor: the similarity matrices,\n",
    "            so that similarity[:, n, m] = f(mat_0[:, n], mat_1[:, m])\n",
    "        \"\"\"\n",
    "        super(AttentionMatrix, self).__init__()\n",
    "        self.hidden_size = hidden_size #thats the embedding size\n",
    "        self.projection = nn.Linear(3*hidden_size, 1) #Thats the linear Projection w^T * [h; u; h*u] from the Paper.\n",
    "        return\n",
    "\n",
    "    def forward(self, mat_0, mat_1):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        batch, n_0, _ = mat_0.size() #H in R^{batch_size, number of words in context, 2 x embedding_size} \n",
    "        _, n_1, _ = mat_1.size() #U in R^{batch_size, number of words in query, 2 x embedding_size}\n",
    "        mat_0, mat_1 = self.tile_to_match(mat_0, mat_1)\n",
    "        mat_p = mat_0*mat_1\n",
    "        combined = torch.cat((mat_0, mat_1, mat_p), dim=3) \n",
    "        # projected down to [b, n, m]\n",
    "        projected = self.projection(\n",
    "            combined.view(batch*n_0*n_1, 3*self.hidden_size)) #Smash Tensor to Matrix of the given Size\n",
    "        projected = projected.view(batch, n_0, n_1)\n",
    "        return projected\n",
    "\n",
    "    @classmethod\n",
    "    def tile_to_match(cls, mat_0, mat_1):\n",
    "        \"\"\"\n",
    "        Enables broadcasting between mat_0 and mat_1.\n",
    "        Both are tiled to 4 dimensions, from 3.\n",
    "\n",
    "        Shape:\n",
    "            mat_0: [b, n, e], and\n",
    "            mat_1: [b, m, e].\n",
    "\n",
    "        Then, they get reshaped and expanded:\n",
    "            mat_0: [b, n, e] -> [b, n, 1, e] -> [b, n, m, e]\n",
    "            mat_1: [b, m, e] -> [b, 1, m, e] -> [b, n, m, e]\n",
    "        \"\"\"\n",
    "        batch, n_0, size = mat_0.size()\n",
    "        batch_1, n_1, size_1 = mat_1.size()\n",
    "        assert batch == batch_1\n",
    "        assert size_1 == size\n",
    "        mat_0 = mat_0.unsqueeze(2).expand(\n",
    "            batch, n_0, n_1, size)\n",
    "        mat_1 = mat_1.unsqueeze(1).expand(\n",
    "            batch, n_0, n_1, size)\n",
    "        return mat_0, mat_1\n",
    "    \n",
    "        \"\"\"\n",
    "        Ein Tensor mehrdimensionaler Tensor ist pro Dimension ein Vektor mit Pointern auf die nächsten Vektoren \n",
    "        (Vorstellung: Baum mit Knoten (Vektoren von Pointern auf Kinder) und Blätter (Vektoren mit Inhalt))\n",
    "        Dann ist unsqueeze(k) das Einfügen von Vektoren der Größe 1 in Ebene k mit einem Pointer auf die nächste Ebene (war vorher Ebene k)\n",
    "        expand() erweitert die Größe der Vektoren mit Größe 1.\n",
    "        cat() fügt bei dim eine neue Ebene ein mit zeigern auf die concatinierten Elemente\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidafModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional attention flow model for question answering.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedder,\n",
    "                 num_highways,\n",
    "                 num_lstm,\n",
    "                 hidden_size,\n",
    "                 dropout):\n",
    "        \"\"\"\n",
    "        Create a BiDAF model. The input is a tensor of indices, or a tuple of\n",
    "        same. The outputs are start and end log probability vectors.\n",
    "\n",
    "        Overall model, assuming no batches:\n",
    "            1. The passage and question are encoded independently using a\n",
    "            shared set of embeddings, highway layers and a bidirectional\n",
    "            LSTM layer.\n",
    "            2. The passage and question are combined into an attention matrix.\n",
    "            3. The attention matrix is applied to the question, to get a\n",
    "            question-in-passage matrix, with one row per token in the passage.\n",
    "            4. The same attention matrix is applied to the passage, to get a\n",
    "            passage-in-question vector, which is then tiled to get one row per\n",
    "            token in the passage.\n",
    "            5. The resulting matrices are concatenated with the passage, and\n",
    "            with their product with the passage.\n",
    "            6. This is then passed through a stack of bidirectional LSTMs.\n",
    "            7. The results is projected down to 1 dimension, to get the start\n",
    "            logits.\n",
    "            8. This is also used as attention, and combined with the LSTM stack\n",
    "            inputs and outputs, and passed through a final LSTM.\n",
    "            9. The output is again concatenated with step 5, and projected down\n",
    "            to 1 dimension, to get the end logits.\n",
    "            10. A log-softmax is then applied to the logits.\n",
    "\n",
    "        Parameters:\n",
    "            :param: embedder (Module): the module in that will embed the\n",
    "            passage and question\n",
    "            :param: num_highways (int): the number of highway layers to use\n",
    "            :param: num_lstm (int): the number of LSTM layers to use\n",
    "            :param: hidden_size (int): The size of the hidden layers;\n",
    "            effectively doubled for bidirectional LSTMs\n",
    "            :param: dropout (float,>=0 or None) Dropout probability\n",
    "\n",
    "        Variables/sub-modules:\n",
    "            embedder: the embeddings\n",
    "            highways: the highway layers\n",
    "            seq_encoder: the LSTM used after the highway layers to get the\n",
    "            passage and question representations\n",
    "            attention: the module used to get the attention matrix\n",
    "            extractor: the stack of LSTM following attention\n",
    "            end_encoder: the final LSTM, used to get the end logits\n",
    "            start_projection: the projection to get the start logits\n",
    "            end_projection: the projection to get the end logits\n",
    "\n",
    "        Input:\n",
    "            :param: passage: features sent to embedder for the passages\n",
    "            :param: p_lengths: vector containing the passage lengths\n",
    "            :param: question: features sent to embedder for the questions\n",
    "            :param: q_lengths: vector containing the question lengths\n",
    "\n",
    "        Return:\n",
    "            :return: start_log_probs: (batch, passage_size) float tensor\n",
    "            containing the log probabilities of the start points\n",
    "            :return: end_log_probs: (batch, passage_size) float tensor\n",
    "            containing the log probabilities of the end points\n",
    "        \"\"\"\n",
    "        super(BidafModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidir_hidden_size = 2*hidden_size\n",
    "        self.embedder = embedder\n",
    "        self.highways = Highways(embedder.output_dim, num_highways)\n",
    "        self.seq_encoder = nn.LSTM(embedder.output_dim,\n",
    "                                   hidden_size,\n",
    "                                   num_layers=1,\n",
    "                                   batch_first=True,\n",
    "                                   dropout=0,\n",
    "                                   bidirectional=True)\n",
    "        self.extractor = nn.LSTM(4*self.bidir_hidden_size,\n",
    "                                 hidden_size,\n",
    "                                 num_layers=num_lstm,\n",
    "                                 batch_first=True,\n",
    "                                 dropout=0,\n",
    "                                 bidirectional=True)\n",
    "        self.end_encoder = nn.LSTM(7*self.bidir_hidden_size,\n",
    "                                   hidden_size,\n",
    "                                   num_layers=1,\n",
    "                                   batch_first=True,\n",
    "                                   dropout=dropout,\n",
    "                                   bidirectional=True)\n",
    "        self.attention = AttentionMatrix(self.bidir_hidden_size)\n",
    "\n",
    "        # Second hidden_size is for extractor.\n",
    "        self.start_projection = nn.Linear(\n",
    "            4*self.bidir_hidden_size + self.bidir_hidden_size, 1)\n",
    "        self.end_projection = nn.Linear(\n",
    "            4*self.bidir_hidden_size + self.bidir_hidden_size, 1)\n",
    "\n",
    "        if dropout and dropout > 0:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = lambda nop: nop\n",
    "        return\n",
    "\n",
    "    @classmethod\n",
    "    def _pack_and_unpack_lstm(cls, input, lengths, seq_encoder):\n",
    "        \"\"\"\n",
    "        LSTM, when using batches, should be called with a PackedSequence.\n",
    "        Doing this will deal with the different lengths in the batch.\n",
    "        PackedSequence must be created from batches where the sequences are\n",
    "        stored with decreasing lengths.\n",
    "\n",
    "        _pack_and_unpack_lstm handles this issue.\n",
    "        It re-orders its input, pack it, sends it through the LSTM and finally\n",
    "        restore the original order.\n",
    "\n",
    "        This is not general purpose: in particular, it does not handle initial\n",
    "        and final states.\n",
    "        \"\"\"\n",
    "        s_lengths, indexes = lengths.sort(0, descending=True)\n",
    "        s_input = input.index_select(0, indexes)\n",
    "\n",
    "        i_range = torch.arange(lengths.size()[0]).type_as(lengths.data)\n",
    "        i_range = Variable(i_range)\n",
    "        _, reverses = indexes.sort(0, descending=False)\n",
    "        reverses = i_range.index_select(0, reverses)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            s_input, s_lengths.data.tolist(), batch_first=True)\n",
    "\n",
    "        output, _ = seq_encoder(packed)\n",
    "        # Unpack and apply reverse index.\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            output, batch_first=True)\n",
    "        output = output.index_select(0, reverses)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @classmethod\n",
    "    def _apply_attention_to_question(cls, similarity, enc_question, mask):\n",
    "        \"\"\"\n",
    "        Apply attention to question, while masking for lengths\n",
    "        \"\"\"\n",
    "        # similarity: [batch, m_passage, m_question]\n",
    "        # enc_question: [batch, m_question, hidden_size]\n",
    "        # mask: [batch, m_question]\n",
    "        batch, m_p, m_q = similarity.size()\n",
    "\n",
    "        _sim = similarity.view(\n",
    "            batch*m_p, m_q)\n",
    "\n",
    "        tmp_mask = mask.unsqueeze(1).expand(\n",
    "            batch, m_p, m_q).contiguous().float()\n",
    "        tmp_mask = tmp_mask.view(batch*m_p, m_q)\n",
    "        _sim = nn.functional.softmax(_sim*tmp_mask - (1-tmp_mask)*1e20, dim=1)\n",
    "        _sim = _sim.view(batch, m_p, m_q)\n",
    "\n",
    "        out = _sim.bmm(enc_question)\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def _apply_attention_to_passage(cls, similarity, enc_passage,\n",
    "                                    p_mask, q_mask):\n",
    "        \"\"\"\n",
    "        Apply attention to passage, while masking for lengths.\n",
    "        \"\"\"\n",
    "        # similarity: [batch, m_p, m_q]\n",
    "        # enc_passage: [batch, m_p, hidden_size]\n",
    "        # p_mask: [batch, m_p]\n",
    "        # q_mask: [batch, m_q]\n",
    "        batch, m_p, m_q = similarity.size()\n",
    "\n",
    "        # Mask the similarity\n",
    "        tmp_mask = q_mask.unsqueeze(1).expand(\n",
    "            batch, m_p, m_q).contiguous().float()\n",
    "        similarity = similarity * tmp_mask - (1-tmp_mask)*1e20\n",
    "        # Pick the token in the question with the highest similarity with a\n",
    "        # given token in the passage as the similarity between the entire\n",
    "        # question and that passage token\n",
    "        similarity = similarity.max(dim=2)[0]\n",
    "        # Final similarity: [batch, m_p]\n",
    "\n",
    "        tmp_mask = (1-p_mask)\n",
    "        tmp_mask = 1e20*tmp_mask\n",
    "        similarity = nn.functional.softmax(similarity*p_mask - tmp_mask, dim=1)\n",
    "        out = similarity.unsqueeze(1).bmm(enc_passage).squeeze(1)\n",
    "        return out\n",
    "\n",
    "    def _encode(self, features, lengths):\n",
    "        \"\"\"\n",
    "        Encode text with the embedder, highway layers and initial LSTM.\n",
    "        \"\"\"\n",
    "        embedded = self.embedder(features)\n",
    "        batch_size, num_tokens = embedded.size()[:2]\n",
    "        embedded = self.highways(embedded.view(\n",
    "            batch_size*num_tokens, -1))\n",
    "        embedded = embedded.view(batch_size, num_tokens, -1)\n",
    "        encoded = self.dropout(self._pack_and_unpack_lstm(\n",
    "            embedded, lengths, self.seq_encoder))\n",
    "        return encoded\n",
    "\n",
    "    @classmethod\n",
    "    def _create_mask_like(cls, lengths, like):\n",
    "        \"\"\"\n",
    "        Create masks based on lengths. The mask is then converted to match the\n",
    "        type of `like`, a Variable.\n",
    "        \"\"\"\n",
    "        mask = torch.zeros(like.size()[:2])\n",
    "        for ind, _length in enumerate(lengths.data):\n",
    "            mask[ind, :_length] = 1\n",
    "        mask = mask.type_as(like.data)\n",
    "        mask = Variable(mask, requires_grad=False)\n",
    "        return mask\n",
    "\n",
    "    def _attention(self, enc_passage, enc_question, p_mask, q_mask):\n",
    "        \"\"\"\n",
    "        Get and apply the attention matrix for the passage and question.\n",
    "        \"\"\"\n",
    "        batch_size, p_num_tokens = enc_passage.size()[:2]\n",
    "        # Similarity score (unnormalized) between passage and question.\n",
    "        # Shape: [batch, p_num_tokens, q_num_tokens]\n",
    "        similarity = self.attention(enc_passage, enc_question)\n",
    "\n",
    "        # Shape: [batch, p_num_tokens, hidden_size]\n",
    "        question_in_passage = self._apply_attention_to_question(\n",
    "            similarity, enc_question, q_mask)\n",
    "\n",
    "        # Shape: [batch, hidden_size]\n",
    "        passage_in_question = self._apply_attention_to_passage(\n",
    "            similarity, enc_passage, p_mask, q_mask)\n",
    "        passage_in_question = passage_in_question.unsqueeze(1).expand(\n",
    "            batch_size, p_num_tokens, self.bidir_hidden_size)\n",
    "        return question_in_passage, passage_in_question\n",
    "\n",
    "    def forward(self, passage, p_lengths, question, q_lengths):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        p_lengths contains the lengths of the passages in the batch\n",
    "        q_length contains the lengths of the questions in the batch\n",
    "        \"\"\"\n",
    "\n",
    "        # Encode the text\n",
    "        enc_passage = self._encode(passage, p_lengths)\n",
    "        enc_question = self._encode(question, q_lengths)\n",
    "\n",
    "        # Get the sizes\n",
    "        batch_size, p_num_tokens = enc_passage.size()[:2]\n",
    "        q_batch_size, q_num_tokens = enc_question.size()[:2]\n",
    "        assert batch_size == q_batch_size\n",
    "        assert batch_size == p_lengths.size()[0]\n",
    "        assert batch_size == q_lengths.size()[0]\n",
    "\n",
    "        # Create the masks\n",
    "        p_mask = self._create_mask_like(p_lengths, enc_passage)\n",
    "        q_mask = self._create_mask_like(q_lengths, enc_question)\n",
    "\n",
    "        # Get similarities and apply the attention mechanism\n",
    "        (question_in_passage, passage_in_question) = \\\n",
    "            self._attention(enc_passage, enc_question, p_mask, q_mask)\n",
    "\n",
    "        # Concatenate the passage and similarities, then use a LSTM stack to\n",
    "        # extract features.\n",
    "        # 4 [b, p_num_tokens, hidden_size]\n",
    "        # -> [b, n, 4*hidden_size]\n",
    "        merged_passage = torch.cat([\n",
    "            enc_passage,\n",
    "            question_in_passage,\n",
    "            enc_passage * question_in_passage,\n",
    "            enc_passage * passage_in_question],\n",
    "            dim=2)\n",
    "        extracted = self.dropout(self._pack_and_unpack_lstm(\n",
    "            merged_passage, p_lengths, self.extractor))\n",
    "\n",
    "        # Use the features to get the start point probability vectors.\n",
    "        # Also use it to as attention over the features.\n",
    "        start_input = self.dropout(\n",
    "            torch.cat([merged_passage, extracted], dim=2))\n",
    "        # [b, p_num_tokens, 4*h] -> [b, n, 1] -> [b, n]\n",
    "        start_projection = self.start_projection(start_input).squeeze(2)\n",
    "        # Mask\n",
    "        start_logits = start_projection*p_mask + (p_mask-1)*1e20\n",
    "        # And turns into probabilities\n",
    "        start_probs = nn.functional.softmax(start_logits, dim=1)\n",
    "        # And then into representation, as attention.\n",
    "        # [b, 1, hidden_size] -> [b, p_num_tokens, hidden_size]\n",
    "        start_reps = start_probs.unsqueeze(1).bmm(extracted)\n",
    "        start_reps = start_reps.expand(\n",
    "            batch_size, p_num_tokens, self.bidir_hidden_size)\n",
    "\n",
    "        # Uses various level of features to create the end point probability\n",
    "        # vectors.\n",
    "        # [b, n, 7*hidden_size]\n",
    "        end_reps = torch.cat([\n",
    "            merged_passage,\n",
    "            extracted,\n",
    "            start_reps,\n",
    "            extracted * start_reps],\n",
    "            dim=2)\n",
    "        enc_end = self.dropout(self._pack_and_unpack_lstm(\n",
    "            end_reps, p_lengths, self.end_encoder))\n",
    "        end_input = self.dropout(torch.cat([\n",
    "            merged_passage, enc_end], dim=2))\n",
    "        # [b, p_num_tokens, 7*h] -> [b, n, 1] -> [b, n]\n",
    "        end_projection = self.end_projection(end_input).squeeze(2)\n",
    "        # Mask\n",
    "        end_logits = end_projection*p_mask + (p_mask-1)*1e20\n",
    "\n",
    "        # Applies the final log-softmax to get the actual log-probability\n",
    "        # vectors.\n",
    "        start_log_probs = nn.functional.log_softmax(start_logits, dim=1)\n",
    "        end_log_probs = nn.functional.log_softmax(end_logits, dim=1)\n",
    "\n",
    "        return start_log_probs, end_log_probs\n",
    "\n",
    "    @classmethod\n",
    "    def get_loss(cls, start_log_probs, end_log_probs, starts, ends):\n",
    "        \"\"\"\n",
    "        Get the loss, $-\\log P(s|p,q)P(e|p,q)$.\n",
    "        The start and end labels are expected to be in span format,\n",
    "        so that text[start:end] is the answer.\n",
    "        \"\"\"\n",
    "\n",
    "        # Subtracts 1 from the end points, to get the exact indices, not 1\n",
    "        # after the end.\n",
    "        loss = nll_loss(start_log_probs, starts) +\\\n",
    "            nll_loss(end_log_probs, ends-1)\n",
    "        return loss\n",
    "\n",
    "    @classmethod\n",
    "    def get_best_span(cls, start_log_probs, end_log_probs):\n",
    "        \"\"\"\n",
    "        Get the best span.\n",
    "        \"\"\"\n",
    "        if isinstance(start_log_probs, Variable):\n",
    "            start_log_probs = start_log_probs.data\n",
    "        if isinstance(end_log_probs, Variable):\n",
    "            end_log_probs = end_log_probs.data\n",
    "\n",
    "        batch_size, num_tokens = start_log_probs.size()\n",
    "        start_end = torch.zeros(batch_size, 2).long()\n",
    "        max_val = start_log_probs[:, 0] + end_log_probs[:, 0]\n",
    "        max_start = start_log_probs[:, 0]\n",
    "        arg_max_start = torch.zeros(batch_size).long()\n",
    "\n",
    "        for batch in range(batch_size):\n",
    "            _start_lp = start_log_probs[batch]\n",
    "            _end_lp = end_log_probs[batch]\n",
    "            for t_s in range(1, num_tokens):\n",
    "                if max_start[batch] < _start_lp[t_s]:\n",
    "                    arg_max_start[batch] = t_s\n",
    "                    max_start[batch] = _start_lp[t_s]\n",
    "\n",
    "                cur_score = max_start[batch] + _end_lp[t_s]\n",
    "                if max_val[batch] < cur_score:\n",
    "                    start_end[batch, 0] = arg_max_start[batch]\n",
    "                    start_end[batch, 1] = t_s\n",
    "                    max_val[batch] = cur_score\n",
    "\n",
    "        # Place the end point one time step after the end, so that\n",
    "        # passage[s:e] works.\n",
    "        start_end[:, 1] += 1\n",
    "        return start_end\n",
    "\n",
    "    @classmethod\n",
    "    def get_combined_logits(cls, start_log_probs, end_log_probs):\n",
    "        \"\"\"\n",
    "        Combines the start and end log probability vectors into a matrix.\n",
    "        The rows correspond to start points, the columns to end points.\n",
    "        So, the value at m[s,e] is the log probability of the span from s to e.\n",
    "        \"\"\"\n",
    "        batch_size, p_num_tokens = start_log_probs.size()\n",
    "\n",
    "        t_starts = start_log_probs.unsqueeze(2).expand(\n",
    "            batch_size, p_num_tokens, p_num_tokens)\n",
    "        t_ends = end_log_probs.unsqueeze(1).expand(\n",
    "            batch_size, p_num_tokens, p_num_tokens)\n",
    "        return t_starts + t_ends\n",
    "\n",
    "    @classmethod\n",
    "    def get_combined_loss(cls, combined, starts, ends):\n",
    "        \"\"\"\n",
    "        Get the loss, $-\\log P(s,e|p,q)$.\n",
    "        In practice, with:\n",
    "            1. $\\Psi_s(s|p,q)$ the start logits,\n",
    "            2. $\\Psi_e(e|p,q)$ the end logits,\n",
    "            3. $Z_s = \\log\\sum_{i}\\exp\\Psi_s(i|p,q)$, the start partition,\n",
    "            4. $Z_e = \\log\\sum_{i}\\exp\\Psi_e(i|p,q)$, the end partition, and\n",
    "            5. $Z_c = \\log\\sum_{i}\\sum{j>=i}\\exp(\\Psi_s(i|p,q)+\\Psi_e(i|p,q))$,\n",
    "            the combined partition,\n",
    "        the default loss is:\n",
    "            $Z_s + Z_e - \\Psi_s(s|p,q) - \\Psi_e(e|p,q)$,\n",
    "        and the combined loss is:\n",
    "            $Z_c - \\Psi_s(s|p,q) - \\Psi_e(e|p,q)$.\n",
    "\n",
    "        The combined loss uses a normalization that ignores invalid end points.\n",
    "        This is not a major difference, and should only slow things down during\n",
    "        training.\n",
    "        This loss is only used to validate and to compare.\n",
    "        \"\"\"\n",
    "        batch_size, num_tokens, _other = combined.size()\n",
    "        assert num_tokens == _other\n",
    "        mask = torch.zeros(batch_size, num_tokens, num_tokens).float()\n",
    "        for start in range(1, num_tokens):\n",
    "            mask[:, start, :start] = -1e20\n",
    "        mask = mask.type_as(combined.data)\n",
    "        combined = combined + Variable(mask)\n",
    "        combined = combined.view(batch_size, num_tokens*num_tokens)\n",
    "        combined = nn.functional.log_softmax(combined, dim=1)\n",
    "        labels = starts * num_tokens + ends\n",
    "        return nll_loss(combined, labels)\n",
    "\n",
    "    @classmethod\n",
    "    def _parse_config(cls, config, vocab, c_vocab):\n",
    "        num_tokens = len(vocab)\n",
    "        num_chars = len(c_vocab)\n",
    "\n",
    "        token_embs = mrcqa.modules.TokenEmbedding(\n",
    "            num_tokens, config['embedding_dim'],\n",
    "            output_dim=config.get('embedding_reduce'))\n",
    "\n",
    "        _config = config['characters']\n",
    "        char_embs = mrcqa.modules.CharEmbedding(\n",
    "            num_chars,\n",
    "            _config.get('dim', 16),\n",
    "            _config.get('num_filters', 100),\n",
    "            _config.get('filter_sizes', [5]))\n",
    "        args = (\n",
    "                mrcqa.modules.CatEmbedding([token_embs, char_embs]),\n",
    "                config.get('num_highways', 2),\n",
    "                config.get('num_lstm', 2),\n",
    "                config.get('hidden_size', 100),\n",
    "                config.get('dropout', 0.2))\n",
    "        return args\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config, vocab, c_vocab):\n",
    "        \"\"\"\n",
    "        Create a model using the model description in the configuration file.\n",
    "        \"\"\"\n",
    "        model = cls(*cls._parse_config(config, vocab, c_vocab))\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, config, checkpoint):\n",
    "        \"\"\"\n",
    "        Load a model, on CPU and eval mode.\n",
    "\n",
    "        Parameters:\n",
    "            :param: config: a dictionary with the model's configuration\n",
    "            :param: checkpoint: a h5 files containing the model's parameters.\n",
    "\n",
    "        Returns:\n",
    "            :return: the model, on the cpu and in evaluation mode.\n",
    "\n",
    "        Example:\n",
    "            ```\n",
    "            with open('config.yaml') as f_o:\n",
    "                config = yaml.load(f_o)\n",
    "\n",
    "            with closing(h5py.File('checkpoint.h5', mode='r')) as checkpoint:\n",
    "                model, vocab, c_vocab = BidafModel.from_checkpoint(\n",
    "                    config, checkpoint)\n",
    "            model.cuda()\n",
    "            ```\n",
    "        \"\"\"\n",
    "        model_vocab = checkpoint['vocab']\n",
    "        model_c_vocab = checkpoint['c_vocab']\n",
    "\n",
    "        model_vocab = {id_: tok for id_, tok in enumerate(model_vocab)}\n",
    "        model_c_vocab = {id_: tok for id_, tok in enumerate(model_c_vocab)}\n",
    "\n",
    "        model = cls.from_config(\n",
    "                config,\n",
    "                model_vocab,\n",
    "                model_c_vocab)\n",
    "\n",
    "        model.load_state_dict({\n",
    "            name: torch.from_numpy(np.array(val))\n",
    "            for name, val in\n",
    "            checkpoint['model'].items()})\n",
    "        model.cpu().eval()\n",
    "        return model, model_vocab, model_c_vocab\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
