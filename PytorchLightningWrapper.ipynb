{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Directional Attention Flow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Checkpoint and Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U PyYAML\n",
    "!pip install -U h5py\n",
    "!pip install pytorch-lightning\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "pwd = os.getcwd()\n",
    "\n",
    "class Arguments():\n",
    "    data = os.path.join(pwd, 'DATA', 'train_v2.1.json')\n",
    "    exp_folder = os.path.join(pwd, 'Experimente/EarlyStoppingAdaDelta')\n",
    "    word_rep = os.path.join(pwd, 'DATA', 'glove.840B.300d.txt')\n",
    "    #word_rep = None\n",
    "    cuda = torch.cuda.is_available()\n",
    "    use_covariance = False\n",
    "    force_restart = False\n",
    "    train_original_data = os.path.join(pwd, 'DATA', 'train_v2.1.json')\n",
    "    train_splitted_data = os.path.join(pwd, 'DATA', 'train_part.json')\n",
    "    val_data = os.path.join(pwd, 'DATA', 'dev_v2.1.json')\n",
    "    test_splitted_data = os.path.join(pwd, 'DATA', 'eval_part.json')\n",
    "    val_splitted_data = os.path.join(pwd, 'DATA', 'dev_part.json')\n",
    "    test_reference_file = os.path.join(pwd, 'DATA', 'test_reference.json')\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "if not os.path.exists(args.exp_folder):\n",
    "    os.makedirs(args.exp_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Configurations (instead of config.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_yaml = \"\"\"\n",
    "    bidaf:\n",
    "        dropout: 0.2\n",
    "        num_highways: 2\n",
    "        num_lstm: 2\n",
    "        hidden_size: 100\n",
    "        embedding_dim: 300\n",
    "        embedding_reduce: 100\n",
    "        characters:\n",
    "            dim: 16\n",
    "            num_filters: 100\n",
    "            filter_sizes:\n",
    "                - 5\n",
    "    training:\n",
    "        lr: 0.001\n",
    "        betas:\n",
    "            - 0.9\n",
    "            - 0.999\n",
    "        eps: 0.00000001\n",
    "        weigth_decay: 0\n",
    "        epochs: 1\n",
    "        batch_size: 60\n",
    "        limit: 400\n",
    "\"\"\"\n",
    "config = yaml.load(config_yaml, Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the MSMARCO Bidaf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(pwd,'MsmarcoQuestionAnswering','Baseline'))\n",
    "sys.path.append(os.path.join(pwd,'MsmarcoQuestionAnswering','Baseline','scripts'))\n",
    "sys.path.append(os.path.join(pwd,'MsmarcoQuestionAnswering','Evaluation'))\n",
    "\n",
    "import MsmarcoQuestionAnswering.Baseline.scripts.checkpointing as checkpointing\n",
    "import MsmarcoQuestionAnswering.Baseline.scripts.train as train_manager\n",
    "import MsmarcoQuestionAnswering.Evaluation.ms_marco_eval as eval_manager\n",
    "#import MsmarcoQuestionAnswering.Baseline.scripts.predict as predict_manager\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MsmarcoQuestionAnswering.Baseline.mrcqa as mrcqa\n",
    "import MsmarcoQuestionAnswering.Baseline.scripts.dataset as dataset\n",
    "import json as json\n",
    "import numpy as np\n",
    "from random import shuffle, randint\n",
    "\n",
    "def try_to_split_testset(percentual_size_test, reduced_whole_size=1, force_renew=False):\n",
    "    if os.path.isfile(args.train_splitted_data) and os.path.isfile(args.test_splitted_data) and os.path.isfile(args.val_splitted_data) and not force_renew:\n",
    "        return;\n",
    "    else:\n",
    "        args.force_restart = True\n",
    "        with open(args.train_original_data) as f_o:\n",
    "            train_json = json.load(f_o);\n",
    "        qids = list(train_json['query_id'].keys());\n",
    "        shuffle(qids);\n",
    "        train_size = len(qids)\n",
    "        train_size = int(reduced_whole_size * train_size)\n",
    "        new_train_size = int((1 - percentual_size_test) * train_size)\n",
    "        new_test_size = train_size - new_train_size\n",
    "        print(\"New Train Set has {} Datapoints\".format(new_train_size))\n",
    "        print(\"New Test Set has {} Datapoints\".format(new_test_size))\n",
    "\n",
    "        \n",
    "        qids_train = qids[0:new_train_size]\n",
    "        qids_test = qids[new_train_size:train_size]\n",
    "        \n",
    "        def copy_dict_part(old_dict, qids):\n",
    "            count = 0;\n",
    "            new_dict = dict()\n",
    "            keys = old_dict.keys()\n",
    "            for qid in qids:\n",
    "                count = count + 1;\n",
    "                if count % 10000 == 0:\n",
    "                    print('Copy progress: {}'.format(count/len(qids)))\n",
    "                for key in keys:\n",
    "                    if not key in new_dict:\n",
    "                        new_dict[key] = dict()\n",
    "                    new_dict[key][qid] = train_json[key][qid]\n",
    "            return new_dict;\n",
    "        \n",
    "        print('Start creating new train set:')\n",
    "        new_train = copy_dict_part(train_json, qids_train)\n",
    "        print('Start creating new test set:')\n",
    "        new_test = copy_dict_part(train_json, qids_test)\n",
    "        \n",
    "        with open(args.train_splitted_data, 'w') as write_f:\n",
    "            write_f.write(json.dumps(new_train))\n",
    "        with open(args.test_splitted_data, 'w') as write_f:\n",
    "            write_f.write(json.dumps(new_test))\n",
    "        \n",
    "        create_reference_file(new_test, args.test_reference_file)\n",
    "            \n",
    "        with open(args.val_data) as f_o:\n",
    "            val_json = json.load(f_o);\n",
    "        qids = list(val_json['query_id'].keys())\n",
    "        shuffle(qids)\n",
    "        val_size = len(qids)\n",
    "        new_val_size = int(reduced_whole_size * val_size)\n",
    "        print(\"New Validation Set has {} Datapoints\".format(new_val_size))\n",
    "\n",
    "        qids_val = qids[0:new_val_size]\n",
    "        print('Start creating new val set:')\n",
    "        new_val = copy_dict_part(val_json, qids_val)\n",
    "        with open(args.val_splitted_data, 'w') as write_f:\n",
    "            write_f.write(json.dumps(new_val))\n",
    "            \n",
    "def load_data(path,limit):\n",
    "    with open(path) as f_o:\n",
    "        data, _ = dataset.load_data(json.load(f_o), span_only=True, answered_only=True, loading_limit=limit)\n",
    "    return data\n",
    "\n",
    "def create_reference_file(data_obj, reference_file_path):\n",
    "        print(\"Create test reference file\")\n",
    "        with open(reference_file_path, 'w+') as write_f:\n",
    "            for qid in data_obj[\"answers\"]:\n",
    "                try:\n",
    "                    correct = {\"query_id\": str(qid)}\n",
    "                    correct[\"answers\"] = data_obj[\"answers\"][str(qid)]\n",
    "                    write_f.write(json.dumps(correct))\n",
    "                    write_f.write(\"\\n\")\n",
    "                except KeyError:\n",
    "                    print(\"Key Error: \"+str(obj[\"query_id\"]))\n",
    "        print(\"Done creating reference file\")\n",
    "\n",
    "def init_model(id_to_token, id_to_char):\n",
    "    return mrcqa.BidafModel.from_config(config['bidaf'], id_to_token, id_to_char)\n",
    "\n",
    "def reload_model(checkpoint):\n",
    "    model, id_to_token, id_to_char = mrcqa.BidafModel.from_checkpoint(config['bidaf'], checkpoint)\n",
    "    if torch.cuda.is_available() and args.cuda:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "    return model, id_to_token, id_to_char\n",
    "\n",
    "def inverse_dict(base_dict):\n",
    "    return {tok: id_ for id_, tok in base_dict.items()}\n",
    "\n",
    "def get_loader(data, config, used_data_per_batch=1.0):\n",
    "    data = dataset.EpochGen(\n",
    "        data,\n",
    "        batch_size=config.get('training', {}).get('batch_size', 32),\n",
    "        shuffle=True,\n",
    "        used_data_per_batch=used_data_per_batch)\n",
    "    return data\n",
    "\n",
    "def get_optimizer(model, config, state):\n",
    "    \"\"\"\n",
    "    Get the optimizer\n",
    "    \"\"\"\n",
    "    parameters = filter(lambda p: p.requires_grad,\n",
    "                        model.parameters())\n",
    "    \"\"\" ADAM Optimizer\n",
    "    optimizer = torch.optim.Adam(\n",
    "        parameters,\n",
    "        lr=config['training'].get('lr', 0.01),\n",
    "        betas=config['training'].get('betas', (0.9, 0.999)),\n",
    "        eps=config['training'].get('eps', 1e-8),\n",
    "        weight_decay=config['training'].get('weight_decay', 0))\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" ADAGRAD Optimizer\n",
    "    optimizer = torch.optim.Adagrad(\n",
    "        parameters,\n",
    "        lr=config['training'].get('lr', 0.01),\n",
    "        weight_decay=config['training'].get('weight_decay', 0))\n",
    "    \"\"\" \n",
    "    \"\"\" ADADELTA Optimizer \"\"\"\n",
    "    optimizer = torch.optim.Adadelta(\n",
    "        parameters,\n",
    "        lr=0.5)\n",
    "    \n",
    "    if state is not None:\n",
    "        optimizer.load_state_dict(state)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "def load_pretrained_embeddings(path, model, id_to_token):\n",
    "    with open(path) as f_o:\n",
    "            pre_trained = dataset.SymbolEmbSourceText(f_o, set(tok for id_, tok in id_to_token.items() if id_ != 0))\n",
    "    mean, cov = pre_trained.get_norm_stats(args.use_covariance)\n",
    "    rng = np.random.RandomState(2)\n",
    "    oovs = dataset.SymbolEmbSourceNorm(mean, cov, rng, args.use_covariance)\n",
    "    model.embedder.embeddings[0].embeddings.weight.data = torch.from_numpy(dataset.symbol_injection(id_to_token, 0, model.embedder.embeddings[0].embeddings.weight.data.numpy(), pre_trained, oovs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_init(train_path, val_path, test_path, config, args, loading_limit=None, used_data_per_train_batch=1.0):\n",
    "    token_to_id = {'': 0}\n",
    "    char_to_id = {'': 0}\n",
    "    print('Load Train Data [1/6]')\n",
    "    train_data = load_data(train_path,loading_limit)\n",
    "    print('Load Validation Data [1/6]')\n",
    "    val_data = load_data(val_path,loading_limit)\n",
    "    print('Load Test Data [1/6]')\n",
    "    test_data = load_data(test_path,loading_limit)\n",
    "    \n",
    "    print('Tokenize Train Data [2/6]')\n",
    "    train_data = dataset.tokenize_data(train_data, token_to_id, char_to_id)\n",
    "    print('Tokenize Validation Data [2/6]')\n",
    "    val_data = dataset.tokenize_data(val_data, token_to_id, char_to_id)\n",
    "    print('Tokenize Test Data [2/6]')\n",
    "    test_data = dataset.tokenize_data(test_data, token_to_id, char_to_id)\n",
    "    \n",
    "    train_loader = get_loader(train_data, config, used_data_per_batch=used_data_per_train_batch)\n",
    "    val_loader = get_loader(val_data, config, used_data_per_batch=used_data_per_train_batch)\n",
    "    test_loader = get_loader(test_data, config)\n",
    "\n",
    "    print('Create Inverse Dictionaries [3/6]')\n",
    "    id_to_token = inverse_dict(token_to_id)\n",
    "    id_to_char = inverse_dict(char_to_id)\n",
    "\n",
    "    print('Initiate Model [4/6]')\n",
    "    model = init_model(id_to_token, id_to_char)\n",
    "\n",
    "    if args.word_rep:\n",
    "        print('Load pre-trained embeddings [5/6]')\n",
    "        load_pretrained_embeddings(args.word_rep, model, id_to_token)\n",
    "    else:\n",
    "        print('No pre-trained embeddings given [5/6]')\n",
    "        pass  # No pretraining, just keep the random values.\n",
    "\n",
    "    if torch.cuda.is_available() and args.cuda:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = get_optimizer(model, config, state=None)\n",
    "    print('Done init_state [6/6]')\n",
    "    return model, id_to_token, id_to_char, optimizer, train_loader, val_loader, test_loader   \n",
    "\n",
    "\n",
    "def new_reload(train_path, val_path, test_path, checkpoint, training_state, config, args,loading_limit=None, used_data_per_train_batch=1.0):\n",
    "    print('Load Model from Checkpoint [1/5]')\n",
    "    model, id_to_token, id_to_char = reload_model(checkpoint)\n",
    "\n",
    "    optimizer = get_optimizer(model, config, training_state)\n",
    "\n",
    "    print('Create Inverse Dictionaries [2/5]')\n",
    "    token_to_id = inverse_dict(id_to_token)\n",
    "    char_to_id = inverse_dict(id_to_char)\n",
    "\n",
    "    len_tok_voc = len(token_to_id)\n",
    "    len_char_voc = len(char_to_id)\n",
    "\n",
    "    print('Load Train Data [3/5]')\n",
    "    train_data = load_data(train_path,loading_limit)\n",
    "    print('Load Validation Data [3/5]')\n",
    "    val_data = load_data(val_path,loading_limit)\n",
    "    print('Load Test Data [3/5]')\n",
    "    test_data = load_data(test_path,loading_limit)\n",
    "    \n",
    "    limit_passage = config.get('training', {}).get('limit')\n",
    "\n",
    "    print('Tokenize Train Data [4/5]')\n",
    "    train_data = dataset.tokenize_data(train_data, token_to_id, char_to_id)\n",
    "    print('Tokenize Validation Data [4/5]')\n",
    "    val_data = dataset.tokenize_data(val_data, token_to_id, char_to_id)\n",
    "    print('Tokenize Test Data [4/5]')\n",
    "    test_data = dataset.tokenize_data(test_data, token_to_id, char_to_id)\n",
    "\n",
    "    train_loader = get_loader(train_data, config, used_data_per_batch=used_data_per_train_batch)\n",
    "    val_loader = get_loader(val_data, config)\n",
    "    test_loader = get_loader(test_data, config)\n",
    "\n",
    "    assert len(token_to_id) == len_tok_voc\n",
    "    assert len(char_to_id) == len_char_voc\n",
    "\n",
    "    print('Done reload_state [5/5]')\n",
    "    return model, id_to_token, id_to_char, optimizer, train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch Lightning Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to train...\n",
      "Load Train Data [1/6]\n",
      "Start Organizing Data...\n",
      "Organizing progress: 0.0 x 10⁴\n",
      "Organizing progress: 1.0 x 10⁴\n",
      "Organizing progress: 2.0 x 10⁴\n",
      "Organizing progress: 3.0 x 10⁴\n",
      "Organizing progress: 4.0 x 10⁴\n",
      "Organizing progress: 5.0 x 10⁴\n",
      "Organizing progress: 6.0 x 10⁴\n",
      "Organizing progress: 7.0 x 10⁴\n",
      "Organizing progress: 8.0 x 10⁴\n",
      "Organizing progress: 9.0 x 10⁴\n",
      "Organizing progress: 10.0 x 10⁴\n",
      "Organizing progress: 11.0 x 10⁴\n",
      "Organizing progress: 12.0 x 10⁴\n",
      "Organizing progress: 13.0 x 10⁴\n",
      "Organizing progress: 14.0 x 10⁴\n",
      "Organizing progress: 15.0 x 10⁴\n",
      "Organizing progress: 16.0 x 10⁴\n",
      "Organizing progress: 17.0 x 10⁴\n",
      "Organizing progress: 18.0 x 10⁴\n",
      "Organizing progress: 19.0 x 10⁴\n",
      "Organizing progress: 20.0 x 10⁴\n",
      "Organizing progress: 21.0 x 10⁴\n",
      "Organizing progress: 22.0 x 10⁴\n",
      "Organizing progress: 23.0 x 10⁴\n",
      "Organizing progress: 24.0 x 10⁴\n",
      "Organizing progress: 25.0 x 10⁴\n",
      "Organizing progress: 26.0 x 10⁴\n",
      "Organizing progress: 27.0 x 10⁴\n",
      "Organizing progress: 28.0 x 10⁴\n",
      "Organizing progress: 29.0 x 10⁴\n",
      "Organizing progress: 30.0 x 10⁴\n",
      "Organizing progress: 31.0 x 10⁴\n",
      "Organizing progress: 32.0 x 10⁴\n",
      "Organizing progress: 33.0 x 10⁴\n",
      "Organizing progress: 34.0 x 10⁴\n",
      "Organizing progress: 35.0 x 10⁴\n",
      "Organizing progress: 36.0 x 10⁴\n",
      "Organizing progress: 37.0 x 10⁴\n",
      "Organizing progress: 38.0 x 10⁴\n",
      "Organizing progress: 39.0 x 10⁴\n",
      "Organizing progress: 40.0 x 10⁴\n",
      "Organizing progress: 41.0 x 10⁴\n",
      "Organizing progress: 42.0 x 10⁴\n",
      "Organizing progress: 43.0 x 10⁴\n",
      "Organizing progress: 44.0 x 10⁴\n",
      "Organizing progress: 45.0 x 10⁴\n",
      "Organizing progress: 46.0 x 10⁴\n",
      "Organizing progress: 47.0 x 10⁴\n",
      "Organizing progress: 48.0 x 10⁴\n",
      "Organizing progress: 49.0 x 10⁴\n",
      "Organizing progress: 50.0 x 10⁴\n",
      "Organizing progress: 51.0 x 10⁴\n",
      "Organizing progress: 52.0 x 10⁴\n",
      "Organizing progress: 53.0 x 10⁴\n",
      "Organizing progress: 54.0 x 10⁴\n",
      "Organizing progress: 55.0 x 10⁴\n",
      "Organizing progress: 56.0 x 10⁴\n",
      "Organizing progress: 57.0 x 10⁴\n",
      "Organizing progress: 58.0 x 10⁴\n",
      "Organizing progress: 59.0 x 10⁴\n",
      "Organizing progress: 60.0 x 10⁴\n",
      "Organizing progress: 61.0 x 10⁴\n",
      "Organizing progress: 62.0 x 10⁴\n",
      "Organizing progress: 63.0 x 10⁴\n",
      "Organizing progress: 64.0 x 10⁴\n",
      "Organizing progress: 65.0 x 10⁴\n",
      "Organizing progress: 66.0 x 10⁴\n",
      "Organizing progress: 67.0 x 10⁴\n",
      "Organizing progress: 68.0 x 10⁴\n",
      "Organizing progress: 69.0 x 10⁴\n",
      "Organizing progress: 70.0 x 10⁴\n",
      "Organizing progress: 71.0 x 10⁴\n",
      "Organizing progress: 72.0 x 10⁴\n",
      "Load Validation Data [1/6]\n",
      "Start Organizing Data...\n",
      "Organizing progress: 0.0 x 10⁴\n",
      "Organizing progress: 1.0 x 10⁴\n",
      "Organizing progress: 2.0 x 10⁴\n",
      "Organizing progress: 3.0 x 10⁴\n",
      "Organizing progress: 4.0 x 10⁴\n",
      "Organizing progress: 5.0 x 10⁴\n",
      "Organizing progress: 6.0 x 10⁴\n",
      "Organizing progress: 7.0 x 10⁴\n",
      "Organizing progress: 8.0 x 10⁴\n",
      "Organizing progress: 9.0 x 10⁴\n",
      "Organizing progress: 10.0 x 10⁴\n",
      "Load Test Data [1/6]\n",
      "Start Organizing Data...\n",
      "Organizing progress: 0.0 x 10⁴\n",
      "Organizing progress: 1.0 x 10⁴\n",
      "Organizing progress: 2.0 x 10⁴\n",
      "Organizing progress: 3.0 x 10⁴\n",
      "Organizing progress: 4.0 x 10⁴\n",
      "Organizing progress: 5.0 x 10⁴\n",
      "Organizing progress: 6.0 x 10⁴\n",
      "Organizing progress: 7.0 x 10⁴\n",
      "Organizing progress: 8.0 x 10⁴\n",
      "Tokenize Train Data [2/6]\n",
      "0.0 x 10⁴/31.5261 x 10⁴\n",
      "1.0 x 10⁴/31.5261 x 10⁴\n",
      "2.0 x 10⁴/31.5261 x 10⁴\n",
      "3.0 x 10⁴/31.5261 x 10⁴\n",
      "4.0 x 10⁴/31.5261 x 10⁴\n",
      "5.0 x 10⁴/31.5261 x 10⁴\n",
      "6.0 x 10⁴/31.5261 x 10⁴\n",
      "7.0 x 10⁴/31.5261 x 10⁴\n",
      "8.0 x 10⁴/31.5261 x 10⁴\n",
      "9.0 x 10⁴/31.5261 x 10⁴\n",
      "10.0 x 10⁴/31.5261 x 10⁴\n",
      "11.0 x 10⁴/31.5261 x 10⁴\n",
      "12.0 x 10⁴/31.5261 x 10⁴\n",
      "13.0 x 10⁴/31.5261 x 10⁴\n",
      "14.0 x 10⁴/31.5261 x 10⁴\n",
      "15.0 x 10⁴/31.5261 x 10⁴\n",
      "16.0 x 10⁴/31.5261 x 10⁴\n",
      "17.0 x 10⁴/31.5261 x 10⁴\n",
      "18.0 x 10⁴/31.5261 x 10⁴\n",
      "19.0 x 10⁴/31.5261 x 10⁴\n",
      "20.0 x 10⁴/31.5261 x 10⁴\n",
      "21.0 x 10⁴/31.5261 x 10⁴\n",
      "22.0 x 10⁴/31.5261 x 10⁴\n",
      "23.0 x 10⁴/31.5261 x 10⁴\n",
      "24.0 x 10⁴/31.5261 x 10⁴\n",
      "25.0 x 10⁴/31.5261 x 10⁴\n",
      "26.0 x 10⁴/31.5261 x 10⁴\n",
      "27.0 x 10⁴/31.5261 x 10⁴\n",
      "28.0 x 10⁴/31.5261 x 10⁴\n",
      "29.0 x 10⁴/31.5261 x 10⁴\n",
      "30.0 x 10⁴/31.5261 x 10⁴\n",
      "31.0 x 10⁴/31.5261 x 10⁴\n",
      "Tokenize Validation Data [2/6]\n",
      "0.0 x 10⁴/3.114 x 10⁴\n",
      "1.0 x 10⁴/3.114 x 10⁴\n",
      "2.0 x 10⁴/3.114 x 10⁴\n",
      "3.0 x 10⁴/3.114 x 10⁴\n",
      "Tokenize Test Data [2/6]\n",
      "0.0 x 10⁴/3.4979 x 10⁴\n",
      "1.0 x 10⁴/3.4979 x 10⁴\n",
      "2.0 x 10⁴/3.4979 x 10⁴\n",
      "3.0 x 10⁴/3.4979 x 10⁴\n",
      "Create Inverse Dictionaries [3/6]\n",
      "Initiate Model [4/6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages/torch/nn/modules/rnn.py:47: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-trained embeddings [5/6]\n",
      "Embeddings Loaded: 0.0 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.1 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.2 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.3 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.4 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.5 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.6 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.7 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.8 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 0.9 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.0 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.1 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.2 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.3 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.4 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.5 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.6 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.7 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.8 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 1.9 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 2.0 Mio / 2.1 Mio\n",
      "Embeddings Loaded: 2.1 Mio / 2.1 Mio\n",
      "Done init_state [6/6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-d0f8574f1204>:15: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  checkpoint_w = h5py.File(os.path.join(args.exp_folder, 'checkpoint'))\n"
     ]
    }
   ],
   "source": [
    "try_to_split_testset(0.1,1, False); #We use 100% of the given Data. And 10% of the Training Data will be used as Test Data. True means force rewrite Datasets.\n",
    "\n",
    "checkpoint_w, training_state_w, epoch_w = train_manager.try_to_resume(\n",
    "            args.force_restart, args.exp_folder)\n",
    "\n",
    "if checkpoint_w:\n",
    "    print('Resuming training...')\n",
    "    model_w, id_to_token_w, id_to_char_w, optimizer_w, train_loader, val_loader, test_loader = new_reload(args.train_splitted_data, args.val_splitted_data, \n",
    "                                                                                                          args.test_splitted_data, checkpoint_w, \n",
    "                                                                                                          training_state_w, config, args, used_data_per_train_batch=0.1)\n",
    "else:\n",
    "    print('Preparing to train...')\n",
    "    model_w, id_to_token_w, id_to_char_w, optimizer_w, train_loader, val_loader, test_loader = new_init(args.train_splitted_data, args.val_splitted_data, \n",
    "                                                                                                        args.test_splitted_data,config, args, used_data_per_train_batch=0.1)\n",
    "    checkpoint_w = h5py.File(os.path.join(args.exp_folder, 'checkpoint'))\n",
    "    checkpointing.save_vocab(checkpoint_w, 'vocab', id_to_token_w)\n",
    "    checkpointing.save_vocab(checkpoint_w, 'c_vocab', id_to_char_w)\n",
    "\n",
    "if torch.cuda.is_available() and args.cuda:\n",
    "    train_loader.tensor_type = torch.cuda.LongTensor\n",
    "    val_loader.tensor_type = torch.cuda.LongTensor\n",
    "    test_loader.tensor_type = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast change Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_w = get_optimizer(model_w, config, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_saves = dict();\n",
    "epoch_saves['train_loss'] = []\n",
    "epoch_saves['val_loss'] = []\n",
    "epoch_saves['test_loss'] = []\n",
    "\n",
    "#Used for test evaluation\n",
    "qid2candidate = {}\n",
    "\n",
    "import re\n",
    "regex_drop_char = re.compile('[^a-z0-9\\s]+')\n",
    "regex_multi_space = re.compile('\\s+')\n",
    "\n",
    "class BidafLightningWrapper(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def setup(self,stage):\n",
    "        pass;\n",
    "            \n",
    "    def prepare_data(self):\n",
    "        pass;\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optimizer_w;\n",
    "\n",
    "    def forward(self, passage, p_lengths, question, q_lengths):\n",
    "        return model_w(passage, p_lengths, question, q_lengths)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_loader;\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return val_loader;\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return test_loader;\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        qids, passages, queries, answers, _ = batch\n",
    "        start_log_probs, end_log_probs = self(passages[:2], passages[2], queries[:2], queries[2])\n",
    "        loss = model_w.get_loss(start_log_probs, end_log_probs, answers[:, 0], answers[:, 1])\n",
    "        return {'loss': loss, 'train_loss': loss, 'log': {'train_loss': loss}}\n",
    "\n",
    "    def training_epoch_end(self, results):\n",
    "        checkpointing.checkpoint(model_w, epoch_w, optimizer_w, checkpoint_w, args.exp_folder)\n",
    "        model_w.cuda()\n",
    "        mean_loss = self.save_statistics('train',results)\n",
    "        return {'log': {'train_loss': mean_loss}}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        qids, passages, queries, answers, _ = batch\n",
    "        start_log_probs, end_log_probs = self(passages[:2], passages[2], queries[:2], queries[2])\n",
    "        loss = model_w.get_loss(start_log_probs, end_log_probs, answers[:, 0], answers[:, 1])\n",
    "        return {'val_loss': loss, 'log': {'val_loss': loss}}\n",
    "    \n",
    "    def validation_epoch_end(self, results):\n",
    "        val_loss_mean = self.save_statistics('val',results)\n",
    "        return {'val_loss': val_loss_mean}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        qids, passages, queries, answers, mappings = batch\n",
    "        start_log_probs, end_log_probs = self(passages[:2], passages[2], queries[:2], queries[2])\n",
    "        loss = model_w.get_loss(start_log_probs, end_log_probs, answers[:, 0], answers[:, 1])\n",
    "\n",
    "        predictions = model_w.get_best_span(start_log_probs, end_log_probs)\n",
    "        predictions = predictions.cpu()\n",
    "        passages = passages[0].cpu().data\n",
    "        for qid, mapping, tokens, pred in zip(qids, mappings, passages, predictions):\n",
    "            toks = tokens[pred[0]:pred[1]]\n",
    "            start = mapping[pred[0], 0]\n",
    "            end = mapping[pred[1]-1, 1]\n",
    "            toks = regex_multi_space.sub(' ', regex_drop_char.sub(' ', ' '.join(id_to_token_w[int(tok)] for tok in toks).lower())).strip()\n",
    "            if qid not in qid2candidate:\n",
    "                qid2candidate[qid] = []\n",
    "            qid2candidate[qid].append(str(toks))\n",
    "        return {'test_loss': loss}\n",
    "\n",
    "    def test_epoch_end(self, results):\n",
    "        no_ans_set = set()\n",
    "        out_dict = {}\n",
    "        \n",
    "        #print(\"\\t no answer set\")\n",
    "        for qid in qid2candidate:\n",
    "            if len(qid2candidate[qid]) < 1 or 'No Answer Present.' in qid2candidate[qid]:\n",
    "                no_ans_set.add(qid)\n",
    "        #print(\"\\t take random answer from possible ones\")\n",
    "        for qid in qid2candidate:\n",
    "            pick = randint(0,len(qid2candidate[qid])-1)\n",
    "            out_dict[qid] = [qid2candidate[qid][pick]]\n",
    "        \n",
    "        mean_test_loss = self.save_statistics('test',results)\n",
    "        test_metrics = eval_manager.compute_metrics_from_model(args.test_reference_file, out_dict, no_ans_set)\n",
    "        outputfile = os.path.join(args.exp_folder,'metrics.json')\n",
    "        with open(outputfile,'w+') as f_o:\n",
    "            f_o.write(json.dumps(test_metrics))\n",
    "        return {'log': {'test_loss': mean_test_loss}.update(test_metrics)}\n",
    "\n",
    "    def save_statistics(self, phase, results):\n",
    "        key = phase + '_loss'\n",
    "        mean_loss = torch.stack([step[key] for step in results]).mean()\n",
    "        print(\"Mean {} Loss: {}\".format(phase,mean_loss))\n",
    "        #print(epoch_saves.keys())\n",
    "        epoch_saves[key].append([step[key] for step in results])\n",
    "        return mean_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start new Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 100%|██████████| 2/2 [00:00<00:00,  3.36it/s]Mean val Loss: 8.279254913330078\n",
      "Epoch 1:  91%|█████████ | 525/576 [03:01<00:17,  2.89it/s, loss=3.990, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  91%|█████████▏| 526/576 [03:01<00:17,  2.89it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  91%|█████████▏| 527/576 [03:02<00:16,  2.89it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 528/576 [03:02<00:16,  2.90it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 529/576 [03:02<00:16,  2.90it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 530/576 [03:02<00:15,  2.90it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 531/576 [03:03<00:15,  2.90it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 532/576 [03:03<00:15,  2.90it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 533/576 [03:03<00:14,  2.90it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 534/576 [03:03<00:14,  2.91it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 535/576 [03:03<00:14,  2.91it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 536/576 [03:04<00:13,  2.91it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 537/576 [03:04<00:13,  2.91it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 538/576 [03:04<00:13,  2.91it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  94%|█████████▎| 539/576 [03:04<00:12,  2.92it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 540/576 [03:05<00:12,  2.92it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 541/576 [03:05<00:11,  2.92it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 542/576 [03:05<00:11,  2.92it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 543/576 [03:05<00:11,  2.92it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 544/576 [03:06<00:10,  2.92it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 545/576 [03:06<00:10,  2.92it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 546/576 [03:06<00:10,  2.93it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 547/576 [03:06<00:09,  2.93it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 548/576 [03:07<00:09,  2.93it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 549/576 [03:07<00:09,  2.93it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 550/576 [03:07<00:08,  2.93it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 551/576 [03:07<00:08,  2.93it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 552/576 [03:08<00:08,  2.93it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 553/576 [03:08<00:07,  2.93it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 554/576 [03:08<00:07,  2.94it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  96%|█████████▋| 555/576 [03:08<00:07,  2.94it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 556/576 [03:09<00:06,  2.94it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 557/576 [03:09<00:06,  2.94it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 558/576 [03:09<00:06,  2.94it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 559/576 [03:09<00:05,  2.94it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 560/576 [03:10<00:05,  2.95it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 561/576 [03:10<00:05,  2.95it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 562/576 [03:10<00:04,  2.95it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 563/576 [03:10<00:04,  2.95it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 564/576 [03:11<00:04,  2.95it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 565/576 [03:11<00:03,  2.95it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 566/576 [03:11<00:03,  2.95it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 567/576 [03:11<00:03,  2.96it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  99%|█████████▊| 568/576 [03:12<00:02,  2.96it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 569/576 [03:12<00:02,  2.96it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 570/576 [03:12<00:02,  2.96it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 571/576 [03:12<00:01,  2.96it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 572/576 [03:13<00:01,  2.96it/s, loss=3.990, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 573/576 [03:13<00:01,  2.97it/s, loss=3.990, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 574/576 [03:13<00:00,  2.97it/s, loss=3.990, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 575/576 [03:13<00:00,  2.97it/s, loss=3.990, v_num=0]\n",
      "Epoch 1: 100%|██████████| 576/576 [03:13<00:00,  2.97it/s, loss=3.990, v_num=0]Mean val Loss: 3.3787872791290283\n",
      "Epoch 1: 100%|██████████| 576/576 [03:14<00:00,  2.97it/s, loss=3.990, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 4.597168922424316\n",
      "Epoch 2:  91%|█████████ | 525/576 [03:03<00:17,  2.85it/s, loss=3.491, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2:  91%|█████████▏| 526/576 [03:04<00:17,  2.86it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  91%|█████████▏| 527/576 [03:04<00:17,  2.86it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 528/576 [03:04<00:16,  2.86it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 529/576 [03:04<00:16,  2.86it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 530/576 [03:05<00:16,  2.86it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 531/576 [03:05<00:15,  2.87it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 532/576 [03:05<00:15,  2.87it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 533/576 [03:05<00:14,  2.87it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 534/576 [03:05<00:14,  2.87it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 535/576 [03:06<00:14,  2.87it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 536/576 [03:06<00:13,  2.87it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 537/576 [03:06<00:13,  2.88it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 538/576 [03:06<00:13,  2.88it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  94%|█████████▎| 539/576 [03:07<00:12,  2.88it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 540/576 [03:07<00:12,  2.88it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 541/576 [03:07<00:12,  2.88it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 542/576 [03:08<00:11,  2.88it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 543/576 [03:08<00:11,  2.88it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 544/576 [03:08<00:11,  2.89it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 545/576 [03:08<00:10,  2.89it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 546/576 [03:08<00:10,  2.89it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 547/576 [03:09<00:10,  2.89it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 548/576 [03:09<00:09,  2.89it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 549/576 [03:09<00:09,  2.89it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 550/576 [03:10<00:08,  2.89it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 551/576 [03:10<00:08,  2.90it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 552/576 [03:10<00:08,  2.90it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 553/576 [03:10<00:07,  2.90it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 554/576 [03:10<00:07,  2.90it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  96%|█████████▋| 555/576 [03:11<00:07,  2.90it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 556/576 [03:11<00:06,  2.90it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 557/576 [03:11<00:06,  2.90it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 558/576 [03:12<00:06,  2.91it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 559/576 [03:12<00:05,  2.91it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 560/576 [03:12<00:05,  2.91it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 561/576 [03:12<00:05,  2.91it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 562/576 [03:13<00:04,  2.91it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 563/576 [03:13<00:04,  2.91it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 564/576 [03:13<00:04,  2.91it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 565/576 [03:13<00:03,  2.92it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 566/576 [03:13<00:03,  2.92it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 567/576 [03:14<00:03,  2.92it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  99%|█████████▊| 568/576 [03:14<00:02,  2.92it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 569/576 [03:14<00:02,  2.92it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 570/576 [03:15<00:02,  2.92it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 571/576 [03:15<00:01,  2.92it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 572/576 [03:15<00:01,  2.93it/s, loss=3.491, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 573/576 [03:15<00:01,  2.93it/s, loss=3.491, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 574/576 [03:16<00:00,  2.93it/s, loss=3.491, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 575/576 [03:16<00:00,  2.93it/s, loss=3.491, v_num=0]\n",
      "Epoch 2: 100%|██████████| 576/576 [03:16<00:00,  2.93it/s, loss=3.491, v_num=0]Mean val Loss: 3.1840062141418457\n",
      "Epoch 2: 100%|██████████| 576/576 [03:16<00:00,  2.93it/s, loss=3.491, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 3.7161266803741455\n",
      "Epoch 3:  91%|█████████ | 525/576 [03:03<00:17,  2.86it/s, loss=3.270, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3:  91%|█████████▏| 526/576 [03:03<00:17,  2.86it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  91%|█████████▏| 527/576 [03:03<00:17,  2.87it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  92%|█████████▏| 528/576 [03:04<00:16,  2.87it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  92%|█████████▏| 529/576 [03:04<00:16,  2.87it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  92%|█████████▏| 530/576 [03:04<00:16,  2.87it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  92%|█████████▏| 531/576 [03:04<00:15,  2.87it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  92%|█████████▏| 532/576 [03:05<00:15,  2.87it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  93%|█████████▎| 533/576 [03:05<00:14,  2.87it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  93%|█████████▎| 534/576 [03:05<00:14,  2.87it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  93%|█████████▎| 535/576 [03:05<00:14,  2.88it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  93%|█████████▎| 536/576 [03:06<00:13,  2.88it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  93%|█████████▎| 537/576 [03:06<00:13,  2.88it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  93%|█████████▎| 538/576 [03:06<00:13,  2.88it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  94%|█████████▎| 539/576 [03:06<00:12,  2.88it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  94%|█████████▍| 540/576 [03:07<00:12,  2.88it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  94%|█████████▍| 541/576 [03:07<00:12,  2.88it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  94%|█████████▍| 542/576 [03:07<00:11,  2.89it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  94%|█████████▍| 543/576 [03:08<00:11,  2.89it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  94%|█████████▍| 544/576 [03:08<00:11,  2.89it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  95%|█████████▍| 545/576 [03:08<00:10,  2.89it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  95%|█████████▍| 546/576 [03:08<00:10,  2.89it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  95%|█████████▍| 547/576 [03:09<00:10,  2.89it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  95%|█████████▌| 548/576 [03:09<00:09,  2.89it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  95%|█████████▌| 549/576 [03:09<00:09,  2.89it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  95%|█████████▌| 550/576 [03:09<00:08,  2.90it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 551/576 [03:10<00:08,  2.90it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 552/576 [03:10<00:08,  2.90it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 553/576 [03:10<00:07,  2.90it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 554/576 [03:11<00:07,  2.90it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  96%|█████████▋| 555/576 [03:11<00:07,  2.90it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  97%|█████████▋| 556/576 [03:11<00:06,  2.90it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  97%|█████████▋| 557/576 [03:11<00:06,  2.90it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  97%|█████████▋| 558/576 [03:12<00:06,  2.91it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  97%|█████████▋| 559/576 [03:12<00:05,  2.91it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  97%|█████████▋| 560/576 [03:12<00:05,  2.91it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  97%|█████████▋| 561/576 [03:12<00:05,  2.91it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  98%|█████████▊| 562/576 [03:12<00:04,  2.91it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  98%|█████████▊| 563/576 [03:13<00:04,  2.91it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  98%|█████████▊| 564/576 [03:13<00:04,  2.92it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  98%|█████████▊| 565/576 [03:13<00:03,  2.92it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  98%|█████████▊| 566/576 [03:14<00:03,  2.92it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  98%|█████████▊| 567/576 [03:14<00:03,  2.92it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  99%|█████████▊| 568/576 [03:14<00:02,  2.92it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  99%|█████████▉| 569/576 [03:14<00:02,  2.92it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  99%|█████████▉| 570/576 [03:15<00:02,  2.92it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  99%|█████████▉| 571/576 [03:15<00:01,  2.92it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  99%|█████████▉| 572/576 [03:15<00:01,  2.92it/s, loss=3.270, v_num=0]\n",
      "Epoch 3:  99%|█████████▉| 573/576 [03:15<00:01,  2.92it/s, loss=3.270, v_num=0]\n",
      "Epoch 3: 100%|█████████▉| 574/576 [03:16<00:00,  2.92it/s, loss=3.270, v_num=0]\n",
      "Epoch 3: 100%|█████████▉| 575/576 [03:16<00:00,  2.93it/s, loss=3.270, v_num=0]\n",
      "Epoch 3: 100%|██████████| 576/576 [03:16<00:00,  2.93it/s, loss=3.270, v_num=0]Mean val Loss: 2.86798095703125\n",
      "Epoch 3: 100%|██████████| 576/576 [03:16<00:00,  2.92it/s, loss=3.270, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 3.3076510429382324\n",
      "Epoch 4:  91%|█████████ | 525/576 [03:06<00:18,  2.82it/s, loss=3.079, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4:  91%|█████████▏| 526/576 [03:06<00:17,  2.82it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  91%|█████████▏| 527/576 [03:06<00:17,  2.82it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  92%|█████████▏| 528/576 [03:07<00:17,  2.82it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  92%|█████████▏| 529/576 [03:07<00:16,  2.82it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  92%|█████████▏| 530/576 [03:07<00:16,  2.83it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  92%|█████████▏| 531/576 [03:07<00:15,  2.83it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  92%|█████████▏| 532/576 [03:08<00:15,  2.83it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  93%|█████████▎| 533/576 [03:08<00:15,  2.83it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  93%|█████████▎| 534/576 [03:08<00:14,  2.83it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  93%|█████████▎| 535/576 [03:08<00:14,  2.83it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  93%|█████████▎| 536/576 [03:08<00:14,  2.84it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  93%|█████████▎| 537/576 [03:09<00:13,  2.84it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  93%|█████████▎| 538/576 [03:09<00:13,  2.84it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  94%|█████████▎| 539/576 [03:09<00:13,  2.84it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  94%|█████████▍| 540/576 [03:10<00:12,  2.84it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  94%|█████████▍| 541/576 [03:10<00:12,  2.84it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  94%|█████████▍| 542/576 [03:10<00:11,  2.84it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  94%|█████████▍| 543/576 [03:10<00:11,  2.85it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  94%|█████████▍| 544/576 [03:11<00:11,  2.85it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  95%|█████████▍| 545/576 [03:11<00:10,  2.85it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  95%|█████████▍| 546/576 [03:11<00:10,  2.85it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  95%|█████████▍| 547/576 [03:11<00:10,  2.85it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  95%|█████████▌| 548/576 [03:12<00:09,  2.85it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  95%|█████████▌| 549/576 [03:12<00:09,  2.85it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  95%|█████████▌| 550/576 [03:12<00:09,  2.85it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 551/576 [03:12<00:08,  2.86it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 552/576 [03:13<00:08,  2.86it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 553/576 [03:13<00:08,  2.86it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 554/576 [03:13<00:07,  2.86it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  96%|█████████▋| 555/576 [03:13<00:07,  2.86it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  97%|█████████▋| 556/576 [03:14<00:06,  2.86it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  97%|█████████▋| 557/576 [03:14<00:06,  2.86it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  97%|█████████▋| 558/576 [03:14<00:06,  2.86it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  97%|█████████▋| 559/576 [03:15<00:05,  2.87it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  97%|█████████▋| 560/576 [03:15<00:05,  2.87it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  97%|█████████▋| 561/576 [03:15<00:05,  2.87it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  98%|█████████▊| 562/576 [03:15<00:04,  2.87it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  98%|█████████▊| 563/576 [03:16<00:04,  2.87it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  98%|█████████▊| 564/576 [03:16<00:04,  2.87it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  98%|█████████▊| 565/576 [03:16<00:03,  2.88it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  98%|█████████▊| 566/576 [03:16<00:03,  2.88it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  98%|█████████▊| 567/576 [03:16<00:03,  2.88it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  99%|█████████▊| 568/576 [03:17<00:02,  2.88it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  99%|█████████▉| 569/576 [03:17<00:02,  2.88it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  99%|█████████▉| 570/576 [03:17<00:02,  2.88it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  99%|█████████▉| 571/576 [03:18<00:01,  2.88it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  99%|█████████▉| 572/576 [03:18<00:01,  2.88it/s, loss=3.079, v_num=0]\n",
      "Epoch 4:  99%|█████████▉| 573/576 [03:18<00:01,  2.88it/s, loss=3.079, v_num=0]\n",
      "Epoch 4: 100%|█████████▉| 574/576 [03:18<00:00,  2.88it/s, loss=3.079, v_num=0]\n",
      "Epoch 4: 100%|█████████▉| 575/576 [03:19<00:00,  2.89it/s, loss=3.079, v_num=0]\n",
      "Epoch 4: 100%|██████████| 576/576 [03:19<00:00,  2.89it/s, loss=3.079, v_num=0]Mean val Loss: 2.9346261024475098\n",
      "Epoch 4: 100%|██████████| 576/576 [03:19<00:00,  2.88it/s, loss=3.079, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 3.09236741065979\n",
      "Epoch 5:  91%|█████████ | 525/576 [03:07<00:18,  2.79it/s, loss=2.831, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5:  91%|█████████▏| 526/576 [03:08<00:17,  2.79it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  91%|█████████▏| 527/576 [03:08<00:17,  2.80it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  92%|█████████▏| 528/576 [03:08<00:17,  2.80it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  92%|█████████▏| 529/576 [03:08<00:16,  2.80it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  92%|█████████▏| 530/576 [03:09<00:16,  2.80it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  92%|█████████▏| 531/576 [03:09<00:16,  2.80it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  92%|█████████▏| 532/576 [03:09<00:15,  2.81it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  93%|█████████▎| 533/576 [03:09<00:15,  2.81it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  93%|█████████▎| 534/576 [03:10<00:14,  2.81it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  93%|█████████▎| 535/576 [03:10<00:14,  2.81it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  93%|█████████▎| 536/576 [03:10<00:14,  2.81it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  93%|█████████▎| 537/576 [03:10<00:13,  2.81it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  93%|█████████▎| 538/576 [03:11<00:13,  2.81it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  94%|█████████▎| 539/576 [03:11<00:13,  2.82it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  94%|█████████▍| 540/576 [03:11<00:12,  2.82it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  94%|█████████▍| 541/576 [03:11<00:12,  2.82it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  94%|█████████▍| 542/576 [03:12<00:12,  2.82it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  94%|█████████▍| 543/576 [03:12<00:11,  2.82it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  94%|█████████▍| 544/576 [03:12<00:11,  2.82it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  95%|█████████▍| 545/576 [03:13<00:10,  2.82it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  95%|█████████▍| 546/576 [03:13<00:10,  2.82it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  95%|█████████▍| 547/576 [03:13<00:10,  2.82it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  95%|█████████▌| 548/576 [03:13<00:09,  2.83it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  95%|█████████▌| 549/576 [03:14<00:09,  2.83it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  95%|█████████▌| 550/576 [03:14<00:09,  2.83it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  96%|█████████▌| 551/576 [03:14<00:08,  2.83it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  96%|█████████▌| 552/576 [03:14<00:08,  2.83it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  96%|█████████▌| 553/576 [03:15<00:08,  2.83it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  96%|█████████▌| 554/576 [03:15<00:07,  2.83it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  96%|█████████▋| 555/576 [03:15<00:07,  2.84it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  97%|█████████▋| 556/576 [03:15<00:07,  2.84it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  97%|█████████▋| 557/576 [03:16<00:06,  2.84it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  97%|█████████▋| 558/576 [03:16<00:06,  2.84it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  97%|█████████▋| 559/576 [03:16<00:05,  2.84it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  97%|█████████▋| 560/576 [03:17<00:05,  2.84it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  97%|█████████▋| 561/576 [03:17<00:05,  2.84it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  98%|█████████▊| 562/576 [03:17<00:04,  2.84it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  98%|█████████▊| 563/576 [03:17<00:04,  2.85it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  98%|█████████▊| 564/576 [03:18<00:04,  2.85it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  98%|█████████▊| 565/576 [03:18<00:03,  2.85it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  98%|█████████▊| 566/576 [03:18<00:03,  2.85it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  98%|█████████▊| 567/576 [03:18<00:03,  2.85it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  99%|█████████▊| 568/576 [03:19<00:02,  2.85it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  99%|█████████▉| 569/576 [03:19<00:02,  2.85it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  99%|█████████▉| 570/576 [03:19<00:02,  2.85it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  99%|█████████▉| 571/576 [03:19<00:01,  2.86it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  99%|█████████▉| 572/576 [03:20<00:01,  2.85it/s, loss=2.831, v_num=0]\n",
      "Epoch 5:  99%|█████████▉| 573/576 [03:20<00:01,  2.86it/s, loss=2.831, v_num=0]\n",
      "Epoch 5: 100%|█████████▉| 574/576 [03:20<00:00,  2.86it/s, loss=2.831, v_num=0]\n",
      "Epoch 5: 100%|█████████▉| 575/576 [03:21<00:00,  2.86it/s, loss=2.831, v_num=0]\n",
      "Epoch 5: 100%|██████████| 576/576 [03:21<00:00,  2.86it/s, loss=2.831, v_num=0]Mean val Loss: 2.661099910736084\n",
      "Epoch 5: 100%|██████████| 576/576 [03:21<00:00,  2.86it/s, loss=2.831, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.9311323165893555\n",
      "Epoch 6:  91%|█████████ | 525/576 [03:06<00:18,  2.82it/s, loss=2.912, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6:  91%|█████████▏| 526/576 [03:06<00:17,  2.82it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  91%|█████████▏| 527/576 [03:06<00:17,  2.83it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  92%|█████████▏| 528/576 [03:06<00:16,  2.83it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  92%|█████████▏| 529/576 [03:07<00:16,  2.83it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  92%|█████████▏| 530/576 [03:07<00:16,  2.83it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  92%|█████████▏| 531/576 [03:07<00:15,  2.83it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  92%|█████████▏| 532/576 [03:07<00:15,  2.83it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  93%|█████████▎| 533/576 [03:08<00:15,  2.83it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  93%|█████████▎| 534/576 [03:08<00:14,  2.84it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  93%|█████████▎| 535/576 [03:08<00:14,  2.84it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  93%|█████████▎| 536/576 [03:08<00:14,  2.84it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  93%|█████████▎| 537/576 [03:08<00:13,  2.84it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  93%|█████████▎| 538/576 [03:09<00:13,  2.84it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  94%|█████████▎| 539/576 [03:09<00:13,  2.84it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  94%|█████████▍| 540/576 [03:09<00:12,  2.85it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  94%|█████████▍| 541/576 [03:09<00:12,  2.85it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  94%|█████████▍| 542/576 [03:10<00:11,  2.85it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  94%|█████████▍| 543/576 [03:10<00:11,  2.85it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  94%|█████████▍| 544/576 [03:10<00:11,  2.85it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  95%|█████████▍| 545/576 [03:10<00:10,  2.86it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  95%|█████████▍| 546/576 [03:11<00:10,  2.86it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  95%|█████████▍| 547/576 [03:11<00:10,  2.86it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  95%|█████████▌| 548/576 [03:11<00:09,  2.86it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  95%|█████████▌| 549/576 [03:11<00:09,  2.86it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  95%|█████████▌| 550/576 [03:12<00:09,  2.86it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  96%|█████████▌| 551/576 [03:12<00:08,  2.87it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  96%|█████████▌| 552/576 [03:12<00:08,  2.87it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  96%|█████████▌| 553/576 [03:12<00:08,  2.87it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  96%|█████████▌| 554/576 [03:12<00:07,  2.87it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  96%|█████████▋| 555/576 [03:13<00:07,  2.87it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  97%|█████████▋| 556/576 [03:13<00:06,  2.87it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  97%|█████████▋| 557/576 [03:13<00:06,  2.88it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  97%|█████████▋| 558/576 [03:13<00:06,  2.88it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  97%|█████████▋| 559/576 [03:14<00:05,  2.88it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  97%|█████████▋| 560/576 [03:14<00:05,  2.88it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  97%|█████████▋| 561/576 [03:14<00:05,  2.88it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  98%|█████████▊| 562/576 [03:14<00:04,  2.88it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  98%|█████████▊| 563/576 [03:15<00:04,  2.88it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  98%|█████████▊| 564/576 [03:15<00:04,  2.89it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  98%|█████████▊| 565/576 [03:15<00:03,  2.89it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  98%|█████████▊| 566/576 [03:16<00:03,  2.89it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  98%|█████████▊| 567/576 [03:16<00:03,  2.89it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  99%|█████████▊| 568/576 [03:16<00:02,  2.89it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  99%|█████████▉| 569/576 [03:16<00:02,  2.89it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  99%|█████████▉| 570/576 [03:17<00:02,  2.89it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  99%|█████████▉| 571/576 [03:17<00:01,  2.89it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  99%|█████████▉| 572/576 [03:17<00:01,  2.89it/s, loss=2.912, v_num=0]\n",
      "Epoch 6:  99%|█████████▉| 573/576 [03:17<00:01,  2.90it/s, loss=2.912, v_num=0]\n",
      "Epoch 6: 100%|█████████▉| 574/576 [03:18<00:00,  2.90it/s, loss=2.912, v_num=0]\n",
      "Epoch 6: 100%|█████████▉| 575/576 [03:18<00:00,  2.90it/s, loss=2.912, v_num=0]\n",
      "Epoch 6: 100%|██████████| 576/576 [03:18<00:00,  2.90it/s, loss=2.912, v_num=0]Mean val Loss: 2.660499334335327\n",
      "Epoch 6: 100%|██████████| 576/576 [03:18<00:00,  2.90it/s, loss=2.912, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.8558292388916016\n",
      "Epoch 7:  91%|█████████ | 525/576 [03:07<00:18,  2.80it/s, loss=2.715, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7:  91%|█████████▏| 526/576 [03:07<00:17,  2.80it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  91%|█████████▏| 527/576 [03:08<00:17,  2.80it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  92%|█████████▏| 528/576 [03:08<00:17,  2.80it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  92%|█████████▏| 529/576 [03:08<00:16,  2.81it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  92%|█████████▏| 530/576 [03:08<00:16,  2.81it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  92%|█████████▏| 531/576 [03:09<00:16,  2.81it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  92%|█████████▏| 532/576 [03:09<00:15,  2.81it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  93%|█████████▎| 533/576 [03:09<00:15,  2.81it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  93%|█████████▎| 534/576 [03:09<00:14,  2.81it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  93%|█████████▎| 535/576 [03:10<00:14,  2.81it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  93%|█████████▎| 536/576 [03:10<00:14,  2.82it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  93%|█████████▎| 537/576 [03:10<00:13,  2.82it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  93%|█████████▎| 538/576 [03:10<00:13,  2.82it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  94%|█████████▎| 539/576 [03:11<00:13,  2.82it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  94%|█████████▍| 540/576 [03:11<00:12,  2.82it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  94%|█████████▍| 541/576 [03:11<00:12,  2.82it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  94%|█████████▍| 542/576 [03:11<00:12,  2.83it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  94%|█████████▍| 543/576 [03:12<00:11,  2.83it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  94%|█████████▍| 544/576 [03:12<00:11,  2.83it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  95%|█████████▍| 545/576 [03:12<00:10,  2.83it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  95%|█████████▍| 546/576 [03:12<00:10,  2.83it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  95%|█████████▍| 547/576 [03:13<00:10,  2.83it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  95%|█████████▌| 548/576 [03:13<00:09,  2.83it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  95%|█████████▌| 549/576 [03:13<00:09,  2.84it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  95%|█████████▌| 550/576 [03:13<00:09,  2.84it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  96%|█████████▌| 551/576 [03:14<00:08,  2.84it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  96%|█████████▌| 552/576 [03:14<00:08,  2.84it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  96%|█████████▌| 553/576 [03:14<00:08,  2.84it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  96%|█████████▌| 554/576 [03:14<00:07,  2.84it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  96%|█████████▋| 555/576 [03:15<00:07,  2.84it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  97%|█████████▋| 556/576 [03:15<00:07,  2.85it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  97%|█████████▋| 557/576 [03:15<00:06,  2.85it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  97%|█████████▋| 558/576 [03:15<00:06,  2.85it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  97%|█████████▋| 559/576 [03:16<00:05,  2.85it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  97%|█████████▋| 560/576 [03:16<00:05,  2.85it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  97%|█████████▋| 561/576 [03:16<00:05,  2.85it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  98%|█████████▊| 562/576 [03:16<00:04,  2.85it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  98%|█████████▊| 563/576 [03:17<00:04,  2.86it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  98%|█████████▊| 564/576 [03:17<00:04,  2.86it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  98%|█████████▊| 565/576 [03:17<00:03,  2.86it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  98%|█████████▊| 566/576 [03:17<00:03,  2.86it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  98%|█████████▊| 567/576 [03:18<00:03,  2.86it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  99%|█████████▊| 568/576 [03:18<00:02,  2.86it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  99%|█████████▉| 569/576 [03:18<00:02,  2.87it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  99%|█████████▉| 570/576 [03:18<00:02,  2.87it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  99%|█████████▉| 571/576 [03:19<00:01,  2.87it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  99%|█████████▉| 572/576 [03:19<00:01,  2.87it/s, loss=2.715, v_num=0]\n",
      "Epoch 7:  99%|█████████▉| 573/576 [03:19<00:01,  2.87it/s, loss=2.715, v_num=0]\n",
      "Epoch 7: 100%|█████████▉| 574/576 [03:19<00:00,  2.87it/s, loss=2.715, v_num=0]\n",
      "Epoch 7: 100%|█████████▉| 575/576 [03:20<00:00,  2.87it/s, loss=2.715, v_num=0]\n",
      "Epoch 7: 100%|██████████| 576/576 [03:20<00:00,  2.88it/s, loss=2.715, v_num=0]Mean val Loss: 2.6503326892852783\n",
      "Epoch 7: 100%|██████████| 576/576 [03:20<00:00,  2.87it/s, loss=2.715, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.745938777923584\n",
      "Epoch 8:  91%|█████████ | 525/576 [03:08<00:18,  2.78it/s, loss=2.646, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8:  91%|█████████▏| 526/576 [03:08<00:17,  2.79it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  91%|█████████▏| 527/576 [03:09<00:17,  2.79it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  92%|█████████▏| 528/576 [03:09<00:17,  2.79it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  92%|█████████▏| 529/576 [03:09<00:16,  2.79it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  92%|█████████▏| 530/576 [03:09<00:16,  2.79it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  92%|█████████▏| 531/576 [03:10<00:16,  2.79it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  92%|█████████▏| 532/576 [03:10<00:15,  2.80it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  93%|█████████▎| 533/576 [03:10<00:15,  2.80it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  93%|█████████▎| 534/576 [03:10<00:15,  2.80it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  93%|█████████▎| 535/576 [03:10<00:14,  2.80it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  93%|█████████▎| 536/576 [03:11<00:14,  2.80it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  93%|█████████▎| 537/576 [03:11<00:13,  2.80it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  93%|█████████▎| 538/576 [03:11<00:13,  2.81it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  94%|█████████▎| 539/576 [03:12<00:13,  2.81it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  94%|█████████▍| 540/576 [03:12<00:12,  2.81it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  94%|█████████▍| 541/576 [03:12<00:12,  2.81it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  94%|█████████▍| 542/576 [03:12<00:12,  2.81it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  94%|█████████▍| 543/576 [03:13<00:11,  2.81it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  94%|█████████▍| 544/576 [03:13<00:11,  2.81it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  95%|█████████▍| 545/576 [03:13<00:11,  2.82it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  95%|█████████▍| 546/576 [03:13<00:10,  2.82it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  95%|█████████▍| 547/576 [03:14<00:10,  2.82it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  95%|█████████▌| 548/576 [03:14<00:09,  2.82it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  95%|█████████▌| 549/576 [03:14<00:09,  2.82it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  95%|█████████▌| 550/576 [03:14<00:09,  2.82it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  96%|█████████▌| 551/576 [03:14<00:08,  2.83it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  96%|█████████▌| 552/576 [03:15<00:08,  2.83it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  96%|█████████▌| 553/576 [03:15<00:08,  2.83it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  96%|█████████▌| 554/576 [03:15<00:07,  2.83it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  96%|█████████▋| 555/576 [03:15<00:07,  2.83it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  97%|█████████▋| 556/576 [03:16<00:07,  2.83it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  97%|█████████▋| 557/576 [03:16<00:06,  2.84it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  97%|█████████▋| 558/576 [03:16<00:06,  2.84it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  97%|█████████▋| 559/576 [03:16<00:05,  2.84it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  97%|█████████▋| 560/576 [03:17<00:05,  2.84it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  97%|█████████▋| 561/576 [03:17<00:05,  2.84it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  98%|█████████▊| 562/576 [03:17<00:04,  2.84it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  98%|█████████▊| 563/576 [03:17<00:04,  2.85it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  98%|█████████▊| 564/576 [03:18<00:04,  2.85it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  98%|█████████▊| 565/576 [03:18<00:03,  2.84it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  98%|█████████▊| 566/576 [03:18<00:03,  2.85it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  98%|█████████▊| 567/576 [03:19<00:03,  2.85it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  99%|█████████▊| 568/576 [03:19<00:02,  2.85it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  99%|█████████▉| 569/576 [03:19<00:02,  2.85it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  99%|█████████▉| 570/576 [03:19<00:02,  2.85it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  99%|█████████▉| 571/576 [03:20<00:01,  2.85it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  99%|█████████▉| 572/576 [03:20<00:01,  2.86it/s, loss=2.646, v_num=0]\n",
      "Epoch 8:  99%|█████████▉| 573/576 [03:20<00:01,  2.86it/s, loss=2.646, v_num=0]\n",
      "Epoch 8: 100%|█████████▉| 574/576 [03:20<00:00,  2.86it/s, loss=2.646, v_num=0]\n",
      "Epoch 8: 100%|█████████▉| 575/576 [03:20<00:00,  2.86it/s, loss=2.646, v_num=0]\n",
      "Epoch 8: 100%|██████████| 576/576 [03:21<00:00,  2.86it/s, loss=2.646, v_num=0]Mean val Loss: 2.5303561687469482\n",
      "Epoch 8: 100%|██████████| 576/576 [03:21<00:00,  2.86it/s, loss=2.646, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.6940810680389404\n",
      "Epoch 9:  91%|█████████ | 525/576 [03:08<00:18,  2.78it/s, loss=2.804, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9:  91%|█████████▏| 526/576 [03:08<00:17,  2.79it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  91%|█████████▏| 527/576 [03:09<00:17,  2.79it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  92%|█████████▏| 528/576 [03:09<00:17,  2.79it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  92%|█████████▏| 529/576 [03:09<00:16,  2.79it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  92%|█████████▏| 530/576 [03:09<00:16,  2.79it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  92%|█████████▏| 531/576 [03:10<00:16,  2.79it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  92%|█████████▏| 532/576 [03:10<00:15,  2.80it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  93%|█████████▎| 533/576 [03:10<00:15,  2.80it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  93%|█████████▎| 534/576 [03:10<00:15,  2.80it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  93%|█████████▎| 535/576 [03:10<00:14,  2.80it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  93%|█████████▎| 536/576 [03:11<00:14,  2.80it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  93%|█████████▎| 537/576 [03:11<00:13,  2.80it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  93%|█████████▎| 538/576 [03:11<00:13,  2.81it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  94%|█████████▎| 539/576 [03:12<00:13,  2.81it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  94%|█████████▍| 540/576 [03:12<00:12,  2.81it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  94%|█████████▍| 541/576 [03:12<00:12,  2.81it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  94%|█████████▍| 542/576 [03:12<00:12,  2.81it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  94%|█████████▍| 543/576 [03:12<00:11,  2.81it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  94%|█████████▍| 544/576 [03:13<00:11,  2.81it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  95%|█████████▍| 545/576 [03:13<00:11,  2.82it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  95%|█████████▍| 546/576 [03:13<00:10,  2.82it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  95%|█████████▍| 547/576 [03:13<00:10,  2.82it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  95%|█████████▌| 548/576 [03:14<00:09,  2.82it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  95%|█████████▌| 549/576 [03:14<00:09,  2.82it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  95%|█████████▌| 550/576 [03:14<00:09,  2.82it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  96%|█████████▌| 551/576 [03:14<00:08,  2.83it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  96%|█████████▌| 552/576 [03:15<00:08,  2.83it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  96%|█████████▌| 553/576 [03:15<00:08,  2.83it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  96%|█████████▌| 554/576 [03:15<00:07,  2.83it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  96%|█████████▋| 555/576 [03:15<00:07,  2.83it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  97%|█████████▋| 556/576 [03:16<00:07,  2.83it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  97%|█████████▋| 557/576 [03:16<00:06,  2.84it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  97%|█████████▋| 558/576 [03:16<00:06,  2.84it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  97%|█████████▋| 559/576 [03:16<00:05,  2.84it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  97%|█████████▋| 560/576 [03:17<00:05,  2.84it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  97%|█████████▋| 561/576 [03:17<00:05,  2.84it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  98%|█████████▊| 562/576 [03:17<00:04,  2.84it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  98%|█████████▊| 563/576 [03:17<00:04,  2.84it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  98%|█████████▊| 564/576 [03:18<00:04,  2.85it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  98%|█████████▊| 565/576 [03:18<00:03,  2.85it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  98%|█████████▊| 566/576 [03:18<00:03,  2.85it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  98%|█████████▊| 567/576 [03:18<00:03,  2.85it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  99%|█████████▊| 568/576 [03:19<00:02,  2.85it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  99%|█████████▉| 569/576 [03:19<00:02,  2.85it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  99%|█████████▉| 570/576 [03:19<00:02,  2.86it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  99%|█████████▉| 571/576 [03:19<00:01,  2.86it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  99%|█████████▉| 572/576 [03:20<00:01,  2.86it/s, loss=2.804, v_num=0]\n",
      "Epoch 9:  99%|█████████▉| 573/576 [03:20<00:01,  2.86it/s, loss=2.804, v_num=0]\n",
      "Epoch 9: 100%|█████████▉| 574/576 [03:20<00:00,  2.86it/s, loss=2.804, v_num=0]\n",
      "Epoch 9: 100%|█████████▉| 575/576 [03:20<00:00,  2.86it/s, loss=2.804, v_num=0]\n",
      "Epoch 9: 100%|██████████| 576/576 [03:21<00:00,  2.86it/s, loss=2.804, v_num=0]Mean val Loss: 2.489271402359009\n",
      "Epoch 9: 100%|██████████| 576/576 [03:21<00:00,  2.86it/s, loss=2.804, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.649001359939575\n",
      "Epoch 10:  91%|█████████ | 525/576 [03:05<00:18,  2.82it/s, loss=2.532, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10:  91%|█████████▏| 526/576 [03:06<00:17,  2.83it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  91%|█████████▏| 527/576 [03:06<00:17,  2.83it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  92%|█████████▏| 528/576 [03:06<00:16,  2.83it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  92%|█████████▏| 529/576 [03:06<00:16,  2.83it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  92%|█████████▏| 530/576 [03:07<00:16,  2.83it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  92%|█████████▏| 531/576 [03:07<00:15,  2.83it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  92%|█████████▏| 532/576 [03:07<00:15,  2.84it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  93%|█████████▎| 533/576 [03:07<00:15,  2.84it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  93%|█████████▎| 534/576 [03:08<00:14,  2.84it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  93%|█████████▎| 535/576 [03:08<00:14,  2.84it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  93%|█████████▎| 536/576 [03:08<00:14,  2.84it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  93%|█████████▎| 537/576 [03:08<00:13,  2.84it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  93%|█████████▎| 538/576 [03:09<00:13,  2.85it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  94%|█████████▎| 539/576 [03:09<00:12,  2.85it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  94%|█████████▍| 540/576 [03:09<00:12,  2.85it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  94%|█████████▍| 541/576 [03:09<00:12,  2.85it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  94%|█████████▍| 542/576 [03:10<00:11,  2.85it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  94%|█████████▍| 543/576 [03:10<00:11,  2.85it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  94%|█████████▍| 544/576 [03:10<00:11,  2.85it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  95%|█████████▍| 545/576 [03:10<00:10,  2.85it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  95%|█████████▍| 546/576 [03:11<00:10,  2.86it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  95%|█████████▍| 547/576 [03:11<00:10,  2.86it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  95%|█████████▌| 548/576 [03:11<00:09,  2.86it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  95%|█████████▌| 549/576 [03:11<00:09,  2.86it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  95%|█████████▌| 550/576 [03:12<00:09,  2.86it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  96%|█████████▌| 551/576 [03:12<00:08,  2.86it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  96%|█████████▌| 552/576 [03:12<00:08,  2.87it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  96%|█████████▌| 553/576 [03:12<00:08,  2.87it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  96%|█████████▌| 554/576 [03:13<00:07,  2.87it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  96%|█████████▋| 555/576 [03:13<00:07,  2.87it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  97%|█████████▋| 556/576 [03:13<00:06,  2.87it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  97%|█████████▋| 557/576 [03:13<00:06,  2.87it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  97%|█████████▋| 558/576 [03:14<00:06,  2.88it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  97%|█████████▋| 559/576 [03:14<00:05,  2.88it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  97%|█████████▋| 560/576 [03:14<00:05,  2.88it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  97%|█████████▋| 561/576 [03:14<00:05,  2.88it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  98%|█████████▊| 562/576 [03:15<00:04,  2.88it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  98%|█████████▊| 563/576 [03:15<00:04,  2.88it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  98%|█████████▊| 564/576 [03:15<00:04,  2.88it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  98%|█████████▊| 565/576 [03:15<00:03,  2.89it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  98%|█████████▊| 566/576 [03:16<00:03,  2.89it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  98%|█████████▊| 567/576 [03:16<00:03,  2.89it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  99%|█████████▊| 568/576 [03:16<00:02,  2.89it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  99%|█████████▉| 569/576 [03:17<00:02,  2.89it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  99%|█████████▉| 570/576 [03:17<00:02,  2.89it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  99%|█████████▉| 571/576 [03:17<00:01,  2.89it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  99%|█████████▉| 572/576 [03:17<00:01,  2.89it/s, loss=2.532, v_num=0]\n",
      "Epoch 10:  99%|█████████▉| 573/576 [03:18<00:01,  2.89it/s, loss=2.532, v_num=0]\n",
      "Epoch 10: 100%|█████████▉| 574/576 [03:18<00:00,  2.89it/s, loss=2.532, v_num=0]\n",
      "Epoch 10: 100%|█████████▉| 575/576 [03:18<00:00,  2.90it/s, loss=2.532, v_num=0]\n",
      "Epoch 10: 100%|██████████| 576/576 [03:18<00:00,  2.90it/s, loss=2.532, v_num=0]Mean val Loss: 2.4488184452056885\n",
      "Epoch 10: 100%|██████████| 576/576 [03:19<00:00,  2.89it/s, loss=2.532, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.5852270126342773\n",
      "Epoch 11:  91%|█████████ | 525/576 [03:06<00:18,  2.81it/s, loss=2.727, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11:  91%|█████████▏| 526/576 [03:07<00:17,  2.81it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  91%|█████████▏| 527/576 [03:07<00:17,  2.81it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  92%|█████████▏| 528/576 [03:07<00:17,  2.81it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  92%|█████████▏| 529/576 [03:07<00:16,  2.81it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  92%|█████████▏| 530/576 [03:08<00:16,  2.82it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  92%|█████████▏| 531/576 [03:08<00:15,  2.82it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  92%|█████████▏| 532/576 [03:08<00:15,  2.82it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  93%|█████████▎| 533/576 [03:08<00:15,  2.82it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  93%|█████████▎| 534/576 [03:09<00:14,  2.82it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  93%|█████████▎| 535/576 [03:09<00:14,  2.82it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  93%|█████████▎| 536/576 [03:09<00:14,  2.83it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  93%|█████████▎| 537/576 [03:10<00:13,  2.83it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  93%|█████████▎| 538/576 [03:10<00:13,  2.83it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  94%|█████████▎| 539/576 [03:10<00:13,  2.83it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  94%|█████████▍| 540/576 [03:11<00:12,  2.83it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  94%|█████████▍| 541/576 [03:11<00:12,  2.83it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  94%|█████████▍| 542/576 [03:11<00:12,  2.83it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  94%|█████████▍| 543/576 [03:11<00:11,  2.83it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  94%|█████████▍| 544/576 [03:11<00:11,  2.83it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  95%|█████████▍| 545/576 [03:12<00:10,  2.84it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  95%|█████████▍| 546/576 [03:12<00:10,  2.84it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  95%|█████████▍| 547/576 [03:12<00:10,  2.84it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  95%|█████████▌| 548/576 [03:12<00:09,  2.84it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  95%|█████████▌| 549/576 [03:13<00:09,  2.84it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  95%|█████████▌| 550/576 [03:13<00:09,  2.84it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  96%|█████████▌| 551/576 [03:13<00:08,  2.84it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  96%|█████████▌| 552/576 [03:14<00:08,  2.84it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  96%|█████████▌| 553/576 [03:14<00:08,  2.84it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  96%|█████████▌| 554/576 [03:14<00:07,  2.85it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  96%|█████████▋| 555/576 [03:14<00:07,  2.85it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  97%|█████████▋| 556/576 [03:15<00:07,  2.85it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  97%|█████████▋| 557/576 [03:15<00:06,  2.85it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  97%|█████████▋| 558/576 [03:15<00:06,  2.85it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  97%|█████████▋| 559/576 [03:15<00:05,  2.85it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  97%|█████████▋| 560/576 [03:16<00:05,  2.86it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  97%|█████████▋| 561/576 [03:16<00:05,  2.86it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  98%|█████████▊| 562/576 [03:16<00:04,  2.86it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  98%|█████████▊| 563/576 [03:16<00:04,  2.86it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  98%|█████████▊| 564/576 [03:17<00:04,  2.86it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  98%|█████████▊| 565/576 [03:17<00:03,  2.86it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  98%|█████████▊| 566/576 [03:17<00:03,  2.86it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  98%|█████████▊| 567/576 [03:17<00:03,  2.87it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  99%|█████████▊| 568/576 [03:18<00:02,  2.87it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  99%|█████████▉| 569/576 [03:18<00:02,  2.87it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  99%|█████████▉| 570/576 [03:18<00:02,  2.87it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  99%|█████████▉| 571/576 [03:18<00:01,  2.87it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  99%|█████████▉| 572/576 [03:19<00:01,  2.87it/s, loss=2.727, v_num=0]\n",
      "Epoch 11:  99%|█████████▉| 573/576 [03:19<00:01,  2.87it/s, loss=2.727, v_num=0]\n",
      "Epoch 11: 100%|█████████▉| 574/576 [03:19<00:00,  2.88it/s, loss=2.727, v_num=0]\n",
      "Epoch 11: 100%|█████████▉| 575/576 [03:19<00:00,  2.88it/s, loss=2.727, v_num=0]\n",
      "Epoch 11: 100%|██████████| 576/576 [03:20<00:00,  2.88it/s, loss=2.727, v_num=0]Mean val Loss: 2.324497938156128\n",
      "Epoch 11: 100%|██████████| 576/576 [03:20<00:00,  2.88it/s, loss=2.727, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.569406509399414\n",
      "Epoch 12:  91%|█████████ | 525/576 [03:06<00:18,  2.81it/s, loss=2.433, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12:  91%|█████████▏| 526/576 [03:07<00:17,  2.81it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  91%|█████████▏| 527/576 [03:07<00:17,  2.81it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  92%|█████████▏| 528/576 [03:07<00:17,  2.82it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  92%|█████████▏| 529/576 [03:07<00:16,  2.82it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  92%|█████████▏| 530/576 [03:07<00:16,  2.82it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  92%|█████████▏| 531/576 [03:08<00:15,  2.82it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  92%|█████████▏| 532/576 [03:08<00:15,  2.82it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  93%|█████████▎| 533/576 [03:08<00:15,  2.82it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  93%|█████████▎| 534/576 [03:08<00:14,  2.83it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  93%|█████████▎| 535/576 [03:09<00:14,  2.83it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  93%|█████████▎| 536/576 [03:09<00:14,  2.83it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  93%|█████████▎| 537/576 [03:09<00:13,  2.83it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  93%|█████████▎| 538/576 [03:09<00:13,  2.83it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  94%|█████████▎| 539/576 [03:10<00:13,  2.84it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  94%|█████████▍| 540/576 [03:10<00:12,  2.83it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  94%|█████████▍| 541/576 [03:10<00:12,  2.83it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  94%|█████████▍| 542/576 [03:11<00:11,  2.84it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  94%|█████████▍| 543/576 [03:11<00:11,  2.84it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  94%|█████████▍| 544/576 [03:11<00:11,  2.84it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  95%|█████████▍| 545/576 [03:12<00:10,  2.84it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  95%|█████████▍| 546/576 [03:12<00:10,  2.84it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  95%|█████████▍| 547/576 [03:12<00:10,  2.84it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  95%|█████████▌| 548/576 [03:13<00:09,  2.84it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  95%|█████████▌| 549/576 [03:13<00:09,  2.84it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  95%|█████████▌| 550/576 [03:13<00:09,  2.84it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  96%|█████████▌| 551/576 [03:13<00:08,  2.84it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  96%|█████████▌| 552/576 [03:14<00:08,  2.84it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  96%|█████████▌| 553/576 [03:14<00:08,  2.85it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  96%|█████████▌| 554/576 [03:14<00:07,  2.85it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  96%|█████████▋| 555/576 [03:14<00:07,  2.85it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  97%|█████████▋| 556/576 [03:14<00:07,  2.85it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  97%|█████████▋| 557/576 [03:15<00:06,  2.85it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  97%|█████████▋| 558/576 [03:15<00:06,  2.86it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  97%|█████████▋| 559/576 [03:15<00:05,  2.86it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  97%|█████████▋| 560/576 [03:15<00:05,  2.86it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  97%|█████████▋| 561/576 [03:16<00:05,  2.86it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  98%|█████████▊| 562/576 [03:16<00:04,  2.86it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  98%|█████████▊| 563/576 [03:16<00:04,  2.86it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  98%|█████████▊| 564/576 [03:16<00:04,  2.87it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  98%|█████████▊| 565/576 [03:17<00:03,  2.87it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  98%|█████████▊| 566/576 [03:17<00:03,  2.87it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  98%|█████████▊| 567/576 [03:17<00:03,  2.87it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  99%|█████████▊| 568/576 [03:18<00:02,  2.87it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  99%|█████████▉| 569/576 [03:18<00:02,  2.87it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  99%|█████████▉| 570/576 [03:18<00:02,  2.87it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  99%|█████████▉| 571/576 [03:18<00:01,  2.87it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  99%|█████████▉| 572/576 [03:19<00:01,  2.87it/s, loss=2.433, v_num=0]\n",
      "Epoch 12:  99%|█████████▉| 573/576 [03:19<00:01,  2.88it/s, loss=2.433, v_num=0]\n",
      "Epoch 12: 100%|█████████▉| 574/576 [03:19<00:00,  2.88it/s, loss=2.433, v_num=0]\n",
      "Epoch 12: 100%|█████████▉| 575/576 [03:19<00:00,  2.88it/s, loss=2.433, v_num=0]\n",
      "Epoch 12: 100%|██████████| 576/576 [03:19<00:00,  2.88it/s, loss=2.433, v_num=0]Mean val Loss: 2.3466079235076904\n",
      "Epoch 12: 100%|██████████| 576/576 [03:20<00:00,  2.88it/s, loss=2.433, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.526210069656372\n",
      "Epoch 13:  91%|█████████ | 525/576 [03:07<00:18,  2.80it/s, loss=2.454, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13:  91%|█████████▏| 526/576 [03:07<00:17,  2.81it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  91%|█████████▏| 527/576 [03:07<00:17,  2.81it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  92%|█████████▏| 528/576 [03:07<00:17,  2.81it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  92%|█████████▏| 529/576 [03:08<00:16,  2.81it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  92%|█████████▏| 530/576 [03:08<00:16,  2.81it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  92%|█████████▏| 531/576 [03:08<00:15,  2.81it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  92%|█████████▏| 532/576 [03:08<00:15,  2.82it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  93%|█████████▎| 533/576 [03:09<00:15,  2.82it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  93%|█████████▎| 534/576 [03:09<00:14,  2.82it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  93%|█████████▎| 535/576 [03:09<00:14,  2.82it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  93%|█████████▎| 536/576 [03:09<00:14,  2.82it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  93%|█████████▎| 537/576 [03:10<00:13,  2.82it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  93%|█████████▎| 538/576 [03:10<00:13,  2.83it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  94%|█████████▎| 539/576 [03:10<00:13,  2.83it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  94%|█████████▍| 540/576 [03:10<00:12,  2.83it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  94%|█████████▍| 541/576 [03:11<00:12,  2.83it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  94%|█████████▍| 542/576 [03:11<00:12,  2.83it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  94%|█████████▍| 543/576 [03:11<00:11,  2.83it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  94%|█████████▍| 544/576 [03:11<00:11,  2.84it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  95%|█████████▍| 545/576 [03:12<00:10,  2.84it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  95%|█████████▍| 546/576 [03:12<00:10,  2.84it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  95%|█████████▍| 547/576 [03:12<00:10,  2.84it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  95%|█████████▌| 548/576 [03:12<00:09,  2.84it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  95%|█████████▌| 549/576 [03:13<00:09,  2.84it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  95%|█████████▌| 550/576 [03:13<00:09,  2.84it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  96%|█████████▌| 551/576 [03:13<00:08,  2.84it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  96%|█████████▌| 552/576 [03:13<00:08,  2.85it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  96%|█████████▌| 553/576 [03:14<00:08,  2.85it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  96%|█████████▌| 554/576 [03:14<00:07,  2.85it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  96%|█████████▋| 555/576 [03:14<00:07,  2.85it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  97%|█████████▋| 556/576 [03:14<00:07,  2.85it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  97%|█████████▋| 557/576 [03:15<00:06,  2.85it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  97%|█████████▋| 558/576 [03:15<00:06,  2.86it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  97%|█████████▋| 559/576 [03:15<00:05,  2.86it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  97%|█████████▋| 560/576 [03:15<00:05,  2.86it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  97%|█████████▋| 561/576 [03:16<00:05,  2.86it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  98%|█████████▊| 562/576 [03:16<00:04,  2.86it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  98%|█████████▊| 563/576 [03:16<00:04,  2.86it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  98%|█████████▊| 564/576 [03:16<00:04,  2.86it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  98%|█████████▊| 565/576 [03:17<00:03,  2.87it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  98%|█████████▊| 566/576 [03:17<00:03,  2.87it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  98%|█████████▊| 567/576 [03:17<00:03,  2.87it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  99%|█████████▊| 568/576 [03:17<00:02,  2.87it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  99%|█████████▉| 569/576 [03:18<00:02,  2.87it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  99%|█████████▉| 570/576 [03:18<00:02,  2.87it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  99%|█████████▉| 571/576 [03:18<00:01,  2.87it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  99%|█████████▉| 572/576 [03:18<00:01,  2.88it/s, loss=2.454, v_num=0]\n",
      "Epoch 13:  99%|█████████▉| 573/576 [03:19<00:01,  2.88it/s, loss=2.454, v_num=0]\n",
      "Epoch 13: 100%|█████████▉| 574/576 [03:19<00:00,  2.88it/s, loss=2.454, v_num=0]\n",
      "Epoch 13: 100%|█████████▉| 575/576 [03:19<00:00,  2.88it/s, loss=2.454, v_num=0]\n",
      "Epoch 13: 100%|██████████| 576/576 [03:19<00:00,  2.88it/s, loss=2.454, v_num=0]Mean val Loss: 2.341433048248291\n",
      "Epoch 13: 100%|██████████| 576/576 [03:20<00:00,  2.88it/s, loss=2.454, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.4711055755615234\n",
      "Epoch 14:  91%|█████████ | 525/576 [03:06<00:18,  2.81it/s, loss=2.538, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14:  91%|█████████▏| 526/576 [03:07<00:17,  2.81it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  91%|█████████▏| 527/576 [03:07<00:17,  2.81it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  92%|█████████▏| 528/576 [03:07<00:17,  2.81it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  92%|█████████▏| 529/576 [03:07<00:16,  2.82it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  92%|█████████▏| 530/576 [03:08<00:16,  2.82it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  92%|█████████▏| 531/576 [03:08<00:15,  2.82it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  92%|█████████▏| 532/576 [03:08<00:15,  2.82it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  93%|█████████▎| 533/576 [03:08<00:15,  2.82it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  93%|█████████▎| 534/576 [03:09<00:14,  2.82it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  93%|█████████▎| 535/576 [03:09<00:14,  2.83it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  93%|█████████▎| 536/576 [03:09<00:14,  2.83it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  93%|█████████▎| 537/576 [03:09<00:13,  2.83it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  93%|█████████▎| 538/576 [03:10<00:13,  2.83it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  94%|█████████▎| 539/576 [03:10<00:13,  2.83it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  94%|█████████▍| 540/576 [03:10<00:12,  2.83it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  94%|█████████▍| 541/576 [03:10<00:12,  2.84it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  94%|█████████▍| 542/576 [03:10<00:11,  2.84it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  94%|█████████▍| 543/576 [03:11<00:11,  2.84it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  94%|█████████▍| 544/576 [03:11<00:11,  2.84it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  95%|█████████▍| 545/576 [03:11<00:10,  2.84it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  95%|█████████▍| 546/576 [03:11<00:10,  2.85it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  95%|█████████▍| 547/576 [03:12<00:10,  2.85it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  95%|█████████▌| 548/576 [03:12<00:09,  2.85it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  95%|█████████▌| 549/576 [03:12<00:09,  2.85it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  95%|█████████▌| 550/576 [03:12<00:09,  2.85it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  96%|█████████▌| 551/576 [03:13<00:08,  2.85it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  96%|█████████▌| 552/576 [03:13<00:08,  2.85it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  96%|█████████▌| 553/576 [03:13<00:08,  2.85it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  96%|█████████▌| 554/576 [03:14<00:07,  2.85it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  96%|█████████▋| 555/576 [03:14<00:07,  2.86it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  97%|█████████▋| 556/576 [03:14<00:07,  2.86it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  97%|█████████▋| 557/576 [03:14<00:06,  2.86it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  97%|█████████▋| 558/576 [03:15<00:06,  2.86it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  97%|█████████▋| 559/576 [03:15<00:05,  2.86it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  97%|█████████▋| 560/576 [03:15<00:05,  2.86it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  97%|█████████▋| 561/576 [03:15<00:05,  2.86it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  98%|█████████▊| 562/576 [03:16<00:04,  2.87it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  98%|█████████▊| 563/576 [03:16<00:04,  2.87it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  98%|█████████▊| 564/576 [03:16<00:04,  2.87it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  98%|█████████▊| 565/576 [03:16<00:03,  2.87it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  98%|█████████▊| 566/576 [03:17<00:03,  2.87it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  98%|█████████▊| 567/576 [03:17<00:03,  2.87it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  99%|█████████▊| 568/576 [03:17<00:02,  2.87it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  99%|█████████▉| 569/576 [03:17<00:02,  2.87it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  99%|█████████▉| 570/576 [03:18<00:02,  2.88it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  99%|█████████▉| 571/576 [03:18<00:01,  2.88it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  99%|█████████▉| 572/576 [03:18<00:01,  2.88it/s, loss=2.538, v_num=0]\n",
      "Epoch 14:  99%|█████████▉| 573/576 [03:19<00:01,  2.88it/s, loss=2.538, v_num=0]\n",
      "Epoch 14: 100%|█████████▉| 574/576 [03:19<00:00,  2.88it/s, loss=2.538, v_num=0]\n",
      "Epoch 14: 100%|█████████▉| 575/576 [03:19<00:00,  2.88it/s, loss=2.538, v_num=0]\n",
      "Epoch 14: 100%|██████████| 576/576 [03:19<00:00,  2.88it/s, loss=2.538, v_num=0]Mean val Loss: 2.2854719161987305\n",
      "Epoch 14: 100%|██████████| 576/576 [03:19<00:00,  2.88it/s, loss=2.538, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.4832160472869873\n",
      "Epoch 15:  91%|█████████ | 525/576 [03:05<00:18,  2.83it/s, loss=2.295, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15:  91%|█████████▏| 526/576 [03:05<00:17,  2.83it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  91%|█████████▏| 527/576 [03:06<00:17,  2.83it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  92%|█████████▏| 528/576 [03:06<00:16,  2.83it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  92%|█████████▏| 529/576 [03:06<00:16,  2.83it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  92%|█████████▏| 530/576 [03:06<00:16,  2.84it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  92%|█████████▏| 531/576 [03:07<00:15,  2.84it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  92%|█████████▏| 532/576 [03:07<00:15,  2.84it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  93%|█████████▎| 533/576 [03:07<00:15,  2.84it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  93%|█████████▎| 534/576 [03:07<00:14,  2.84it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  93%|█████████▎| 535/576 [03:08<00:14,  2.85it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  93%|█████████▎| 536/576 [03:08<00:14,  2.85it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  93%|█████████▎| 537/576 [03:08<00:13,  2.85it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  93%|█████████▎| 538/576 [03:08<00:13,  2.85it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  94%|█████████▎| 539/576 [03:08<00:12,  2.85it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  94%|█████████▍| 540/576 [03:09<00:12,  2.85it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  94%|█████████▍| 541/576 [03:09<00:12,  2.85it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  94%|█████████▍| 542/576 [03:09<00:11,  2.85it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  94%|█████████▍| 543/576 [03:10<00:11,  2.86it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  94%|█████████▍| 544/576 [03:10<00:11,  2.86it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  95%|█████████▍| 545/576 [03:10<00:10,  2.86it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  95%|█████████▍| 546/576 [03:10<00:10,  2.86it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  95%|█████████▍| 547/576 [03:11<00:10,  2.86it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  95%|█████████▌| 548/576 [03:11<00:09,  2.86it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  95%|█████████▌| 549/576 [03:11<00:09,  2.86it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  95%|█████████▌| 550/576 [03:11<00:09,  2.87it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  96%|█████████▌| 551/576 [03:12<00:08,  2.87it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  96%|█████████▌| 552/576 [03:12<00:08,  2.87it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  96%|█████████▌| 553/576 [03:12<00:08,  2.87it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  96%|█████████▌| 554/576 [03:12<00:07,  2.87it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  96%|█████████▋| 555/576 [03:13<00:07,  2.87it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  97%|█████████▋| 556/576 [03:13<00:06,  2.87it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  97%|█████████▋| 557/576 [03:13<00:06,  2.88it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  97%|█████████▋| 558/576 [03:13<00:06,  2.88it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  97%|█████████▋| 559/576 [03:14<00:05,  2.88it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  97%|█████████▋| 560/576 [03:14<00:05,  2.88it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  97%|█████████▋| 561/576 [03:14<00:05,  2.88it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  98%|█████████▊| 562/576 [03:15<00:04,  2.88it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  98%|█████████▊| 563/576 [03:15<00:04,  2.88it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  98%|█████████▊| 564/576 [03:15<00:04,  2.88it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  98%|█████████▊| 565/576 [03:15<00:03,  2.89it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  98%|█████████▊| 566/576 [03:16<00:03,  2.88it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  98%|█████████▊| 567/576 [03:16<00:03,  2.89it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  99%|█████████▊| 568/576 [03:16<00:02,  2.89it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  99%|█████████▉| 569/576 [03:17<00:02,  2.89it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  99%|█████████▉| 570/576 [03:17<00:02,  2.89it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  99%|█████████▉| 571/576 [03:17<00:01,  2.89it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  99%|█████████▉| 572/576 [03:17<00:01,  2.89it/s, loss=2.295, v_num=0]\n",
      "Epoch 15:  99%|█████████▉| 573/576 [03:18<00:01,  2.89it/s, loss=2.295, v_num=0]\n",
      "Epoch 15: 100%|█████████▉| 574/576 [03:18<00:00,  2.89it/s, loss=2.295, v_num=0]\n",
      "Epoch 15: 100%|█████████▉| 575/576 [03:18<00:00,  2.89it/s, loss=2.295, v_num=0]\n",
      "Epoch 15: 100%|██████████| 576/576 [03:18<00:00,  2.90it/s, loss=2.295, v_num=0]Mean val Loss: 2.2318289279937744\n",
      "Epoch 15: 100%|██████████| 576/576 [03:19<00:00,  2.89it/s, loss=2.295, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.4353690147399902\n",
      "Epoch 16:  91%|█████████ | 525/576 [03:07<00:18,  2.81it/s, loss=2.388, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16:  91%|█████████▏| 526/576 [03:07<00:17,  2.81it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  91%|█████████▏| 527/576 [03:07<00:17,  2.81it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  92%|█████████▏| 528/576 [03:07<00:17,  2.81it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  92%|█████████▏| 529/576 [03:08<00:16,  2.81it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  92%|█████████▏| 530/576 [03:08<00:16,  2.82it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  92%|█████████▏| 531/576 [03:08<00:15,  2.82it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  92%|█████████▏| 532/576 [03:08<00:15,  2.82it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  93%|█████████▎| 533/576 [03:08<00:15,  2.82it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  93%|█████████▎| 534/576 [03:09<00:14,  2.82it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  93%|█████████▎| 535/576 [03:09<00:14,  2.82it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  93%|█████████▎| 536/576 [03:09<00:14,  2.83it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  93%|█████████▎| 537/576 [03:09<00:13,  2.83it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  93%|█████████▎| 538/576 [03:10<00:13,  2.83it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  94%|█████████▎| 539/576 [03:10<00:13,  2.83it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  94%|█████████▍| 540/576 [03:10<00:12,  2.83it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  94%|█████████▍| 541/576 [03:10<00:12,  2.83it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  94%|█████████▍| 542/576 [03:11<00:11,  2.84it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  94%|█████████▍| 543/576 [03:11<00:11,  2.84it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  94%|█████████▍| 544/576 [03:11<00:11,  2.84it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  95%|█████████▍| 545/576 [03:11<00:10,  2.84it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  95%|█████████▍| 546/576 [03:12<00:10,  2.84it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  95%|█████████▍| 547/576 [03:12<00:10,  2.85it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  95%|█████████▌| 548/576 [03:12<00:09,  2.85it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  95%|█████████▌| 549/576 [03:12<00:09,  2.85it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  95%|█████████▌| 550/576 [03:12<00:09,  2.85it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  96%|█████████▌| 551/576 [03:13<00:08,  2.85it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  96%|█████████▌| 552/576 [03:13<00:08,  2.85it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  96%|█████████▌| 553/576 [03:13<00:08,  2.86it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  96%|█████████▌| 554/576 [03:13<00:07,  2.86it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  96%|█████████▋| 555/576 [03:14<00:07,  2.86it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  97%|█████████▋| 556/576 [03:14<00:06,  2.86it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  97%|█████████▋| 557/576 [03:14<00:06,  2.86it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  97%|█████████▋| 558/576 [03:14<00:06,  2.86it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  97%|█████████▋| 559/576 [03:15<00:05,  2.86it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  97%|█████████▋| 560/576 [03:15<00:05,  2.87it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  97%|█████████▋| 561/576 [03:15<00:05,  2.87it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  98%|█████████▊| 562/576 [03:15<00:04,  2.87it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  98%|█████████▊| 563/576 [03:16<00:04,  2.87it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  98%|█████████▊| 564/576 [03:16<00:04,  2.87it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  98%|█████████▊| 565/576 [03:16<00:03,  2.87it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  98%|█████████▊| 566/576 [03:16<00:03,  2.87it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  98%|█████████▊| 567/576 [03:17<00:03,  2.88it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  99%|█████████▊| 568/576 [03:17<00:02,  2.88it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  99%|█████████▉| 569/576 [03:17<00:02,  2.88it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  99%|█████████▉| 570/576 [03:17<00:02,  2.88it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  99%|█████████▉| 571/576 [03:18<00:01,  2.88it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  99%|█████████▉| 572/576 [03:18<00:01,  2.88it/s, loss=2.388, v_num=0]\n",
      "Epoch 16:  99%|█████████▉| 573/576 [03:18<00:01,  2.88it/s, loss=2.388, v_num=0]\n",
      "Epoch 16: 100%|█████████▉| 574/576 [03:18<00:00,  2.89it/s, loss=2.388, v_num=0]\n",
      "Epoch 16: 100%|█████████▉| 575/576 [03:19<00:00,  2.89it/s, loss=2.388, v_num=0]\n",
      "Epoch 16: 100%|██████████| 576/576 [03:19<00:00,  2.89it/s, loss=2.388, v_num=0]Mean val Loss: 2.2559781074523926\n",
      "Epoch 16: 100%|██████████| 576/576 [03:19<00:00,  2.89it/s, loss=2.388, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.386939525604248\n",
      "Epoch 17:  91%|█████████ | 525/576 [03:06<00:18,  2.81it/s, loss=2.345, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17:  91%|█████████▏| 526/576 [03:07<00:17,  2.81it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  91%|█████████▏| 527/576 [03:07<00:17,  2.81it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  92%|█████████▏| 528/576 [03:07<00:17,  2.81it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  92%|█████████▏| 529/576 [03:07<00:16,  2.81it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  92%|█████████▏| 530/576 [03:08<00:16,  2.82it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  92%|█████████▏| 531/576 [03:08<00:15,  2.82it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  92%|█████████▏| 532/576 [03:08<00:15,  2.82it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  93%|█████████▎| 533/576 [03:08<00:15,  2.82it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  93%|█████████▎| 534/576 [03:09<00:14,  2.82it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  93%|█████████▎| 535/576 [03:09<00:14,  2.83it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  93%|█████████▎| 536/576 [03:09<00:14,  2.83it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  93%|█████████▎| 537/576 [03:09<00:13,  2.83it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  93%|█████████▎| 538/576 [03:10<00:13,  2.83it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  94%|█████████▎| 539/576 [03:10<00:13,  2.83it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  94%|█████████▍| 540/576 [03:10<00:12,  2.83it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  94%|█████████▍| 541/576 [03:10<00:12,  2.84it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  94%|█████████▍| 542/576 [03:10<00:11,  2.84it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  94%|█████████▍| 543/576 [03:11<00:11,  2.84it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  94%|█████████▍| 544/576 [03:11<00:11,  2.84it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  95%|█████████▍| 545/576 [03:11<00:10,  2.84it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  95%|█████████▍| 546/576 [03:12<00:10,  2.84it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  95%|█████████▍| 547/576 [03:12<00:10,  2.84it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  95%|█████████▌| 548/576 [03:12<00:09,  2.84it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  95%|█████████▌| 549/576 [03:13<00:09,  2.84it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  95%|█████████▌| 550/576 [03:13<00:09,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  96%|█████████▌| 551/576 [03:13<00:08,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  96%|█████████▌| 552/576 [03:13<00:08,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  96%|█████████▌| 553/576 [03:14<00:08,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  96%|█████████▌| 554/576 [03:14<00:07,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  96%|█████████▋| 555/576 [03:14<00:07,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  97%|█████████▋| 556/576 [03:15<00:07,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  97%|█████████▋| 557/576 [03:15<00:06,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  97%|█████████▋| 558/576 [03:15<00:06,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  97%|█████████▋| 559/576 [03:16<00:05,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  97%|█████████▋| 560/576 [03:16<00:05,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  97%|█████████▋| 561/576 [03:16<00:05,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  98%|█████████▊| 562/576 [03:17<00:04,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  98%|█████████▊| 563/576 [03:17<00:04,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  98%|█████████▊| 564/576 [03:17<00:04,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  98%|█████████▊| 565/576 [03:18<00:03,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  98%|█████████▊| 566/576 [03:18<00:03,  2.85it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  98%|█████████▊| 567/576 [03:18<00:03,  2.86it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  99%|█████████▊| 568/576 [03:18<00:02,  2.86it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  99%|█████████▉| 569/576 [03:19<00:02,  2.86it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  99%|█████████▉| 570/576 [03:19<00:02,  2.86it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  99%|█████████▉| 571/576 [03:19<00:01,  2.86it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  99%|█████████▉| 572/576 [03:20<00:01,  2.86it/s, loss=2.345, v_num=0]\n",
      "Epoch 17:  99%|█████████▉| 573/576 [03:20<00:01,  2.86it/s, loss=2.345, v_num=0]\n",
      "Epoch 17: 100%|█████████▉| 574/576 [03:20<00:00,  2.86it/s, loss=2.345, v_num=0]\n",
      "Epoch 17: 100%|█████████▉| 575/576 [03:20<00:00,  2.86it/s, loss=2.345, v_num=0]\n",
      "Epoch 17: 100%|██████████| 576/576 [03:21<00:00,  2.86it/s, loss=2.345, v_num=0]Mean val Loss: 2.2460594177246094\n",
      "Epoch 17: 100%|██████████| 576/576 [03:21<00:00,  2.86it/s, loss=2.345, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.3755292892456055\n",
      "Epoch 18:  91%|█████████ | 525/576 [03:11<00:18,  2.75it/s, loss=2.286, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18:  91%|█████████▏| 526/576 [03:11<00:18,  2.75it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  91%|█████████▏| 527/576 [03:11<00:17,  2.75it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  92%|█████████▏| 528/576 [03:11<00:17,  2.75it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  92%|█████████▏| 529/576 [03:12<00:17,  2.75it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  92%|█████████▏| 530/576 [03:12<00:16,  2.76it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  92%|█████████▏| 531/576 [03:12<00:16,  2.76it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  92%|█████████▏| 532/576 [03:12<00:15,  2.76it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  93%|█████████▎| 533/576 [03:13<00:15,  2.76it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  93%|█████████▎| 534/576 [03:13<00:15,  2.76it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  93%|█████████▎| 535/576 [03:13<00:14,  2.76it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  93%|█████████▎| 536/576 [03:14<00:14,  2.76it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  93%|█████████▎| 537/576 [03:14<00:14,  2.76it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  93%|█████████▎| 538/576 [03:14<00:13,  2.76it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  94%|█████████▎| 539/576 [03:14<00:13,  2.76it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  94%|█████████▍| 540/576 [03:15<00:13,  2.77it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  94%|█████████▍| 541/576 [03:15<00:12,  2.77it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  94%|█████████▍| 542/576 [03:15<00:12,  2.77it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  94%|█████████▍| 543/576 [03:15<00:11,  2.77it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  94%|█████████▍| 544/576 [03:16<00:11,  2.77it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  95%|█████████▍| 545/576 [03:16<00:11,  2.77it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  95%|█████████▍| 546/576 [03:16<00:10,  2.77it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  95%|█████████▍| 547/576 [03:17<00:10,  2.77it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  95%|█████████▌| 548/576 [03:17<00:10,  2.78it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  95%|█████████▌| 549/576 [03:17<00:09,  2.78it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  95%|█████████▌| 550/576 [03:17<00:09,  2.78it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  96%|█████████▌| 551/576 [03:18<00:08,  2.78it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  96%|█████████▌| 552/576 [03:18<00:08,  2.78it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  96%|█████████▌| 553/576 [03:18<00:08,  2.78it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  96%|█████████▌| 554/576 [03:19<00:07,  2.78it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  96%|█████████▋| 555/576 [03:19<00:07,  2.78it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  97%|█████████▋| 556/576 [03:19<00:07,  2.79it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  97%|█████████▋| 557/576 [03:19<00:06,  2.79it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  97%|█████████▋| 558/576 [03:20<00:06,  2.79it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  97%|█████████▋| 559/576 [03:20<00:06,  2.79it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  97%|█████████▋| 560/576 [03:20<00:05,  2.79it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  97%|█████████▋| 561/576 [03:20<00:05,  2.79it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  98%|█████████▊| 562/576 [03:21<00:05,  2.79it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  98%|█████████▊| 563/576 [03:21<00:04,  2.79it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  98%|█████████▊| 564/576 [03:21<00:04,  2.80it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  98%|█████████▊| 565/576 [03:22<00:03,  2.80it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  98%|█████████▊| 566/576 [03:22<00:03,  2.80it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  98%|█████████▊| 567/576 [03:22<00:03,  2.80it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  99%|█████████▊| 568/576 [03:22<00:02,  2.80it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  99%|█████████▉| 569/576 [03:23<00:02,  2.80it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  99%|█████████▉| 570/576 [03:23<00:02,  2.80it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  99%|█████████▉| 571/576 [03:23<00:01,  2.80it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  99%|█████████▉| 572/576 [03:23<00:01,  2.81it/s, loss=2.286, v_num=0]\n",
      "Epoch 18:  99%|█████████▉| 573/576 [03:24<00:01,  2.81it/s, loss=2.286, v_num=0]\n",
      "Epoch 18: 100%|█████████▉| 574/576 [03:24<00:00,  2.81it/s, loss=2.286, v_num=0]\n",
      "Epoch 18: 100%|█████████▉| 575/576 [03:24<00:00,  2.81it/s, loss=2.286, v_num=0]\n",
      "Epoch 18: 100%|██████████| 576/576 [03:25<00:00,  2.81it/s, loss=2.286, v_num=0]Mean val Loss: 2.1074562072753906\n",
      "Epoch 18: 100%|██████████| 576/576 [03:25<00:00,  2.81it/s, loss=2.286, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.3672871589660645\n",
      "Epoch 19:  91%|█████████ | 525/576 [03:06<00:18,  2.81it/s, loss=2.331, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19:  91%|█████████▏| 526/576 [03:07<00:17,  2.81it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  91%|█████████▏| 527/576 [03:07<00:17,  2.81it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  92%|█████████▏| 528/576 [03:07<00:17,  2.81it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  92%|█████████▏| 529/576 [03:07<00:16,  2.81it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  92%|█████████▏| 530/576 [03:08<00:16,  2.82it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  92%|█████████▏| 531/576 [03:08<00:15,  2.82it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  92%|█████████▏| 532/576 [03:08<00:15,  2.82it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  93%|█████████▎| 533/576 [03:09<00:15,  2.82it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  93%|█████████▎| 534/576 [03:09<00:14,  2.82it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  93%|█████████▎| 535/576 [03:09<00:14,  2.82it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  93%|█████████▎| 536/576 [03:09<00:14,  2.82it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  93%|█████████▎| 537/576 [03:10<00:13,  2.82it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  93%|█████████▎| 538/576 [03:10<00:13,  2.83it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  94%|█████████▎| 539/576 [03:10<00:13,  2.83it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  94%|█████████▍| 540/576 [03:10<00:12,  2.83it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  94%|█████████▍| 541/576 [03:11<00:12,  2.83it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  94%|█████████▍| 542/576 [03:11<00:12,  2.83it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  94%|█████████▍| 543/576 [03:11<00:11,  2.83it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  94%|█████████▍| 544/576 [03:11<00:11,  2.83it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  95%|█████████▍| 545/576 [03:12<00:10,  2.84it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  95%|█████████▍| 546/576 [03:12<00:10,  2.84it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  95%|█████████▍| 547/576 [03:12<00:10,  2.84it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  95%|█████████▌| 548/576 [03:13<00:09,  2.84it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  95%|█████████▌| 549/576 [03:13<00:09,  2.84it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  95%|█████████▌| 550/576 [03:13<00:09,  2.84it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  96%|█████████▌| 551/576 [03:13<00:08,  2.84it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  96%|█████████▌| 552/576 [03:14<00:08,  2.84it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  96%|█████████▌| 553/576 [03:14<00:08,  2.85it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  96%|█████████▌| 554/576 [03:14<00:07,  2.85it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  96%|█████████▋| 555/576 [03:14<00:07,  2.85it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  97%|█████████▋| 556/576 [03:15<00:07,  2.85it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  97%|█████████▋| 557/576 [03:15<00:06,  2.85it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  97%|█████████▋| 558/576 [03:15<00:06,  2.85it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  97%|█████████▋| 559/576 [03:15<00:05,  2.86it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  97%|█████████▋| 560/576 [03:16<00:05,  2.86it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  97%|█████████▋| 561/576 [03:16<00:05,  2.86it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  98%|█████████▊| 562/576 [03:16<00:04,  2.86it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  98%|█████████▊| 563/576 [03:16<00:04,  2.86it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  98%|█████████▊| 564/576 [03:16<00:04,  2.86it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  98%|█████████▊| 565/576 [03:17<00:03,  2.86it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  98%|█████████▊| 566/576 [03:17<00:03,  2.86it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  98%|█████████▊| 567/576 [03:17<00:03,  2.86it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  99%|█████████▊| 568/576 [03:18<00:02,  2.87it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  99%|█████████▉| 569/576 [03:18<00:02,  2.87it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  99%|█████████▉| 570/576 [03:18<00:02,  2.87it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  99%|█████████▉| 571/576 [03:18<00:01,  2.87it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  99%|█████████▉| 572/576 [03:19<00:01,  2.87it/s, loss=2.331, v_num=0]\n",
      "Epoch 19:  99%|█████████▉| 573/576 [03:19<00:01,  2.87it/s, loss=2.331, v_num=0]\n",
      "Epoch 19: 100%|█████████▉| 574/576 [03:19<00:00,  2.88it/s, loss=2.331, v_num=0]\n",
      "Epoch 19: 100%|█████████▉| 575/576 [03:19<00:00,  2.88it/s, loss=2.331, v_num=0]\n",
      "Epoch 19: 100%|██████████| 576/576 [03:20<00:00,  2.88it/s, loss=2.331, v_num=0]Mean val Loss: 2.1948800086975098\n",
      "Epoch 19: 100%|██████████| 576/576 [03:20<00:00,  2.87it/s, loss=2.331, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.3560335636138916\n",
      "Epoch 20:  91%|█████████ | 525/576 [03:09<00:18,  2.77it/s, loss=2.387, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20:  91%|█████████▏| 526/576 [03:10<00:18,  2.77it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  91%|█████████▏| 527/576 [03:10<00:17,  2.77it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  92%|█████████▏| 528/576 [03:10<00:17,  2.77it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  92%|█████████▏| 529/576 [03:10<00:16,  2.77it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  92%|█████████▏| 530/576 [03:11<00:16,  2.77it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  92%|█████████▏| 531/576 [03:11<00:16,  2.78it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  92%|█████████▏| 532/576 [03:11<00:15,  2.78it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  93%|█████████▎| 533/576 [03:11<00:15,  2.78it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  93%|█████████▎| 534/576 [03:12<00:15,  2.78it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  93%|█████████▎| 535/576 [03:12<00:14,  2.78it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  93%|█████████▎| 536/576 [03:12<00:14,  2.78it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  93%|█████████▎| 537/576 [03:12<00:14,  2.78it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  93%|█████████▎| 538/576 [03:13<00:13,  2.79it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  94%|█████████▎| 539/576 [03:13<00:13,  2.79it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  94%|█████████▍| 540/576 [03:13<00:12,  2.79it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  94%|█████████▍| 541/576 [03:13<00:12,  2.79it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  94%|█████████▍| 542/576 [03:14<00:12,  2.79it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  94%|█████████▍| 543/576 [03:14<00:11,  2.79it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  94%|█████████▍| 544/576 [03:14<00:11,  2.80it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  95%|█████████▍| 545/576 [03:14<00:11,  2.80it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  95%|█████████▍| 546/576 [03:15<00:10,  2.80it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  95%|█████████▍| 547/576 [03:15<00:10,  2.80it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  95%|█████████▌| 548/576 [03:15<00:09,  2.80it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  95%|█████████▌| 549/576 [03:15<00:09,  2.80it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  95%|█████████▌| 550/576 [03:16<00:09,  2.80it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  96%|█████████▌| 551/576 [03:16<00:08,  2.80it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  96%|█████████▌| 552/576 [03:16<00:08,  2.81it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  96%|█████████▌| 553/576 [03:16<00:08,  2.81it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  96%|█████████▌| 554/576 [03:17<00:07,  2.81it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  96%|█████████▋| 555/576 [03:17<00:07,  2.81it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  97%|█████████▋| 556/576 [03:17<00:07,  2.81it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  97%|█████████▋| 557/576 [03:18<00:06,  2.81it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  97%|█████████▋| 558/576 [03:18<00:06,  2.81it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  97%|█████████▋| 559/576 [03:18<00:06,  2.81it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  97%|█████████▋| 560/576 [03:18<00:05,  2.81it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  97%|█████████▋| 561/576 [03:19<00:05,  2.82it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  98%|█████████▊| 562/576 [03:19<00:04,  2.82it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  98%|█████████▊| 563/576 [03:19<00:04,  2.82it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  98%|█████████▊| 564/576 [03:20<00:04,  2.82it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  98%|█████████▊| 565/576 [03:20<00:03,  2.82it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  98%|█████████▊| 566/576 [03:20<00:03,  2.82it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  98%|█████████▊| 567/576 [03:20<00:03,  2.82it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  99%|█████████▊| 568/576 [03:21<00:02,  2.83it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  99%|█████████▉| 569/576 [03:21<00:02,  2.83it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  99%|█████████▉| 570/576 [03:21<00:02,  2.83it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  99%|█████████▉| 571/576 [03:21<00:01,  2.83it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  99%|█████████▉| 572/576 [03:22<00:01,  2.83it/s, loss=2.387, v_num=0]\n",
      "Epoch 20:  99%|█████████▉| 573/576 [03:22<00:01,  2.83it/s, loss=2.387, v_num=0]\n",
      "Epoch 20: 100%|█████████▉| 574/576 [03:22<00:00,  2.83it/s, loss=2.387, v_num=0]\n",
      "Epoch 20: 100%|█████████▉| 575/576 [03:22<00:00,  2.83it/s, loss=2.387, v_num=0]\n",
      "Epoch 20: 100%|██████████| 576/576 [03:23<00:00,  2.84it/s, loss=2.387, v_num=0]Mean val Loss: 2.131131649017334\n",
      "Epoch 20: 100%|██████████| 576/576 [03:23<00:00,  2.83it/s, loss=2.387, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.3223438262939453\n",
      "Epoch 21:  91%|█████████ | 525/576 [03:03<00:17,  2.85it/s, loss=2.404, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21:  91%|█████████▏| 526/576 [03:04<00:17,  2.86it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  91%|█████████▏| 527/576 [03:04<00:17,  2.86it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  92%|█████████▏| 528/576 [03:04<00:16,  2.86it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  92%|█████████▏| 529/576 [03:04<00:16,  2.86it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  92%|█████████▏| 530/576 [03:05<00:16,  2.86it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  92%|█████████▏| 531/576 [03:05<00:15,  2.87it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  92%|█████████▏| 532/576 [03:05<00:15,  2.87it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  93%|█████████▎| 533/576 [03:05<00:14,  2.87it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  93%|█████████▎| 534/576 [03:06<00:14,  2.87it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  93%|█████████▎| 535/576 [03:06<00:14,  2.87it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  93%|█████████▎| 536/576 [03:06<00:13,  2.87it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  93%|█████████▎| 537/576 [03:06<00:13,  2.88it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  93%|█████████▎| 538/576 [03:07<00:13,  2.88it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  94%|█████████▎| 539/576 [03:07<00:12,  2.88it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  94%|█████████▍| 540/576 [03:07<00:12,  2.88it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  94%|█████████▍| 541/576 [03:07<00:12,  2.88it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  94%|█████████▍| 542/576 [03:07<00:11,  2.88it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  94%|█████████▍| 543/576 [03:08<00:11,  2.88it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  94%|█████████▍| 544/576 [03:08<00:11,  2.89it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  95%|█████████▍| 545/576 [03:08<00:10,  2.89it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  95%|█████████▍| 546/576 [03:08<00:10,  2.89it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  95%|█████████▍| 547/576 [03:09<00:10,  2.89it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  95%|█████████▌| 548/576 [03:09<00:09,  2.89it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  95%|█████████▌| 549/576 [03:09<00:09,  2.89it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  95%|█████████▌| 550/576 [03:09<00:08,  2.90it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  96%|█████████▌| 551/576 [03:10<00:08,  2.90it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  96%|█████████▌| 552/576 [03:10<00:08,  2.90it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  96%|█████████▌| 553/576 [03:10<00:07,  2.90it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  96%|█████████▌| 554/576 [03:10<00:07,  2.90it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  96%|█████████▋| 555/576 [03:11<00:07,  2.90it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  97%|█████████▋| 556/576 [03:11<00:06,  2.91it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  97%|█████████▋| 557/576 [03:11<00:06,  2.91it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  97%|█████████▋| 558/576 [03:11<00:06,  2.91it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  97%|█████████▋| 559/576 [03:12<00:05,  2.91it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  97%|█████████▋| 560/576 [03:12<00:05,  2.91it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  97%|█████████▋| 561/576 [03:12<00:05,  2.91it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  98%|█████████▊| 562/576 [03:12<00:04,  2.91it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  98%|█████████▊| 563/576 [03:13<00:04,  2.91it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  98%|█████████▊| 564/576 [03:13<00:04,  2.91it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  98%|█████████▊| 565/576 [03:13<00:03,  2.92it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  98%|█████████▊| 566/576 [03:14<00:03,  2.92it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  98%|█████████▊| 567/576 [03:14<00:03,  2.92it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  99%|█████████▊| 568/576 [03:14<00:02,  2.92it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  99%|█████████▉| 569/576 [03:14<00:02,  2.92it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  99%|█████████▉| 570/576 [03:14<00:02,  2.92it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  99%|█████████▉| 571/576 [03:15<00:01,  2.92it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  99%|█████████▉| 572/576 [03:15<00:01,  2.93it/s, loss=2.404, v_num=0]\n",
      "Epoch 21:  99%|█████████▉| 573/576 [03:15<00:01,  2.93it/s, loss=2.404, v_num=0]\n",
      "Epoch 21: 100%|█████████▉| 574/576 [03:16<00:00,  2.93it/s, loss=2.404, v_num=0]\n",
      "Epoch 21: 100%|█████████▉| 575/576 [03:16<00:00,  2.93it/s, loss=2.404, v_num=0]\n",
      "Epoch 21: 100%|██████████| 576/576 [03:16<00:00,  2.93it/s, loss=2.404, v_num=0]Mean val Loss: 2.1870346069335938\n",
      "Epoch 21: 100%|██████████| 576/576 [03:16<00:00,  2.93it/s, loss=2.404, v_num=0]\n",
      "                                                           \u001b[AMean train Loss: 2.318448543548584\n",
      "Epoch 21: 100%|██████████| 576/576 [03:38<00:00,  2.63it/s, loss=2.404, v_num=0]\n",
      "Testing: 100%|██████████| 583/583 [08:57<00:00,  1.01it/s]Mean test Loss: 2.5240981578826904\n",
      "{'testlen': 245895, 'reflen': 245145, 'guess': [245895, 226409, 210021, 197173], 'correct': [147070, 114356, 94119, 79966]}\n",
      "ratio: 1.0030594138163087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/Development/QuestionAnweringProject/MSMARCO-Question-Answering/MsmarcoQuestionAnswering/Evaluation/ms_marco_eval.py:167: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  answersimilarity += candidate_answer.similarity(nlp(answer))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-86e1e4d70c05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stop_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelLightning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Development/PythonEnv/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, test_dataloaders, ckpt_path)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m         \u001b[0;31m# on tpu, .spawn means we don't have a trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/PythonEnv/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no-cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/PythonEnv/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36msingle_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinit_scheduler_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_core_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/PythonEnv/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;31m# only load test dataloader for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[0;31m# self.reset_test_dataloader(ref_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/PythonEnv/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(self, test_mode)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;31m# enable no returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_results\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog_bar_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0;31m# add metrics to prog bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/PythonEnv/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/logging.py\u001b[0m in \u001b[0;36mprocess_output\u001b[0;34m(self, output, train)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# use every metric passed in as a candidate for callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# detach all metrics for callbacks to prevent memory leaks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "modelLightning = BidafLightningWrapper()\n",
    "early_stopping = EarlyStopping('val_loss')\n",
    "trainer = Trainer(early_stop_callback=early_stopping, gpus=1)\n",
    "trainer.fit(modelLightning)\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Checkpoint and resume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "modelLightning = BidafLightningWrapper()\n",
    "early_stopping = EarlyStopping('val_loss')\n",
    "trainer = Trainer(early_stop_callback=early_stopping, gpus=1, resume_from_checkpoint= os.path.join(args.exp_folder,\"checkpoint.ckpt\"))\n",
    "trainer.fit(modelLightning)\n",
    "#trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Pytorch Lightning Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(os.path.join(args.exp_folder,\"checkpoint.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Loss to statistics.json (to be executed directly after training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "outputfile = os.path.join(args.exp_folder,'statistics.json')\n",
    "Path(outputfile).touch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeStatisticToDict(key, output_dict, start_epoch = 0):\n",
    "    output_dict[key] = dict()\n",
    "    stat_saves = epoch_saves[key]\n",
    "\n",
    "    for idx in range(0 + start_epoch,len(stat_saves)+start_epoch):\n",
    "        output_dict[key][idx] = dict()\n",
    "        for jdx in range(0,len(stat_saves[idx])):\n",
    "            output_dict[key][idx][jdx] = stat_saves[idx][jdx].item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = dict()\n",
    "\n",
    "writeStatisticToDict('train_loss',output_dict)\n",
    "writeStatisticToDict('val_loss',output_dict)\n",
    "writeStatisticToDict('test_loss',output_dict)\n",
    "\n",
    "with open(outputfile, 'w') as write_f:\n",
    "    write_f.write(json.dumps(output_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Loss from statistics.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputfile, 'r') as read_f:\n",
    "    statistics = json.load(read_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot per Epoch Mean Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def visualizeEpochMeanLoss(key, stats, skip_first=False):\n",
    "    epoch_stats_dict = stats[key]\n",
    "    mean_loss = []\n",
    "    var_loss = []\n",
    "    first = True\n",
    "    for epochNum in epoch_stats_dict:\n",
    "        if first and skip_first:\n",
    "            first = False\n",
    "            continue\n",
    "        epoch_stats = epoch_stats_dict[epochNum].values()\n",
    "        epoch_stats = np.array(list(epoch_stats))\n",
    "        mean_loss_in_actual_epoch = np.average(epoch_stats)\n",
    "        mean_loss = mean_loss + [mean_loss_in_actual_epoch]\n",
    "        variance_loss_in_actual_epoch = np.var(epoch_stats)\n",
    "        var_loss = var_loss + [variance_loss_in_actual_epoch]\n",
    "    \n",
    "    x = np.arange(0,len(epoch_stats_dict.keys()) - skip_first)\n",
    "    plt.errorbar(x, mean_loss, yerr=var_loss, label = key)\n",
    "    \n",
    "def visualizeTestMeanLoss(train_key, test_key, stats):\n",
    "    epoch_stats_dict = stats[test_key]\n",
    "    mean_loss = []\n",
    "    epoch_stats = epoch_stats_dict['0'].values()\n",
    "    mean_loss_in_actual_epoch = sum(epoch_stats)/len(epoch_stats)\n",
    "    for epochs in stats[train_key]:\n",
    "        mean_loss = mean_loss + [mean_loss_in_actual_epoch]\n",
    "    plt.plot(mean_loss, label = test_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualizeEpochMeanLoss('train_loss', statistics)\n",
    "visualizeEpochMeanLoss('val_loss',statistics, skip_first=True)\n",
    "visualizeTestMeanLoss('train_loss','test_loss',statistics)\n",
    "plt.legend(loc='upper right', frameon=True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.ylim(1.5,4)\n",
    "plt.savefig(os.path.join(args.exp_folder, 'loss.png'))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot per Batch Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def visualizeLoss(key, stats):\n",
    "    batch_stats = stats[key]\n",
    "    epoch_stats = []\n",
    "    for key in batch_stats:\n",
    "        epoch_stats = epoch_stats + list(batch_stats[key].values())\n",
    "    plt.plot(epoch_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeLoss('train_loss', statistics)\n",
    "visualizeLoss('val_loss',statistics)\n",
    "visualizeLoss('test_loss',statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
