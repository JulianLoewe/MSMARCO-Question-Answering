{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Directional Attention Flow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencie Installation (Restart Kernel after installing!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U PyYAML\n",
    "!pip install -U h5py\n",
    "!pip install pytorch-lightning\n",
    "!pip install matplotlib\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your Experiment below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "pwd = os.getcwd()\n",
    "\n",
    "class Arguments():\n",
    "    exp_folder = os.path.join(pwd, 'Experimente/newExperimentName')\n",
    "    data = os.path.join(pwd, 'DATA', 'train_v2.1.json')\n",
    "    word_rep = os.path.join(pwd, 'DATA', 'glove.840B.300d.txt')\n",
    "    train_original_data = os.path.join(pwd, 'DATA', 'train_v2.1.json')\n",
    "    val_data = os.path.join(pwd, 'DATA', 'dev_v2.1.json')\n",
    "    cuda = torch.cuda.is_available()\n",
    "    use_covariance = False\n",
    "    force_restart = False\n",
    "    train_splitted_data = os.path.join(pwd, 'DATA', 'train_part.json')\n",
    "    test_splitted_data = os.path.join(pwd, 'DATA', 'eval_part.json')\n",
    "    val_splitted_data = os.path.join(pwd, 'DATA', 'dev_part.json')\n",
    "    test_reference_file = os.path.join(pwd, 'DATA', 'test_reference.json')\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "if not os.path.exists(args.exp_folder):\n",
    "    os.makedirs(args.exp_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Configurations (instead of config.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_yaml = \"\"\"\n",
    "    bidaf:\n",
    "        dropout: 0.2\n",
    "        num_highways: 2\n",
    "        num_lstm: 2\n",
    "        hidden_size: 100\n",
    "        embedding_dim: 300\n",
    "        embedding_reduce: 100\n",
    "        characters:\n",
    "            dim: 16\n",
    "            num_filters: 100\n",
    "            filter_sizes:\n",
    "                - 5\n",
    "    training:\n",
    "        lr: 0.001\n",
    "        betas:\n",
    "            - 0.9\n",
    "            - 0.999\n",
    "        eps: 0.00000001\n",
    "        weigth_decay: 0\n",
    "        epochs: 1\n",
    "        batch_size: 60\n",
    "        limit: 400\n",
    "\"\"\"\n",
    "config = yaml.load(config_yaml, Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the MSMARCO Bidaf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(pwd,'MsmarcoQuestionAnswering','Baseline'))\n",
    "sys.path.append(os.path.join(pwd,'MsmarcoQuestionAnswering','Baseline','scripts'))\n",
    "sys.path.append(os.path.join(pwd,'MsmarcoQuestionAnswering','Evaluation'))\n",
    "\n",
    "import MsmarcoQuestionAnswering.Baseline.scripts.checkpointing as checkpointing\n",
    "import MsmarcoQuestionAnswering.Baseline.scripts.train as train_manager\n",
    "import MsmarcoQuestionAnswering.Evaluation.ms_marco_eval as eval_manager\n",
    "#import MsmarcoQuestionAnswering.Baseline.scripts.predict as predict_manager\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MsmarcoQuestionAnswering.Baseline.mrcqa as mrcqa\n",
    "import MsmarcoQuestionAnswering.Baseline.scripts.dataset as dataset\n",
    "import json as json\n",
    "import numpy as np\n",
    "from random import shuffle, randint\n",
    "\n",
    "def try_to_split_testset(percentual_size_test, reduced_whole_size_train=1,reduced_whole_size_val=1, force_renew=False):\n",
    "    if os.path.isfile(args.train_splitted_data) and os.path.isfile(args.test_splitted_data) and os.path.isfile(args.val_splitted_data) and not force_renew:\n",
    "        return;\n",
    "    else:\n",
    "        args.force_restart = True\n",
    "        with open(args.train_original_data) as f_o:\n",
    "            train_json = json.load(f_o);\n",
    "        qids = list(train_json['query_id'].keys());\n",
    "        shuffle(qids);\n",
    "        train_size = len(qids)\n",
    "        train_size = int(reduced_whole_size_train * train_size)\n",
    "        new_train_size = int((1 - percentual_size_test) * train_size)\n",
    "        new_test_size = train_size - new_train_size\n",
    "        print(\"New Train Set has {} Datapoints\".format(new_train_size))\n",
    "        print(\"New Test Set has {} Datapoints\".format(new_test_size))\n",
    "\n",
    "        \n",
    "        qids_train = qids[0:new_train_size]\n",
    "        qids_test = qids[new_train_size:train_size]\n",
    "        \n",
    "        def copy_dict_part(old_dict, qids):\n",
    "            count = 0;\n",
    "            new_dict = dict()\n",
    "            keys = old_dict.keys()\n",
    "            for qid in qids:\n",
    "                count = count + 1;\n",
    "                if count % 10000 == 0:\n",
    "                    print('Copy progress: {}'.format(count/len(qids)))\n",
    "                for key in keys:\n",
    "                    if not key in new_dict:\n",
    "                        new_dict[key] = dict()\n",
    "                    new_dict[key][qid] = train_json[key][qid]\n",
    "            return new_dict;\n",
    "        \n",
    "        print('Start creating new train set:')\n",
    "        new_train = copy_dict_part(train_json, qids_train)\n",
    "        print('Start creating new test set:')\n",
    "        new_test = copy_dict_part(train_json, qids_test)\n",
    "        \n",
    "        with open(args.train_splitted_data, 'w') as write_f:\n",
    "            write_f.write(json.dumps(new_train))\n",
    "        with open(args.test_splitted_data, 'w') as write_f:\n",
    "            write_f.write(json.dumps(new_test))\n",
    "        \n",
    "        create_reference_file(new_test, args.test_reference_file)\n",
    "            \n",
    "        with open(args.val_data) as f_o:\n",
    "            val_json = json.load(f_o);\n",
    "        qids = list(val_json['query_id'].keys())\n",
    "        shuffle(qids)\n",
    "        val_size = len(qids)\n",
    "        new_val_size = int(reduced_whole_size_val * val_size)\n",
    "        print(\"New Validation Set has {} Datapoints\".format(new_val_size))\n",
    "\n",
    "        qids_val = qids[0:new_val_size]\n",
    "        print('Start creating new val set:')\n",
    "        new_val = copy_dict_part(val_json, qids_val)\n",
    "        #with open(args.val_splitted_data, 'w') as write_f:\n",
    "        #    write_f.write(json.dumps(new_val))\n",
    "            \n",
    "def load_data(path,limit):\n",
    "    with open(path) as f_o:\n",
    "        data, _ = dataset.load_data(json.load(f_o), span_only=True, answered_only=True, loading_limit=limit)\n",
    "    return data\n",
    "\n",
    "def create_reference_file(data_obj, reference_file_path):\n",
    "        print(\"Create test reference file\")\n",
    "        with open(reference_file_path, 'w+') as write_f:\n",
    "            for qid in data_obj[\"answers\"]:\n",
    "                try:\n",
    "                    correct = {\"query_id\": str(qid)}\n",
    "                    correct[\"answers\"] = data_obj[\"answers\"][str(qid)]\n",
    "                    write_f.write(json.dumps(correct))\n",
    "                    write_f.write(\"\\n\")\n",
    "                except KeyError:\n",
    "                    print(\"Key Error: \"+str(obj[\"query_id\"]))\n",
    "        print(\"Done creating reference file\")\n",
    "\n",
    "def init_model(id_to_token, id_to_char):\n",
    "    return mrcqa.BidafModel.from_config(config['bidaf'], id_to_token, id_to_char)\n",
    "\n",
    "def reload_model(checkpoint):\n",
    "    model, id_to_token, id_to_char = mrcqa.BidafModel.from_checkpoint(config['bidaf'], checkpoint)\n",
    "    if torch.cuda.is_available() and args.cuda:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "    return model, id_to_token, id_to_char\n",
    "\n",
    "def inverse_dict(base_dict):\n",
    "    return {tok: id_ for id_, tok in base_dict.items()}\n",
    "\n",
    "def get_loader(data, config, used_data_per_batch=1.0):\n",
    "    data = dataset.EpochGen(\n",
    "        data,\n",
    "        batch_size=config.get('training', {}).get('batch_size', 32),\n",
    "        shuffle=True,\n",
    "        used_data_per_batch=used_data_per_batch)\n",
    "    return data\n",
    "\n",
    "def get_optimizer(model, config, state):\n",
    "    \"\"\"\n",
    "    Get the optimizer\n",
    "    \"\"\"\n",
    "    parameters = filter(lambda p: p.requires_grad,\n",
    "                        model.parameters())\n",
    "    \"\"\" ADAM Optimizer\"\"\"\n",
    "    optimizer = torch.optim.Adam(\n",
    "        parameters,\n",
    "        lr=config['training'].get('lr', 0.01),\n",
    "        betas=config['training'].get('betas', (0.9, 0.999)),\n",
    "        eps=config['training'].get('eps', 1e-8),\n",
    "        weight_decay=config['training'].get('weight_decay', 0))\n",
    "    \n",
    "    \n",
    "    \"\"\" ADAGRAD Optimizer\n",
    "    optimizer = torch.optim.Adagrad(\n",
    "        parameters,\n",
    "        lr=config['training'].get('lr', 1),\n",
    "        weight_decay=config['training'].get('weight_decay', 0))\n",
    "    \"\"\" \n",
    "    \n",
    "    \"\"\" ADADELTA Optimizer \n",
    "    optimizer = torch.optim.Adadelta(\n",
    "        parameters,\n",
    "        lr=0.5)\"\"\"\n",
    "    \n",
    "    if state is not None:\n",
    "        optimizer.load_state_dict(state)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "def load_pretrained_embeddings(path, model, id_to_token):\n",
    "    with open(path) as f_o:\n",
    "            pre_trained = dataset.SymbolEmbSourceText(f_o, set(tok for id_, tok in id_to_token.items() if id_ != 0))\n",
    "    mean, cov = pre_trained.get_norm_stats(args.use_covariance)\n",
    "    rng = np.random.RandomState(2)\n",
    "    oovs = dataset.SymbolEmbSourceNorm(mean, cov, rng, args.use_covariance)\n",
    "    model.embedder.embeddings[0].embeddings.weight.data = torch.from_numpy(dataset.symbol_injection(id_to_token, 0, model.embedder.embeddings[0].embeddings.weight.data.numpy(), pre_trained, oovs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_init(train_path, val_path, test_path, config, args, loading_limit=None, used_data_per_train_epoch=1.0):\n",
    "    token_to_id = {'': 0}\n",
    "    char_to_id = {'': 0}\n",
    "    print('Load Train Data [1/6]')\n",
    "    train_data = load_data(train_path,loading_limit)\n",
    "    print('Load Validation Data [1/6]')\n",
    "    val_data = load_data(val_path,loading_limit)\n",
    "    print('Load Test Data [1/6]')\n",
    "    test_data = load_data(test_path,loading_limit)\n",
    "    \n",
    "    print('Tokenize Train Data [2/6]')\n",
    "    train_data = dataset.tokenize_data(train_data, token_to_id, char_to_id)\n",
    "    print('Tokenize Validation Data [2/6]')\n",
    "    val_data = dataset.tokenize_data(val_data, token_to_id, char_to_id)\n",
    "    print('Tokenize Test Data [2/6]')\n",
    "    test_data = dataset.tokenize_data(test_data, token_to_id, char_to_id)\n",
    "    \n",
    "    train_loader = get_loader(train_data, config, used_data_per_batch=used_data_per_train_epoch)\n",
    "    val_loader = get_loader(val_data, config) #, used_data_per_batch=used_data_per_train_epoch)\n",
    "    test_loader = get_loader(test_data, config)\n",
    "\n",
    "    print('Create Inverse Dictionaries [3/6]')\n",
    "    id_to_token = inverse_dict(token_to_id)\n",
    "    id_to_char = inverse_dict(char_to_id)\n",
    "\n",
    "    print('Initiate Model [4/6]')\n",
    "    model = init_model(id_to_token, id_to_char)\n",
    "\n",
    "    if args.word_rep:\n",
    "        print('Load pre-trained embeddings [5/6]')\n",
    "        load_pretrained_embeddings(args.word_rep, model, id_to_token)\n",
    "    else:\n",
    "        print('No pre-trained embeddings given [5/6]')\n",
    "        pass  # No pretraining, just keep the random values.\n",
    "\n",
    "    if torch.cuda.is_available() and args.cuda:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = get_optimizer(model, config, state=None)\n",
    "    print('Done init_state [6/6]')\n",
    "    return model, id_to_token, id_to_char, optimizer, train_loader, val_loader, test_loader   \n",
    "\n",
    "\n",
    "def new_reload(train_path, val_path, test_path, checkpoint, training_state, config, args,loading_limit=None, used_data_per_train_epoch=1.0):\n",
    "    print('Load Model from Checkpoint [1/5]')\n",
    "    model, id_to_token, id_to_char = reload_model(checkpoint)\n",
    "\n",
    "    optimizer = get_optimizer(model, config, training_state)\n",
    "\n",
    "    print('Create Inverse Dictionaries [2/5]')\n",
    "    token_to_id = inverse_dict(id_to_token)\n",
    "    char_to_id = inverse_dict(id_to_char)\n",
    "\n",
    "    len_tok_voc = len(token_to_id)\n",
    "    len_char_voc = len(char_to_id)\n",
    "\n",
    "    print('Load Train Data [3/5]')\n",
    "    train_data = load_data(train_path,loading_limit)\n",
    "    print('Load Validation Data [3/5]')\n",
    "    val_data = load_data(val_path,loading_limit)\n",
    "    print('Load Test Data [3/5]')\n",
    "    test_data = load_data(test_path,loading_limit)\n",
    "    \n",
    "    limit_passage = config.get('training', {}).get('limit')\n",
    "\n",
    "    print('Tokenize Train Data [4/5]')\n",
    "    train_data = dataset.tokenize_data(train_data, token_to_id, char_to_id)\n",
    "    print('Tokenize Validation Data [4/5]')\n",
    "    val_data = dataset.tokenize_data(val_data, token_to_id, char_to_id)\n",
    "    print('Tokenize Test Data [4/5]')\n",
    "    test_data = dataset.tokenize_data(test_data, token_to_id, char_to_id)\n",
    "\n",
    "    train_loader = get_loader(train_data, config, used_data_per_batch=used_data_per_train_epoch)\n",
    "    val_loader = get_loader(val_data, config) #, used_data_per_batch=used_data_per_train_epoch)\n",
    "    test_loader = get_loader(test_data, config)\n",
    "\n",
    "    assert len(token_to_id) == len_tok_voc\n",
    "    assert len(char_to_id) == len_char_voc\n",
    "\n",
    "    print('Done reload_state [5/5]')\n",
    "    return model, id_to_token, id_to_char, optimizer, train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here the Loading,Splitting,Organizing and Tokenization of the given Data starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first lines can be used to modify the Dataset size used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENTAGE_OF_DATA_TO_USE = 1 #(alpha)\n",
    "PERCENTUAL_SIZE_OF_TEST_SET = 0.1 #(beta) #FEST\n",
    "PERCENTAGE_OF_REDUCED_DATA_TO_USE_PER_EPOCH = 0.1 #(gamma)\n",
    "\n",
    "try_to_split_testset(PERCENTUAL_SIZE_OF_TEST_SET,reduced_whole_size_train=PERCENTAGE_OF_DATA_TO_USE, reduced_whole_size_val=0.1, force_renew= True); #We use 100% of the given Data. And 10% of the Training Data will be used as Test Data. True means force rewrite Datasets.\n",
    "\n",
    "checkpoint_w, training_state_w, epoch_w = train_manager.try_to_resume(\n",
    "            args.force_restart, args.exp_folder)\n",
    "\n",
    "if checkpoint_w:\n",
    "    print('Resuming training...')\n",
    "    model_w, id_to_token_w, id_to_char_w, optimizer_w, train_loader, val_loader, test_loader = new_reload(args.train_splitted_data, args.val_splitted_data, \n",
    "                                                                                                          args.test_splitted_data, checkpoint_w, \n",
    "                                                                                                          training_state_w, config, args, used_data_per_train_epoch=PERCENTAGE_OF_REDUCED_DATA_TO_USE_PER_EPOCH)\n",
    "else:\n",
    "    print('Preparing to train...')\n",
    "    model_w, id_to_token_w, id_to_char_w, optimizer_w, train_loader, val_loader, test_loader = new_init(args.train_splitted_data, args.val_splitted_data, \n",
    "                                                                                                        args.test_splitted_data,config, args, used_data_per_train_epoch=PERCENTAGE_OF_REDUCED_DATA_TO_USE_PER_EPOCH)\n",
    "    checkpoint_w = h5py.File(os.path.join(args.exp_folder, 'checkpoint'))\n",
    "    checkpointing.save_vocab(checkpoint_w, 'vocab', id_to_token_w)\n",
    "    checkpointing.save_vocab(checkpoint_w, 'c_vocab', id_to_char_w)\n",
    "\n",
    "if torch.cuda.is_available() and args.cuda:\n",
    "    train_loader.tensor_type = torch.cuda.LongTensor\n",
    "    val_loader.tensor_type = torch.cuda.LongTensor\n",
    "    test_loader.tensor_type = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pytorch Lightning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_saves = dict();\n",
    "epoch_saves['train_loss'] = []\n",
    "epoch_saves['val_loss'] = []\n",
    "epoch_saves['test_loss'] = []\n",
    "\n",
    "#Used for test evaluation\n",
    "qid2candidate = {}\n",
    "\n",
    "import re\n",
    "regex_drop_char = re.compile('[^a-z0-9\\s]+')\n",
    "regex_multi_space = re.compile('\\s+')\n",
    "\n",
    "class BidafLightningWrapper(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def setup(self,stage):\n",
    "        pass;\n",
    "            \n",
    "    def prepare_data(self):\n",
    "        pass;\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optimizer_w;\n",
    "\n",
    "    def forward(self, passage, p_lengths, question, q_lengths):\n",
    "        return model_w(passage, p_lengths, question, q_lengths)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_loader;\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return val_loader;\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return test_loader;\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        qids, passages, queries, answers, _ = batch\n",
    "        start_log_probs, end_log_probs = self(passages[:2], passages[2], queries[:2], queries[2])\n",
    "        loss = model_w.get_loss(start_log_probs, end_log_probs, answers[:, 0], answers[:, 1])\n",
    "        return {'loss': loss, 'train_loss': loss, 'log': {'train_loss': loss}}\n",
    "\n",
    "    def training_epoch_end(self, results):\n",
    "        checkpointing.checkpoint(model_w, epoch_w, optimizer_w, checkpoint_w, args.exp_folder)\n",
    "        model_w.cuda()\n",
    "        mean_loss = self.save_statistics('train',results)\n",
    "        return {'log': {'train_loss': mean_loss}}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        qids, passages, queries, answers, _ = batch\n",
    "        start_log_probs, end_log_probs = self(passages[:2], passages[2], queries[:2], queries[2])\n",
    "        loss = model_w.get_loss(start_log_probs, end_log_probs, answers[:, 0], answers[:, 1])\n",
    "        return {'val_loss': loss, 'log': {'val_loss': loss}}\n",
    "    \n",
    "    def validation_epoch_end(self, results):\n",
    "        val_loss_mean = self.save_statistics('val',results)\n",
    "        return {'val_loss': val_loss_mean}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        qids, passages, queries, answers, mappings = batch\n",
    "        start_log_probs, end_log_probs = self(passages[:2], passages[2], queries[:2], queries[2])\n",
    "        loss = model_w.get_loss(start_log_probs, end_log_probs, answers[:, 0], answers[:, 1])\n",
    "\n",
    "        predictions = model_w.get_best_span(start_log_probs, end_log_probs)\n",
    "        predictions = predictions.cpu()\n",
    "        passages = passages[0].cpu().data\n",
    "        for qid, mapping, tokens, pred in zip(qids, mappings, passages, predictions):\n",
    "            toks = tokens[pred[0]:pred[1]]\n",
    "            start = mapping[pred[0], 0]\n",
    "            end = mapping[pred[1]-1, 1]\n",
    "            toks = regex_multi_space.sub(' ', regex_drop_char.sub(' ', ' '.join(id_to_token_w[int(tok)] for tok in toks).lower())).strip()\n",
    "            if qid not in qid2candidate:\n",
    "                qid2candidate[qid] = []\n",
    "            qid2candidate[qid].append(str(toks))\n",
    "        return {'test_loss': loss}\n",
    "\n",
    "    def test_epoch_end(self, results):\n",
    "        no_ans_set = set()\n",
    "        out_dict = {}\n",
    "        \n",
    "        #print(\"\\t no answer set\")\n",
    "        for qid in qid2candidate:\n",
    "            if len(qid2candidate[qid]) < 1 or 'No Answer Present.' in qid2candidate[qid]:\n",
    "                no_ans_set.add(qid)\n",
    "        #print(\"\\t take random answer from possible ones\")\n",
    "        for qid in qid2candidate:\n",
    "            pick = randint(0,len(qid2candidate[qid])-1)\n",
    "            out_dict[qid] = [qid2candidate[qid][pick]]\n",
    "        \n",
    "        mean_test_loss = self.save_statistics('test',results)\n",
    "        test_metrics = eval_manager.compute_metrics_from_model(args.test_reference_file, out_dict, no_ans_set)\n",
    "        outputfile = os.path.join(args.exp_folder,'metrics.json')\n",
    "        with open(outputfile,'w+') as f_o:\n",
    "            f_o.write(json.dumps(test_metrics))\n",
    "        return {'log': {'test_loss': mean_test_loss}}\n",
    "\n",
    "    def save_statistics(self, phase, results):\n",
    "        key = phase + '_loss'\n",
    "        mean_loss = torch.stack([step[key] for step in results]).mean()\n",
    "        print(\"Mean {} Loss: {}\".format(phase,mean_loss))\n",
    "        #print(epoch_saves.keys())\n",
    "        epoch_saves[key].append([step[key] for step in results])\n",
    "        return mean_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start new Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "modelLightning = BidafLightningWrapper()\n",
    "#-------Early Stopping-------\n",
    "early_stopping = EarlyStopping('val_loss', patience=3)\n",
    "trainer = Trainer(gpus=1, early_stop_callback=early_stopping)\n",
    "trainer.fit(modelLightning)\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Checkpoint and resume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "modelLightning = BidafLightningWrapper()\n",
    "early_stopping = EarlyStopping('val_loss')\n",
    "trainer = Trainer(gpus=1, resume_from_checkpoint= os.path.join(args.exp_folder,\"checkpoint.ckpt\"))\n",
    "trainer.fit(modelLightning)\n",
    "trainer.test(modelLightning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Pytorch Lightning Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(os.path.join(args.exp_folder,\"checkpoint.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Loss to statistics.json (to be executed directly after training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "outputfile = os.path.join(args.exp_folder,'statistics.json')\n",
    "Path(outputfile).touch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeStatisticToDict(key, output_dict, start_epoch = 0):\n",
    "    output_dict[key] = dict()\n",
    "    stat_saves = epoch_saves[key]\n",
    "\n",
    "    for idx in range(0 + start_epoch,len(stat_saves)+start_epoch):\n",
    "        output_dict[key][idx] = dict()\n",
    "        for jdx in range(0,len(stat_saves[idx])):\n",
    "            output_dict[key][idx][jdx] = stat_saves[idx][jdx].item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = dict()\n",
    "\n",
    "writeStatisticToDict('train_loss',output_dict)\n",
    "writeStatisticToDict('val_loss',output_dict)\n",
    "writeStatisticToDict('test_loss',output_dict)\n",
    "\n",
    "with open(outputfile, 'w') as write_f:\n",
    "    write_f.write(json.dumps(output_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Loss from statistics.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "outputfile = os.path.join(args.exp_folder,'statistics.json')\n",
    "with open(outputfile, 'r') as read_f:\n",
    "    statistics = json.load(read_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot per Epoch Mean Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def visualizeEpochMeanLoss(key, stats, skip_first=False, plot_variance=True, color = None, label = None):\n",
    "    \n",
    "    if not label:\n",
    "        label = key\n",
    "    \n",
    "    epoch_stats_dict = stats[key]\n",
    "    mean_loss = []\n",
    "    var_loss = []\n",
    "    first = True\n",
    "    for epochNum in epoch_stats_dict:\n",
    "        if first and skip_first:\n",
    "            first = False\n",
    "            continue\n",
    "        epoch_stats = epoch_stats_dict[epochNum].values()\n",
    "        epoch_stats = np.array(list(epoch_stats))\n",
    "        mean_loss_in_actual_epoch = np.average(epoch_stats)\n",
    "        mean_loss = mean_loss + [mean_loss_in_actual_epoch]\n",
    "        variance_loss_in_actual_epoch = np.var(epoch_stats)\n",
    "        var_loss = var_loss + [variance_loss_in_actual_epoch]\n",
    "    \n",
    "    x = np.arange(0,len(epoch_stats_dict.keys()) - skip_first)\n",
    "    if plot_variance:\n",
    "        plt.errorbar(x, mean_loss, yerr=var_loss, label = label)\n",
    "    else:\n",
    "        if color:\n",
    "            plt.plot(x,mean_loss, label = label, color = color)\n",
    "        else:    \n",
    "            plt.plot(x,mean_loss, label = label)\n",
    "    \n",
    "def visualizeTestMeanLoss(train_key, test_key, stats):\n",
    "    epoch_stats_dict = stats[test_key]\n",
    "    mean_loss = []\n",
    "    epoch_stats = epoch_stats_dict['0'].values()\n",
    "    mean_loss_in_actual_epoch = sum(epoch_stats)/len(epoch_stats)\n",
    "    for epochs in stats[train_key]:\n",
    "        mean_loss = mean_loss + [mean_loss_in_actual_epoch]\n",
    "    plt.plot(mean_loss, label = test_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeEpochMeanLoss('train_loss', statistics)\n",
    "visualizeEpochMeanLoss('val_loss',statistics, skip_first=True)\n",
    "visualizeTestMeanLoss('train_loss','test_loss',statistics)\n",
    "plt.legend(loc='upper right', frameon=True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(3,6)\n",
    "plt.savefig(os.path.join(args.exp_folder, 'loss.png'))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot per Batch Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def visualizeLoss(key, stats):\n",
    "    batch_stats = stats[key]\n",
    "    epoch_stats = []\n",
    "    for key in batch_stats:\n",
    "        epoch_stats = epoch_stats + list(batch_stats[key].values())\n",
    "    plt.plot(epoch_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeLoss('train_loss', statistics)\n",
    "visualizeLoss('val_loss',statistics)\n",
    "visualizeLoss('test_loss',statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Multiple Validation Curves (Requires LearnCurve_01,LearnCurve_02,LearnCurve_03,LearnCurve_04,LearnCurve_05,LearnCurve_06 and EarlyStoppingAdam experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "pwd = os.getcwd()\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for i in range(1,7,1):\n",
    "    path= os.path.join(pwd, 'Experimente/LearnCurve_0' + str(i),'statistics.json')\n",
    "    with open(path, 'r') as read_f:\n",
    "        statistics = json.load(read_f)\n",
    "    visualizeEpochMeanLoss('val_loss',statistics, skip_first=True,plot_variance=False, color = (0,0,(i)/8,(i)/8), label = '0.' + str(i))\n",
    "    \n",
    "path = os.path.join(pwd, 'Experimente/EarlyStoppingAdam', 'statistics.json')\n",
    "with open(path, 'r') as read_f:\n",
    "    statistics = json.load(read_f)\n",
    "visualizeEpochMeanLoss('val_loss',statistics, skip_first=True,plot_variance=False, color = (0,0,8/8,(8)/8), label = '1' )\n",
    "\n",
    "plt.legend(loc='upper right', frameon=True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(2,3.5)\n",
    "plt.savefig(os.path.join(pwd, 'multiple_vals_1.png'))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Learn Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "pwd = os.getcwd()\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def getLastMeanVar(key, stats):\n",
    "    epoch_stats_dict = stats[key]\n",
    "    first = True\n",
    "    epochNum = list(epoch_stats_dict.keys()).pop()\n",
    "    epoch_stats = epoch_stats_dict[epochNum].values()\n",
    "    epoch_stats = np.array(list(epoch_stats))\n",
    "    mean_loss_in_actual_epoch = np.average(epoch_stats)\n",
    "    variance_loss_in_actual_epoch = np.var(epoch_stats)\n",
    "    return (mean_loss_in_actual_epoch,variance_loss_in_actual_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLastMeanVar(key):\n",
    "    mean_loss = []\n",
    "    var_loss = []\n",
    "    x = []\n",
    "    for i in range(1,7,1):\n",
    "        path= os.path.join(pwd, 'Experimente/LearnCurve_0' + str(i),'statistics.json')\n",
    "        with open(path, 'r') as read_f:\n",
    "            statistics = json.load(read_f)\n",
    "        mean,var = getLastMeanVar(key,statistics)\n",
    "        mean_loss = mean_loss + [mean]\n",
    "        var_loss = var_loss + [var]  \n",
    "        x = x + ['0.'+str(i)]\n",
    "    path = os.path.join(pwd, 'Experimente/EarlyStoppingAdam', 'statistics.json')\n",
    "    with open(path, 'r') as read_f:\n",
    "        statistics = json.load(read_f)\n",
    "    mean,var = getLastMeanVar('val_loss',statistics)\n",
    "    mean_loss = mean_loss + [mean]\n",
    "    var_loss = var_loss + [var]\n",
    "    x = x + ['1']\n",
    "\n",
    "    plt.errorbar(x, mean_loss, yerr=var_loss, label = key)\n",
    "\n",
    "plotLastMeanVar('val_loss')\n",
    "plotLastMeanVar('train_loss')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(loc='upper right', frameon=True)\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0,3.5)\n",
    "plt.savefig(os.path.join(pwd, 'learncurve.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a Summary of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model_w.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
