{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Directional Attention Flow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Checkpoint and Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: PyYAML in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (5.3.1)\n",
      "Requirement already up-to-date: h5py in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from h5py) (1.19.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from h5py) (1.15.0)\n",
      "Requirement already satisfied: pytorch-lightning in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (0.8.4)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pytorch-lightning) (5.3.1)\n",
      "Requirement already satisfied: tensorboard>=1.14 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pytorch-lightning) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pytorch-lightning) (1.19.0)\n",
      "Requirement already satisfied: torch>=1.3 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pytorch-lightning) (1.5.1+cu101)\n",
      "Requirement already satisfied: future>=0.17.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pytorch-lightning) (0.18.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pytorch-lightning) (4.47.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.4.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.34.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.18.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.30.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (3.2.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.9.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.15.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (44.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (2.24.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from tensorboard>=1.14->pytorch-lightning) (3.12.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: matplotlib in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (3.2.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from matplotlib) (1.19.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U PyYAML\n",
    "!pip install -U h5py\n",
    "!pip install pytorch-lightning\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "pwd = os.getcwd()\n",
    "\n",
    "class Arguments():\n",
    "    data = os.path.join(pwd, 'DATA', 'train_v2.1.json')\n",
    "    exp_folder = os.path.join(pwd, 'Experimente/LightningTest')\n",
    "    word_rep = os.path.join(pwd, 'DATA', 'glove.840B.300d.txt')\n",
    "    #word_rep = None\n",
    "    cuda = torch.cuda.is_available()\n",
    "    use_covariance = False\n",
    "    force_restart = False\n",
    "    train_original_data = os.path.join(pwd, 'DATA', 'train_v2.1.json')\n",
    "    train_splitted_data = os.path.join(pwd, 'DATA', 'train_part.json')\n",
    "    val_data = os.path.join(pwd, 'DATA', 'dev_v2.1.json')\n",
    "    test_splitted_data = os.path.join(pwd, 'DATA', 'eval_part.json')\n",
    "    val_splitted_data = os.path.join(pwd, 'DATA', 'dev_part.json')\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "if not os.path.exists(args.exp_folder):\n",
    "    os.makedirs(args.exp_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Configurations (instead of config.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_yaml = \"\"\"\n",
    "    bidaf:\n",
    "        dropout: 0.2\n",
    "        num_highways: 2\n",
    "        num_lstm: 2\n",
    "        hidden_size: 100\n",
    "        embedding_dim: 300\n",
    "        embedding_reduce: 100\n",
    "        characters:\n",
    "            dim: 16\n",
    "            num_filters: 100\n",
    "            filter_sizes:\n",
    "                - 5\n",
    "    training:\n",
    "        lr: 0.001\n",
    "        betas:\n",
    "            - 0.9\n",
    "            - 0.999\n",
    "        eps: 0.00000001\n",
    "        weigth_decay: 0\n",
    "        epochs: 1\n",
    "        batch_size: 60\n",
    "        limit: 400\n",
    "\"\"\"\n",
    "config = yaml.load(config_yaml, Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the MSMARCO Bidaf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(pwd,'MsmarcoQuestionAnswering','Baseline'))\n",
    "sys.path.append(os.path.join(pwd,'MsmarcoQuestionAnswering','Baseline','scripts'))\n",
    "\n",
    "import MsmarcoQuestionAnswering.Baseline.scripts.checkpointing as checkpointing\n",
    "import MsmarcoQuestionAnswering.Baseline.scripts.train as train_manager\n",
    "#import MsmarcoQuestionAnswering.Baseline.scripts.predict as predict_manager\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MsmarcoQuestionAnswering.Baseline.mrcqa as mrcqa\n",
    "import MsmarcoQuestionAnswering.Baseline.scripts.dataset as dataset\n",
    "import json as json\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "def try_to_split_testset(percentual_size_test, reduced_whole_size=1, force_renew=False):\n",
    "    if os.path.isfile(args.train_splitted_data) and os.path.isfile(args.test_splitted_data) and os.path.isfile(args.val_splitted_data) and not force_renew:\n",
    "        return;\n",
    "    else:\n",
    "        args.force_restart = True\n",
    "        with open(args.train_original_data) as f_o:\n",
    "            train_json = json.load(f_o);\n",
    "        qids = list(train_json['query_id'].keys());\n",
    "        shuffle(qids);\n",
    "        train_size = len(qids)\n",
    "        train_size = int(reduced_whole_size * train_size)\n",
    "        new_train_size = int((1 - percentual_size_test) * train_size)\n",
    "        new_test_size = train_size - new_train_size\n",
    "        print(\"New Train Set has {} Datapoints\".format(new_train_size))\n",
    "        print(\"New Test Set has {} Datapoints\".format(new_test_size))\n",
    "\n",
    "        \n",
    "        qids_train = qids[0:new_train_size]\n",
    "        qids_test = qids[new_train_size:train_size]\n",
    "        \n",
    "        def copy_dict_part(old_dict, qids):\n",
    "            count = 0;\n",
    "            new_dict = dict()\n",
    "            keys = old_dict.keys()\n",
    "            for qid in qids:\n",
    "                count = count + 1;\n",
    "                if count % 10000 == 0:\n",
    "                    print('Copy progress: {}'.format(count/len(qids)))\n",
    "                for key in keys:\n",
    "                    if not key in new_dict:\n",
    "                        new_dict[key] = dict()\n",
    "                    new_dict[key][qid] = train_json[key][qid]\n",
    "            return new_dict;\n",
    "        \n",
    "        print('Start creating new train set:')\n",
    "        new_train = copy_dict_part(train_json, qids_train)\n",
    "        print('Start creating new test set:')\n",
    "        new_test = copy_dict_part(train_json, qids_test)\n",
    "        \n",
    "        with open(args.train_splitted_data, 'w') as write_f:\n",
    "            write_f.write(json.dumps(new_train))\n",
    "        with open(args.test_splitted_data, 'w') as write_f:\n",
    "            write_f.write(json.dumps(new_test))\n",
    "            \n",
    "        with open(args.val_data) as f_o:\n",
    "            val_json = json.load(f_o);\n",
    "        qids = list(val_json['query_id'].keys())\n",
    "        shuffle(qids)\n",
    "        val_size = len(qids)\n",
    "        new_val_size = int(reduced_whole_size * val_size)\n",
    "        print(\"New Validation Set has {} Datapoints\".format(new_val_size))\n",
    "\n",
    "        qids_val = qids[0:new_val_size]\n",
    "        print('Start creating new val set:')\n",
    "        new_val = copy_dict_part(val_json, qids_val)\n",
    "        with open(args.val_splitted_data, 'w') as write_f:\n",
    "            write_f.write(json.dumps(new_val))\n",
    "            \n",
    "def load_data(path,limit):\n",
    "    with open(path) as f_o:\n",
    "        data, _ = dataset.load_data(json.load(f_o), span_only=True, answered_only=True, loading_limit=limit)\n",
    "    return data\n",
    "\n",
    "def init_model(id_to_token, id_to_char):\n",
    "    return mrcqa.BidafModel.from_config(config['bidaf'], id_to_token, id_to_char)\n",
    "\n",
    "def reload_model(checkpoint):\n",
    "    model, id_to_token, id_to_char = mrcqa.BidafModel.from_checkpoint(config['bidaf'], checkpoint)\n",
    "    if torch.cuda.is_available() and args.cuda:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "    return model, id_to_token, id_to_char\n",
    "\n",
    "def inverse_dict(base_dict):\n",
    "    return {tok: id_ for id_, tok in base_dict.items()}\n",
    "\n",
    "def get_loader(data, config, used_data_per_batch=1.0):\n",
    "    data = dataset.EpochGen(\n",
    "        data,\n",
    "        batch_size=config.get('training', {}).get('batch_size', 32),\n",
    "        shuffle=True,\n",
    "        used_data_per_batch=used_data_per_batch)\n",
    "    return data\n",
    "\n",
    "def get_optimizer(model, config, state):\n",
    "    \"\"\"\n",
    "    Get the optimizer\n",
    "    \"\"\"\n",
    "    parameters = filter(lambda p: p.requires_grad,\n",
    "                        model.parameters())\n",
    "    optimizer = torch.optim.Adam(\n",
    "        parameters,\n",
    "        lr=config['training'].get('lr', 0.01),\n",
    "        betas=config['training'].get('betas', (0.9, 0.999)),\n",
    "        eps=config['training'].get('eps', 1e-8),\n",
    "        weight_decay=config['training'].get('weight_decay', 0))\n",
    "\n",
    "    if state is not None:\n",
    "        optimizer.load_state_dict(state)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "def load_pretrained_embeddings(path, model, id_to_token):\n",
    "    with open(path) as f_o:\n",
    "            pre_trained = dataset.SymbolEmbSourceText(f_o, set(tok for id_, tok in id_to_token.items() if id_ != 0))\n",
    "    mean, cov = pre_trained.get_norm_stats(args.use_covariance)\n",
    "    rng = np.random.RandomState(2)\n",
    "    oovs = dataset.SymbolEmbSourceNorm(mean, cov, rng, args.use_covariance)\n",
    "    model.embedder.embeddings[0].embeddings.weight.data = torch.from_numpy(dataset.symbol_injection(id_to_token, 0, model.embedder.embeddings[0].embeddings.weight.data.numpy(), pre_trained, oovs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_init(train_path, val_path, test_path, config, args, loading_limit=None, used_data_per_train_batch=1.0):\n",
    "    token_to_id = {'': 0}\n",
    "    char_to_id = {'': 0}\n",
    "    print('Load Train Data [1/6]')\n",
    "    train_data = load_data(train_path,loading_limit)\n",
    "    print('Load Validation Data [1/6]')\n",
    "    val_data = load_data(val_path,loading_limit)\n",
    "    print('Load Test Data [1/6]')\n",
    "    test_data = load_data(test_path,loading_limit)\n",
    "    \n",
    "    print('Tokenize Train Data [2/6]')\n",
    "    train_data = dataset.tokenize_data(train_data, token_to_id, char_to_id)\n",
    "    print('Tokenize Validation Data [2/6]')\n",
    "    val_data = dataset.tokenize_data(val_data, token_to_id, char_to_id)\n",
    "    print('Tokenize Test Data [2/6]')\n",
    "    test_data = dataset.tokenize_data(test_data, token_to_id, char_to_id)\n",
    "    \n",
    "    train_loader = get_loader(train_data, config, used_data_per_batch=used_data_per_train_batch)\n",
    "    val_loader = get_loader(val_data, config)\n",
    "    test_loader = get_loader(test_data, config)\n",
    "\n",
    "    print('Create Inverse Dictionaries [3/6]')\n",
    "    id_to_token = inverse_dict(token_to_id)\n",
    "    id_to_char = inverse_dict(char_to_id)\n",
    "\n",
    "    print('Initiate Model [4/6]')\n",
    "    model = init_model(id_to_token, id_to_char)\n",
    "\n",
    "    if args.word_rep:\n",
    "        print('Load pre-trained embeddings [5/6]')\n",
    "        load_pretrained_embeddings(args.word_rep, model, id_to_token)\n",
    "    else:\n",
    "        print('No pre-trained embeddings given [5/6]')\n",
    "        pass  # No pretraining, just keep the random values.\n",
    "\n",
    "    if torch.cuda.is_available() and args.cuda:\n",
    "        model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = get_optimizer(model, config, state=None)\n",
    "    print('Done init_state [6/6]')\n",
    "    return model, id_to_token, id_to_char, optimizer, train_loader, val_loader, test_loader   \n",
    "\n",
    "\n",
    "def new_reload(train_path, val_path, test_path, checkpoint, training_state, config, args,loading_limit=None, used_data_per_train_batch=1.0):\n",
    "    print('Load Model from Checkpoint [1/5]')\n",
    "    model, id_to_token, id_to_char = reload_model(checkpoint)\n",
    "\n",
    "    optimizer = get_optimizer(model, config, training_state)\n",
    "\n",
    "    print('Create Inverse Dictionaries [2/5]')\n",
    "    token_to_id = inverse_dict(id_to_token)\n",
    "    char_to_id = inverse_dict(id_to_char)\n",
    "\n",
    "    len_tok_voc = len(token_to_id)\n",
    "    len_char_voc = len(char_to_id)\n",
    "\n",
    "    print('Load Train Data [3/5]')\n",
    "    train_data = load_data(train_path,loading_limit)\n",
    "    print('Load Validation Data [3/5]')\n",
    "    val_data = load_data(val_path,loading_limit)\n",
    "    print('Load Test Data [1/6]')\n",
    "    test_data = load_data(test_path,loading_limit)\n",
    "    \n",
    "    limit_passage = config.get('training', {}).get('limit')\n",
    "\n",
    "    print('Tokenize Train Data [4/5]')\n",
    "    train_data = dataset.tokenize_data(train_data, token_to_id, char_to_id)\n",
    "    print('Tokenize Validation Data [4/5]')\n",
    "    val_data = dataset.tokenize_data(val_data, token_to_id, char_to_id)\n",
    "    print('Tokenize Test Data [4/5]')\n",
    "    test_data = dataset.tokenize_data(test_data, token_to_id, char_to_id)\n",
    "\n",
    "    train_loader = get_loader(train_data, config, used_data_per_batch=used_data_per_train_batch)\n",
    "    val_loader = get_loader(val_data, config)\n",
    "    test_loader = get_loader(test_data, config)\n",
    "\n",
    "    assert len(token_to_id) == len_tok_voc\n",
    "    assert len(char_to_id) == len_char_voc\n",
    "\n",
    "    print('Done reload_state [5/5]')\n",
    "    return model, id_to_token, id_to_char, optimizer, train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch Lightning Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/Development/QuestionAnweringProject/MSMARCO-Question-Answering/MsmarcoQuestionAnswering/Baseline/scripts/train.py:31: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  checkpoint = h5py.File(exp_folder + '/checkpoint')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training...\n",
      "Load Model from Checkpoint [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages/torch/nn/modules/rnn.py:47: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Inverse Dictionaries [2/5]\n",
      "Load Train Data [3/5]\n",
      "Start Organizing Data...\n",
      "Organizing progress: 0.0 x 10⁴\n",
      "Organizing progress: 1.0 x 10⁴\n",
      "Organizing progress: 2.0 x 10⁴\n",
      "Organizing progress: 3.0 x 10⁴\n",
      "Organizing progress: 4.0 x 10⁴\n",
      "Organizing progress: 5.0 x 10⁴\n",
      "Organizing progress: 6.0 x 10⁴\n",
      "Organizing progress: 7.0 x 10⁴\n",
      "Organizing progress: 8.0 x 10⁴\n",
      "Organizing progress: 9.0 x 10⁴\n",
      "Organizing progress: 10.0 x 10⁴\n",
      "Organizing progress: 11.0 x 10⁴\n",
      "Organizing progress: 12.0 x 10⁴\n",
      "Organizing progress: 13.0 x 10⁴\n",
      "Organizing progress: 14.0 x 10⁴\n",
      "Load Validation Data [3/5]\n",
      "Start Organizing Data...\n",
      "Organizing progress: 0.0 x 10⁴\n",
      "Organizing progress: 1.0 x 10⁴\n",
      "Organizing progress: 2.0 x 10⁴\n",
      "Load Test Data [1/6]\n",
      "Start Organizing Data...\n",
      "Organizing progress: 0.0 x 10⁴\n",
      "Organizing progress: 1.0 x 10⁴\n",
      "Tokenize Train Data [4/5]\n",
      "0.0 x 10⁴/6.3374 x 10⁴\n",
      "1.0 x 10⁴/6.3374 x 10⁴\n",
      "2.0 x 10⁴/6.3374 x 10⁴\n",
      "3.0 x 10⁴/6.3374 x 10⁴\n",
      "4.0 x 10⁴/6.3374 x 10⁴\n",
      "5.0 x 10⁴/6.3374 x 10⁴\n",
      "6.0 x 10⁴/6.3374 x 10⁴\n",
      "Tokenize Validation Data [4/5]\n",
      "0.0 x 10⁴/0.6138 x 10⁴\n",
      "Tokenize Test Data [4/5]\n",
      "0.0 x 10⁴/0.7002 x 10⁴\n",
      "Done reload_state [5/5]\n"
     ]
    }
   ],
   "source": [
    "try_to_split_testset(0.1,0.2, False);\n",
    "\n",
    "checkpoint_w, training_state_w, epoch_w = train_manager.try_to_resume(\n",
    "            args.force_restart, args.exp_folder)\n",
    "\n",
    "if checkpoint_w:\n",
    "    print('Resuming training...')\n",
    "    model_w, id_to_token_w, id_to_char_w, optimizer_w, train_loader, val_loader, test_loader = new_reload(args.train_splitted_data, args.val_splitted_data, \n",
    "                                                                                                          args.test_splitted_data, checkpoint_w, \n",
    "                                                                                                          training_state_w, config, args, used_data_per_train_batch=0.3)\n",
    "else:\n",
    "    print('Preparing to train...')\n",
    "    model_w, id_to_token_w, id_to_char_w, optimizer_w, train_loader, val_loader, test_loader = new_init(args.train_splitted_data, args.val_splitted_data, \n",
    "                                                                                                        args.test_splitted_data,config, args, used_data_per_train_batch=0.3)\n",
    "    checkpoint_w = h5py.File(os.path.join(args.exp_folder, 'checkpoint'))\n",
    "    checkpointing.save_vocab(checkpoint_w, 'vocab', id_to_token_w)\n",
    "    checkpointing.save_vocab(checkpoint_w, 'c_vocab', id_to_char_w)\n",
    "\n",
    "if torch.cuda.is_available() and args.cuda:\n",
    "    train_loader.tensor_type = torch.cuda.LongTensor\n",
    "    val_loader.tensor_type = torch.cuda.LongTensor\n",
    "    #test_loader.tensor_type = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_saves = dict();\n",
    "epoch_saves['train_loss'] = []\n",
    "epoch_saves['val_loss'] = []\n",
    "epoch_saves['test_loss'] = []\n",
    "\n",
    "\n",
    "class BidafLightningWrapper(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def setup(self,stage):\n",
    "        pass;\n",
    "            \n",
    "    def prepare_data(self):\n",
    "        pass;\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optimizer_w;\n",
    "\n",
    "    def forward(self, passage, p_lengths, question, q_lengths):\n",
    "        return model_w(passage, p_lengths, question, q_lengths)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_loader;\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return val_loader;\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return test_loader;\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        qids, passages, queries, answers, _ = batch\n",
    "        start_log_probs, end_log_probs = self(passages[:2], passages[2], queries[:2], queries[2])\n",
    "        loss = model_w.get_loss(start_log_probs, end_log_probs, answers[:, 0], answers[:, 1])\n",
    "        return {'loss': loss, 'train_loss': loss, 'log': {'train_loss': loss}}\n",
    "\n",
    "    def training_epoch_end(self, results):\n",
    "        checkpointing.checkpoint(model_w, epoch_w, optimizer_w, checkpoint_w, args.exp_folder)\n",
    "        model_w.cuda()\n",
    "        mean_loss = self.save_statistics('train',results)\n",
    "        return {'log': {'train_loss': mean_loss}}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        qids, passages, queries, answers, _ = batch\n",
    "        start_log_probs, end_log_probs = self(passages[:2], passages[2], queries[:2], queries[2])\n",
    "        loss = model_w.get_loss(start_log_probs, end_log_probs, answers[:, 0], answers[:, 1])\n",
    "        return {'val_loss': loss, 'log': {'val_loss': loss}}\n",
    "    \n",
    "    def validation_epoch_end(self, results):\n",
    "        val_loss_mean = self.save_statistics('val',results)\n",
    "        return {'val_loss': val_loss_mean}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        qids, passages, queries, answers, _ = batch\n",
    "        start_log_probs, end_log_probs = self(passages[:2], passages[2], queries[:2], queries[2])\n",
    "        loss = model_w.get_loss(start_log_probs, end_log_probs, answers[:, 0], answers[:, 1])\n",
    "        return {'test_loss': loss}\n",
    "\n",
    "    def test_epoch_end(self, results):\n",
    "        mean_test_loss = self.save_statistics('test',results)\n",
    "        return {'log': {'test_loss': mean_test_loss}}\n",
    "\n",
    "    def save_statistics(self, phase, results):\n",
    "        key = phase + '_loss'\n",
    "        mean_loss = torch.stack([step[key] for step in results]).mean()\n",
    "        print(\"Mean {} Loss: {}\".format(phase,mean_loss))\n",
    "        print(epoch_saves.keys())\n",
    "        epoch_saves[key].append([step['val_loss'] for step in results])\n",
    "        return mean_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 100%|██████████| 2/2 [00:00<00:00,  4.62it/s]Mean val Loss: 2.7524495124816895\n",
      "dict_keys(['train_loss', 'val_loss', 'test_loss'])\n",
      "Epoch 1:   6%|▋         | 27/420 [00:08<02:10,  3.02it/s, loss=2.627, v_num=8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/Development/PythonEnv/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-aaee5d557645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelLightning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Development/PythonEnv/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, test_dataloaders, ckpt_path)\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'best'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/PythonEnv/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, tags_csv, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# add the hparams from csv file to checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/PythonEnv/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# no scheme or with a drive letter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/PythonEnv/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/PythonEnv/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/PythonEnv/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "modelLightning = BidafLightningWrapper()\n",
    "trainer = Trainer(max_epochs=2, gpus=1)\n",
    "trainer.fit(modelLightning)\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "outputfile = os.path.join(args.exp_folder,'statistics.json')\n",
    "Path(outputfile).touch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Loss to statistics.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeStatisticToDict(key, output_dict):\n",
    "    output_dict[key] = dict()\n",
    "    stat_saves = epoch_saves[key]\n",
    "\n",
    "    for idx in range(0,len(stat_saves)):\n",
    "        output_dict[key][idx] = dict()\n",
    "        for jdx in range(0,len(stat_saves[idx])):\n",
    "            output_dict[key][idx][jdx] = stat_saves[idx][jdx].item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = dict()\n",
    "\n",
    "writeStatisticToDict('train_loss',output_dict)\n",
    "writeStatisticToDict('val_loss',output_dict)\n",
    "writeStatisticToDict('test_loss',output_dict)\n",
    "\n",
    "with open(outputfile, 'w') as write_f:\n",
    "    write_f.write(json.dumps(output_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Loss from statistics.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputfile, 'r') as read_f:\n",
    "    statistics = json.load(read_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def visualizeLoss(key, stats):\n",
    "    batch_stats = stats[key]\n",
    "    epoch_stats = []\n",
    "    for key in batch_stats:\n",
    "        epoch_stats = epoch_stats + list(batch_stats[key].values())\n",
    "    plt.plot(epoch_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeLoss('train_loss', statistics)\n",
    "visualizeLoss('val_loss',statistics)\n",
    "visualizeLoss('test_loss',statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
